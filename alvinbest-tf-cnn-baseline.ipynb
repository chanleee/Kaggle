{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":99.440385,"end_time":"2023-11-09T03:17:22.555761","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-09T03:15:43.115376","version":"2.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":7120258,"sourceType":"datasetVersion","datasetId":4106744},{"sourceId":7120280,"sourceType":"datasetVersion","datasetId":4106761},{"sourceId":7193532,"sourceType":"datasetVersion","datasetId":4159930},{"sourceId":7216700,"sourceType":"datasetVersion","datasetId":4176484},{"sourceId":7241878,"sourceType":"datasetVersion","datasetId":4194543}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.013176,"end_time":"2023-11-09T03:15:46.586959","exception":false,"start_time":"2023-11-09T03:15:46.573783","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 0. requirements","metadata":{"papermill":{"duration":0.013293,"end_time":"2023-11-09T03:15:46.613651","exception":false,"start_time":"2023-11-09T03:15:46.600358","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# !pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages <package_name>","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.240576Z","start_time":"2023-12-13T13:27:25.284788Z"},"collapsed":false,"papermill":{"duration":0.021933,"end_time":"2023-11-09T03:15:46.649226","exception":false,"start_time":"2023-11-09T03:15:46.627293","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:23.746555Z","iopub.execute_input":"2023-12-19T23:28:23.747359Z","iopub.status.idle":"2023-12-19T23:28:23.777051Z","shell.execute_reply.started":"2023-12-19T23:28:23.747310Z","shell.execute_reply":"2023-12-19T23:28:23.776112Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1. config 설정","metadata":{"papermill":{"duration":0.013582,"end_time":"2023-11-09T03:15:46.844948","exception":false,"start_time":"2023-11-09T03:15:46.831366","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### 1.1. init config","metadata":{"papermill":{"duration":0.013741,"end_time":"2023-11-09T03:15:46.924523","exception":false,"start_time":"2023-11-09T03:15:46.910782","status":"completed"},"tags":[]}},{"cell_type":"code","source":"MODE = \"inference\"  # train, inference, both\nKAGGLE_DATASET_NAME = \"model-version-52\"\n\nis_train = False\nis_infer = True\nis_pre_test = False\n\nINFER_USE_ML = True\n\nINFER_USE_TF = True\n\nINFER_USE_CNN = True\n\nN_Folds = 5","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.488885Z","start_time":"2023-12-13T13:27:25.294684Z"},"collapsed":false,"papermill":{"duration":0.026122,"end_time":"2023-11-09T03:15:46.964537","exception":false,"start_time":"2023-11-09T03:15:46.938415","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:40:26.129254Z","iopub.execute_input":"2023-12-19T23:40:26.130557Z","iopub.status.idle":"2023-12-19T23:40:26.136609Z","shell.execute_reply.started":"2023-12-19T23:40:26.130514Z","shell.execute_reply":"2023-12-19T23:40:26.135499Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import gc\nimport os\nimport time\nimport warnings\nfrom itertools import combinations\nfrom warnings import simplefilter\nimport functools\nimport time\nfrom numba import njit, prange\nimport numba\nimport pyarrow.parquet as pq\nfrom tqdm import tqdm\nimport glob\nimport polars as pl\nimport datetime\n\nimport joblib\nimport lightgbm as lgb\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport optuna\nfrom functools import partial\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom typing import Dict, List, Optional, Tuple\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer,minmax_scale, QuantileTransformer\nfrom sklearn.decomposition import PCA,TruncatedSVD,LatentDirichletAllocation\nfrom sklearn.neighbors import KNeighborsClassifier,NearestNeighbors\nfrom sklearn.impute import KNNImputer\nimport traceback\nfrom contextlib import contextmanager\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, Subset, random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom itertools import combinations\n\n\npd.set_option('display.max_rows', None)\n\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.765126Z","start_time":"2023-12-13T13:27:25.313606Z"},"papermill":{"duration":5.644978,"end_time":"2023-11-09T03:16:25.965358","exception":false,"start_time":"2023-11-09T03:16:20.32038","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-19T23:32:26.239810Z","iopub.execute_input":"2023-12-19T23:32:26.240975Z","iopub.status.idle":"2023-12-19T23:32:26.252589Z","shell.execute_reply.started":"2023-12-19T23:32:26.240931Z","shell.execute_reply":"2023-12-19T23:32:26.251718Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2. train / inference config","metadata":{"papermill":{"duration":0.014983,"end_time":"2023-11-09T03:16:25.995747","exception":false,"start_time":"2023-11-09T03:16:25.980764","status":"completed"},"tags":[]}},{"cell_type":"code","source":"lgb.__version__, xgb.__version__","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.765439Z","start_time":"2023-12-13T13:27:26.203442Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.565713Z","iopub.execute_input":"2023-12-19T23:28:32.566423Z","iopub.status.idle":"2023-12-19T23:28:32.573364Z","shell.execute_reply.started":"2023-12-19T23:28:32.566394Z","shell.execute_reply":"2023-12-19T23:28:32.572555Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"('3.3.2', '2.0.2')"},"metadata":{}}]},{"cell_type":"code","source":"EPS = 1e-10","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.765535Z","start_time":"2023-12-13T13:27:26.203602Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.574616Z","iopub.execute_input":"2023-12-19T23:28:32.574937Z","iopub.status.idle":"2023-12-19T23:28:32.584355Z","shell.execute_reply.started":"2023-12-19T23:28:32.574914Z","shell.execute_reply":"2023-12-19T23:28:32.583462Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"if MODE == \"train\":\n    print(\"You are in train mode\")\n    model_directory = \"./models/\" + time.strftime(\"%Y%m%d_%H:%M:%S\", time.localtime(time.time() + 9 * 60 * 60))\n    data_directory = \"./data\"\n    train_mode = True\n    infer_mode = False\nelif MODE == \"inference\":\n    print(\"You are in inference mode\")\n    model_directory = f'/kaggle/input/{KAGGLE_DATASET_NAME}'\n    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n    train_mode = False\n    infer_mode = True\nelif MODE == \"both\":\n    print(\"You are in both mode\")\n    model_directory = f'/kaggle/working/'\n    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n    train_mode = True\n    infer_mode = True\nelse:\n    raise ValueError(\"Invalid mode\")","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.765668Z","start_time":"2023-12-13T13:27:26.203651Z"},"collapsed":false,"papermill":{"duration":0.025998,"end_time":"2023-11-09T03:16:26.037324","exception":false,"start_time":"2023-11-09T03:16:26.011326","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.585671Z","iopub.execute_input":"2023-12-19T23:28:32.585914Z","iopub.status.idle":"2023-12-19T23:28:32.594848Z","shell.execute_reply.started":"2023-12-19T23:28:32.585894Z","shell.execute_reply":"2023-12-19T23:28:32.593868Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"You are in inference mode\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 1.3. model config","metadata":{"papermill":{"duration":0.01504,"end_time":"2023-11-09T03:16:26.067759","exception":false,"start_time":"2023-11-09T03:16:26.052719","status":"completed"},"tags":[]}},{"cell_type":"code","source":"config = {\n    ### default config\n    \"data_dir\": data_directory,\n    \"model_dir\": model_directory,\n    \"train_mode\": train_mode,  # True : train, False : not train\n    \"infer_mode\": infer_mode,  # True : inference, False : not inference\n\n    ### model config\n    \"model_name\": [\"lgb_b\"],  # model name\n    \"stacking_mode\": False,  # stacking mode or not (single model도 split되면 그걸로 stacking)\n    \"stacking_algorithm\": None,  # \"optuna\",  # or None\n\n    \"target\": \"target\",\n\n    ### model hyperparameter\n    \"optuna_random_state\": 42,\n\n    ### cv hyperparameter\n    \"split_method\": \"purged\",  # time_series, rolling, blocking, holdout\n    \"n_splits\": 5,  # number of splits\n    \"correct\": True,  # correct boundary\n    \"gap\": 0.05,  # gap between train and test (0.05 = 5% of train size)\n    \"initial_fold_size_ratio\": 0.4,  # initial fold size ratio\n    \"train_test_ratio\": 0.95,  # train, test ratio\n}","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.765735Z","start_time":"2023-12-13T13:27:26.203729Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.595932Z","iopub.execute_input":"2023-12-19T23:28:32.596176Z","iopub.status.idle":"2023-12-19T23:28:32.607365Z","shell.execute_reply.started":"2023-12-19T23:28:32.596156Z","shell.execute_reply":"2023-12-19T23:28:32.606451Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if config[\"stacking_mode\"] == True and config[\"n_splits\"] == 1:\n    raise ValueError(\"stacking mode is True but n_splits is 1, cannot stacking\")\nif config[\"stacking_mode\"] == False and config[\"stacking_algorithm\"] is not None:\n    raise ValueError(\"stacking mode is False but stacking_algorithm is not None, impossible\")\nif config[\"stacking_mode\"] == True and config[\"stacking_algorithm\"] is None:\n    raise ValueError(\"stacking mode is True but stacking_algorithm is None, impossible\")","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.765805Z","start_time":"2023-12-13T13:27:26.209537Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.608544Z","iopub.execute_input":"2023-12-19T23:28:32.608841Z","iopub.status.idle":"2023-12-19T23:28:32.619296Z","shell.execute_reply.started":"2023-12-19T23:28:32.608818Z","shell.execute_reply":"2023-12-19T23:28:32.617518Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### 1.4. model heyperparameter config","metadata":{"papermill":{"duration":0.015075,"end_time":"2023-11-09T03:16:26.139996","exception":false,"start_time":"2023-11-09T03:16:26.124921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"models_config = {\n    \"lgb\": {\n        \"model\": lgb.LGBMRegressor,\n        \"params\": {\n            \"objective\": \"mae\",\n            \"n_estimators\": 9999,  # 2040\n            \"num_leaves\": 126,\n            \"subsample\": 0.7628752081565437,\n            \"colsample_bytree\": 0.6380919043232433,\n            \"learning_rate\": 0.01795041572109495,\n            \"n_jobs\": 4,\n            \"device\": \"gpu\",\n            \"verbosity\": -1,\n            \"importance_type\": \"gain\",\n        },\n    },\n    \"lgb_b\": {\n        \"model\": lgb.LGBMRegressor,\n        \"params\": {\n            \"objective\": \"mae\",\n            \"n_estimators\": 6000,\n            \"num_leaves\": 256,\n            \"subsample\": 0.6,\n            \"colsample_bytree\": 0.8,\n            \"learning_rate\": 0.00871,\n            'max_depth': 11,\n            \"n_jobs\": 4,\n            \"device\": \"gpu\",\n            \"verbosity\": -1,\n            \"importance_type\": \"gain\",\n            \"reg_alpha\": 0.1,\n            \"reg_lambda\": 3.25\n        },\n    },\n    \"xgb\": {\n        \"model\": xgb.XGBRegressor,\n        \"params\": {\n            \"objective\": \"reg:linear\",\n            \"n_estimators\": 9999,  # 2400\n            \"max_depth\": 14,\n            \"eta\": 0.0073356282482453065,\n            \"subsample\": 0.9,\n            \"colsample_bytree\": 0.30000000000000004,\n            \"colsample_bylevel\": 0.9,\n            \"min_child_weight\": 0.4824060812428942,\n            \"reg_lambda\": 182.50819193990537,\n            \"reg_alpha\": 0.03171419713574529,\n            \"gamma\": 0.9162634503670075,\n            \"tree_method\": \"gpu_hist\",\n            \"n_jobs\": 4,\n            \"verbosity\": 0,\n        },\n    },\n}","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:26.765869Z","start_time":"2023-12-13T13:27:26.214478Z"},"collapsed":false,"papermill":{"duration":0.027209,"end_time":"2023-11-09T03:16:26.18251","exception":false,"start_time":"2023-11-09T03:16:26.155301","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.621892Z","iopub.execute_input":"2023-12-19T23:28:32.622861Z","iopub.status.idle":"2023-12-19T23:28:32.632896Z","shell.execute_reply.started":"2023-12-19T23:28:32.622825Z","shell.execute_reply":"2023-12-19T23:28:32.632089Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if MODE == \"train\":\n    if not os.path.exists(config[\"model_dir\"]):\n        os.makedirs(config[\"model_dir\"])\n    if not os.path.exists(config[\"data_dir\"]):\n        os.makedirs(config[\"data_dir\"])\n    !kaggle competitions download optiver-trading-at-the-close -p {config[\"data_dir\"]} --force\n    !unzip -o {config[\"data_dir\"]} /optiver-trading-at-the-close.zip -d {config[\"data_dir\"]}\n    !rm {config[\"data_dir\"]} /optiver-trading-at-the-close.zip","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:34.419801Z","start_time":"2023-12-13T13:27:26.231855Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.638055Z","iopub.execute_input":"2023-12-19T23:28:32.638933Z","iopub.status.idle":"2023-12-19T23:28:32.649951Z","shell.execute_reply.started":"2023-12-19T23:28:32.638895Z","shell.execute_reply":"2023-12-19T23:28:32.648539Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# ## Global Method","metadata":{"papermill":{"duration":0.015155,"end_time":"2023-11-09T03:16:26.253174","exception":false,"start_time":"2023-11-09T03:16:26.238019","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    return df","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:34.473682Z","start_time":"2023-12-13T13:27:34.419882Z"},"collapsed":false,"papermill":{"duration":0.030413,"end_time":"2023-11-09T03:16:26.299001","exception":false,"start_time":"2023-11-09T03:16:26.268588","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.654321Z","iopub.execute_input":"2023-12-19T23:28:32.654663Z","iopub.status.idle":"2023-12-19T23:28:32.667012Z","shell.execute_reply.started":"2023-12-19T23:28:32.654632Z","shell.execute_reply":"2023-12-19T23:28:32.665599Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    \"\"\"\n    Calculate the triplet imbalance for each row in the DataFrame.\n    :param df_values: \n    :param comb_indices: \n    :return: \n    \"\"\"\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            if mid_val == min_val:  # Prevent division by zero\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val + EPS)\n\n    return imbalance_features\n\n\ndef calculate_triplet_imbalance_numba(price, df):\n    \"\"\"\n    Calculate the triplet imbalance for each row in the DataFrame.\n    :param price: \n    :param df: \n    :return: \n    \"\"\"\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:34.473925Z","start_time":"2023-12-13T13:27:34.466511Z"},"collapsed":false,"papermill":{"duration":0.117482,"end_time":"2023-11-09T03:16:26.432299","exception":false,"start_time":"2023-11-09T03:16:26.314817","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.669398Z","iopub.execute_input":"2023-12-19T23:28:32.670076Z","iopub.status.idle":"2023-12-19T23:28:32.755106Z","shell.execute_reply.started":"2023-12-19T23:28:32.670020Z","shell.execute_reply":"2023-12-19T23:28:32.753703Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def print_log(message_format):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # self 확인: 첫 번째 인자가 클래스 인스턴스인지 확인합니다.\n            if args and hasattr(args[0], 'infer'):\n                self = args[0]\n\n                # self.infer가 False이면 아무 것도 출력하지 않고 함수를 바로 반환합니다.\n                if self.infer:\n                    return func(*args, **kwargs)\n\n            start_time = time.time()\n            result = func(*args, **kwargs)\n            end_time = time.time()\n\n            elapsed_time = end_time - start_time\n\n            if result is not None:\n                data_shape = getattr(result, 'shape', 'No shape attribute')\n                shape_message = f\", shape({data_shape})\"\n            else:\n                shape_message = \"\"\n\n            print(f\"\\n{'-' * 100}\")\n            print(message_format.format(func_name=func.__name__, elapsed_time=elapsed_time) + shape_message)\n            print(f\"{'-' * 100}\\n\")\n\n            return result\n\n        return wrapper\n\n    return decorator\n","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:34.474098Z","start_time":"2023-12-13T13:27:34.466993Z"},"collapsed":false,"papermill":{"duration":0.035964,"end_time":"2023-11-09T03:16:26.484162","exception":false,"start_time":"2023-11-09T03:16:26.448198","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.756406Z","iopub.execute_input":"2023-12-19T23:28:32.756695Z","iopub.status.idle":"2023-12-19T23:28:32.765838Z","shell.execute_reply.started":"2023-12-19T23:28:32.756670Z","shell.execute_reply":"2023-12-19T23:28:32.764365Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices) / np.sum(std_error)\n    out = prices - std_error * step\n    return out","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:34.474252Z","start_time":"2023-12-13T13:27:34.472512Z"},"collapsed":false,"papermill":{"duration":0.0316,"end_time":"2023-11-09T03:16:26.537507","exception":false,"start_time":"2023-11-09T03:16:26.505907","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.767946Z","iopub.execute_input":"2023-12-19T23:28:32.769400Z","iopub.status.idle":"2023-12-19T23:28:32.779369Z","shell.execute_reply.started":"2023-12-19T23:28:32.769355Z","shell.execute_reply":"2023-12-19T23:28:32.778385Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def custom_pct_change(series, window=1, epsilon=1e-10):\n    return (series.diff(window) / (series.shift(window) + epsilon)).reset_index(drop=True)","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:34.555872Z","start_time":"2023-12-13T13:27:34.51104Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.780918Z","iopub.execute_input":"2023-12-19T23:28:32.781224Z","iopub.status.idle":"2023-12-19T23:28:32.788980Z","shell.execute_reply.started":"2023-12-19T23:28:32.781201Z","shell.execute_reply":"2023-12-19T23:28:32.787981Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"all_stock_data = {}\nfor s in tqdm(glob.glob(\"./data/alpha/*.csv\") if MODE == \"train\" else glob.glob(\n        \"/kaggle/input/nasdaq-stocks-historical-data/alpha/*.csv\"), desc=\"Processing files\"):\n    stock_df = pd.read_csv(s, dtype={\"ticker\": str})\n    stock_df.query(\"Date >= '2021-08-05' and Date <= '2023-07-06'\", inplace=True)\n    if len(stock_df) > 180:\n        all_stock_data[s[13:-15]] = (stock_df, len(stock_df))\n\nreversed_stock_list = [\n    'MNST', 'WING', 'AXON', 'HON', 'MAR', 'OKTA', 'POOL', 'LRCX', 'YOTA', 'PFG',\n    'NDAQ', 'COIN', 'AMGN', 'TER', 'ADBE', 'ABNB', 'ZBRA', 'KLAC', 'ZI', 'ALNY',\n    'ULTA', 'SSNC', 'ON', 'SWKS', 'AKAM', 'ASML', 'PPBI', 'QRVO', 'FANG', 'ORLY',\n    'LNT', 'AGRX', 'NTAP', 'CROX', 'REGN', 'ROST', 'DLTR', 'ADP', 'EMCG', 'CTAS',\n    'CZR', 'NVDA', 'SAIA', 'JKHY', 'FOSLL', 'MSFT', 'TECH', 'TXRH', 'WDAY', 'FITB',\n    'MTCH', 'ROKU', 'CINF', 'EBAY', 'SNPS', 'FAST', 'ETSY', 'IDXX', 'INTU', 'ZG',\n    'CRWD', 'LYFT', 'RGEN', 'LKQ', 'MKTX', 'EXC', 'LBRDK', 'MRNA', 'PAYX', 'SOFI',\n    'BYND', 'EQIX', 'ADI', 'GEN', 'ALGN', 'CDNS', 'HAS', 'VRTX', 'HOOD', 'WBD',\n    'TXG', 'SGEN', 'OPEN', 'INTC', 'GOOG', 'CAR', 'UPST', 'LSCC', 'NFLX', 'ENTG',\n    'FFIV', 'DOCU', 'MSTR', 'ZION', 'PCTY', 'AMD', 'MRVL', 'NBIX', 'JBLU', 'PARA',\n    'MQ', 'FCNCA', 'TEAM', 'ZS', 'WBA', 'MDLZ', 'TRMB', 'PODD', 'SEDG', 'CSX',\n    'TMUS', 'SPWR', 'AAPL', 'LULU', 'LPLA', 'ILMN', 'CDW', 'GDS', 'MELI', 'MASI',\n    'FOXA', 'KDP', 'AAL', 'GILD', 'ASO', 'UTHR', 'MU', 'MDB', 'WDC', 'CFLT',\n    'SBUX', 'INCY', 'TSCO', 'ISRG', 'VTRS', 'DKNG', 'LITE', 'TTWO', 'SMCI', 'EXPE',\n    'VRTS', 'AMAT', 'AVGO', 'TLRY', 'PCAR', 'CG', 'MIDD', 'APA', 'LNT', 'VRSK',\n    'PANW', 'CSCO', 'SBAC', 'HTZ', 'DBX', 'CHKEW', 'LCID', 'ADSK', 'APLS', 'STLD',\n    'PEP', 'PTON', 'ENPH', 'COST', 'CPRT', 'HST', 'KHC', 'CHRW', 'AMZN', 'ANSS',\n    'HOLX', 'TROW', 'APP', 'FIVE', 'AFRM', 'GOOGL', 'FTNT', 'SWAV', 'ZM', 'META',\n    'GH', 'JBHT', 'UAL', 'MCHP', 'DDOG', 'ODFL', 'CTSH', 'EA', 'RUN', 'CSGP',\n    'DXCM', 'TSLA', 'PTC', 'PYPL', 'PENN', 'XEL', 'XRAY', 'SPLK', 'CMCSA', 'BKR'\n]\n\nstock_list_df = pd.read_csv(\n    './data/nasdaq-screener/nasdaq_screener_1701158836955.csv') if MODE == \"train\" else pd.read_csv(\n    '/kaggle/input/nasdaq-screener/nasdaq_screener_1701158836955.csv')","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.374683Z","start_time":"2023-12-13T13:27:34.511271Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:28:32.790478Z","iopub.execute_input":"2023-12-19T23:28:32.790942Z","iopub.status.idle":"2023-12-19T23:29:31.702300Z","shell.execute_reply.started":"2023-12-19T23:28:32.790918Z","shell.execute_reply":"2023-12-19T23:29:31.701237Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Processing files: 100%|██████████| 4235/4235 [00:58<00:00, 72.28it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_stock_info(df, data, column_name):  # column_name = \"Market Cap\", \"Sector\", \"Industry\"\n    le = LabelEncoder()\n\n    if column_name != \"Market Cap\":\n        stock_list_df[column_name] = le.fit_transform(stock_list_df[column_name])\n\n    df[f'{column_name}'] = -1\n\n    for idx, ticker in enumerate(reversed_stock_list):\n        stock_id_indices = data[data['stock_id'] == idx].index\n        if ticker in stock_list_df[\"Symbol\"].values:\n            value = stock_list_df[stock_list_df[\"Symbol\"] == ticker][column_name].iloc[0]\n            df.loc[stock_id_indices, f'{column_name}'] = value\n\n    return df","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.374854Z","start_time":"2023-12-13T13:27:42.332864Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.704185Z","iopub.execute_input":"2023-12-19T23:29:31.704549Z","iopub.status.idle":"2023-12-19T23:29:31.713245Z","shell.execute_reply.started":"2023-12-19T23:29:31.704516Z","shell.execute_reply":"2023-12-19T23:29:31.710987Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"@numba.jit(nopython=True)\ndef find_exact_two_decimal_divisors(number, target):\n    closest_divisor = None\n    closest_value = None\n    min_diff = np.inf\n\n    for divisor in range(1, int(number) + 1):\n        division_result = number / divisor\n        if np.abs(division_result - np.round(division_result, 2)) < 1e-10:\n            diff = np.abs(division_result - target)\n            if diff < min_diff:\n                min_diff = diff\n                closest_divisor = divisor\n                closest_value = division_result\n\n    return closest_divisor, closest_value","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.374931Z","start_time":"2023-12-13T13:27:42.334182Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.715287Z","iopub.execute_input":"2023-12-19T23:29:31.715835Z","iopub.status.idle":"2023-12-19T23:29:31.733772Z","shell.execute_reply.started":"2023-12-19T23:29:31.715785Z","shell.execute_reply":"2023-12-19T23:29:31.731932Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def estimate_and_verify_prices(df):\n    all_data_df_list = []\n\n    for date_id, group in df.groupby('date_id'):\n        try:\n            tick_size_min = group[\"tick_size_min\"].iloc[0]\n            group = group.copy()\n            # group['date'] = group['date_id'].apply(lambda x: date_id_dict[x])\n            group[\"date\"] = date_id_dict.get(date_id)\n            group['estimated_price'] = group['ask_price'] / tick_size_min * 0.01\n            group['estimated_price_diff'] = group['estimated_price'].diff().abs().fillna(0) * 100\n            results = group.apply(lambda x: find_exact_two_decimal_divisors(x['ask_size'], x['estimated_price']),\n                                  axis=1)\n            group['quantity'], group['integer_validated_value'] = zip(*results)\n\n            for i in range(1, len(group)):\n                prev_val = group.iloc[i - 1]['integer_validated_value']\n                curr_val = group.iloc[i]['integer_validated_value']\n                estimated_diff = int(group.iloc[i]['estimated_price_diff']) * 0.01\n\n                if abs(curr_val - prev_val) > estimated_diff:\n                    if abs((curr_val + estimated_diff) - prev_val) < abs((curr_val - estimated_diff) - prev_val):\n                        group.at[group.index[i], 'integer_validated_value'] = curr_val + estimated_diff\n                    else:\n                        group.at[group.index[i], 'integer_validated_value'] = curr_val - estimated_diff\n\n            all_data_df_list.append(group)\n        except Exception as e:\n            print(e)\n            continue\n\n    return pd.concat(all_data_df_list)","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.375001Z","start_time":"2023-12-13T13:27:42.334328Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.736081Z","iopub.execute_input":"2023-12-19T23:29:31.736599Z","iopub.status.idle":"2023-12-19T23:29:31.748459Z","shell.execute_reply.started":"2023-12-19T23:29:31.736552Z","shell.execute_reply":"2023-12-19T23:29:31.747580Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04, 0.002, 0.002, 0.004, 0.04, 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02, 0.004, 0.006, 0.002, 0.02, 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04, 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04, 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02, 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\nweights = {int(k): v for k, v in enumerate(weights)}","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.402369Z","start_time":"2023-12-13T13:27:42.375033Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.749465Z","iopub.execute_input":"2023-12-19T23:29:31.749753Z","iopub.status.idle":"2023-12-19T23:29:31.769446Z","shell.execute_reply.started":"2023-12-19T23:29:31.749730Z","shell.execute_reply":"2023-12-19T23:29:31.768360Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def weighted_average(n, equal_weight=True):\n    w = []\n\n    if equal_weight:\n        # 모든 원소에 동일한 가중치 (1/n) 부여\n        w = [1 / n for _ in range(n)]\n    else:\n        # 가중치 계산 방식에 따라 다른 가중치 부여\n        for j in range(1, n + 1):\n            j = 2 if j == 1 else j\n            w.append(1 / (2 ** (n + 1 - j)))\n\n    return w\n","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.402578Z","start_time":"2023-12-13T13:27:42.375222Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.770490Z","iopub.execute_input":"2023-12-19T23:29:31.771173Z","iopub.status.idle":"2023-12-19T23:29:31.785169Z","shell.execute_reply.started":"2023-12-19T23:29:31.771142Z","shell.execute_reply":"2023-12-19T23:29:31.784209Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"#### 각 클래스의 method는 각자 필요에 따라 추가 해서 사용하면 됩니다. 이때 class의 주석에 method를 추가하고, method의 주석에는 method의 역할을 간단하게 적어주세요.","metadata":{"papermill":{"duration":0.017946,"end_time":"2023-11-09T03:16:26.571351","exception":false,"start_time":"2023-11-09T03:16:26.553405","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# ## Pre Code","metadata":{"papermill":{"duration":0.019169,"end_time":"2023-11-09T03:16:26.609856","exception":false,"start_time":"2023-11-09T03:16:26.590687","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Split Data Class","metadata":{}},{"cell_type":"code","source":"class Splitter:\n    \"\"\"\n    데이터 분리 클래스\n    \n    Attributes\n    ----------\n    method : str\n        데이터 분리 방식\n    n_splits : int\n        데이터 분리 개수\n    correct : bool\n        데이터 분리 시 boundary를 맞출지 여부\n    initial_fold_size_ratio : float\n        초기 fold size 비율\n    train_test_ratio : float\n        train, test 비율\n        \n    Methods\n    -------\n    split()\n        데이터 분리 수행\n    \"\"\"\n\n    def __init__(self, method, n_splits, correct, initial_fold_size_ratio=0.6, train_test_ratio=0.8, gap=0,\n                 origin_data=None,\n                 overlap=True, train_start=0,\n                 train_end=390, valid_start=391, valid_end=480):\n        self.method = method\n        self.n_splits = n_splits\n        self.correct = correct\n        self.initial_fold_size_ratio = initial_fold_size_ratio\n        self.train_test_ratio = train_test_ratio\n\n        self.gap = gap\n        self.origin_data = origin_data\n        self.overlap = overlap\n\n        # only for holdout method\n        self.train_start = train_start\n        self.train_end = train_end\n        self.valid_start = valid_start\n        self.valid_end = valid_end\n\n        self.target = config[\"target\"]\n\n        self.boundaries = []\n\n    def split(self, data, p_gap=None):\n        self.data = reduce_mem_usage(data)\n        self.all_dates = self.data['date_id_copy'].unique()\n        if self.method == \"time_series\":\n            if self.n_splits <= 1:\n                raise ValueError(\"Time series split method only works with n_splits > 1\")\n            return self._time_series_split(data)\n        elif self.method == \"rolling\":\n            if self.n_splits <= 1:\n                raise ValueError(\"Rolling split method only works with n_splits > 1\")\n            return self._rolling_split(data)\n        elif self.method == \"blocking\":\n            if self.n_splits <= 1:\n                raise ValueError(\"Blocking split method only works with n_splits > 1\")\n            self.initial_fold_size_ratio = 1.0 / self.n_splits\n            return self._rolling_split(data)\n        elif self.method == \"holdout\":\n            if self.n_splits != 1:\n                raise ValueError(\"Holdout method only works with n_splits=1\")\n            return self._holdout_split(data)\n\n        elif self.method == \"purged\":\n            if self.n_splits <= 1:\n                raise ValueError(\"Purged split method only works with n_splits > 1\")\n            return self._purged_split(data, p_gap)\n        else:\n            raise ValueError(\"Invalid method\")\n\n    def _correct_boundary(self, data, idx, direction=\"forward\"):\n        # Correct the boundary based on date_id_copy\n        original_idx = idx\n        if idx == 0 or idx == len(data) - 1:\n            return idx\n        if direction == \"forward\":\n            while idx < len(data) and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n                idx += 1\n        elif direction == \"backward\":\n            while idx > 0 and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n                idx -= 1\n            idx += 1  # adjust to include the boundary\n        return idx\n\n    def _time_series_split(self, data):\n        n = len(data)\n        initial_fold_size = int(n * self.initial_fold_size_ratio)\n        initial_test_size = int(initial_fold_size * (1 - self.train_test_ratio))\n        increment = (1.0 - self.initial_fold_size_ratio) / (self.n_splits - 1)\n\n        for i in range(self.n_splits):\n            fold_size = int(n * (self.initial_fold_size_ratio + i * increment))\n            train_size = fold_size - initial_test_size\n\n            if self.correct:\n                train_size = self._correct_boundary(data, train_size, \"forward\")\n                end_of_test = self._correct_boundary(data, train_size + initial_test_size, \"forward\")\n            else:\n                end_of_test = train_size + initial_test_size\n\n            train_slice = data.iloc[:train_size]\n            test_slice = data.iloc[train_size:end_of_test]\n            if test_slice.shape[0] == 0:\n                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n\n            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n            y_train = train_slice[self.target]\n            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n            y_test = test_slice[self.target]\n\n            self.boundaries.append((\n                train_slice['date_id_copy'].iloc[0],\n                train_slice['date_id_copy'].iloc[-1],\n                test_slice['date_id_copy'].iloc[-1]\n            ))\n            yield X_train, y_train, X_test, y_test\n\n    def _rolling_split(self, data):\n        n = len(data)\n        total_fold_size = int(n * self.initial_fold_size_ratio)\n        test_size = int(total_fold_size * (1 - self.train_test_ratio))\n        gap_size = int(total_fold_size * self.gap)\n        train_size = total_fold_size - test_size\n        rolling_increment = (n - total_fold_size) // (self.n_splits - 1)\n\n        end_of_test = n - 1\n        start_of_test = end_of_test - test_size\n        end_of_train = start_of_test - gap_size\n        start_of_train = end_of_train - train_size\n\n        for _ in range(self.n_splits):\n            if self.correct:\n                start_of_train = self._correct_boundary(data, start_of_train, direction=\"forward\")\n                end_of_train = self._correct_boundary(data, end_of_train, direction=\"backward\")\n                start_of_test = self._correct_boundary(data, start_of_test, direction=\"forward\")\n                end_of_test = self._correct_boundary(data, end_of_test, direction=\"forward\")\n\n            train_slice = data[start_of_train:end_of_train]\n            test_slice = data[start_of_test:end_of_test]\n            if test_slice.shape[0] == 0:\n                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n\n            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n            y_train = train_slice[self.target]\n            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n            y_test = test_slice[self.target]\n\n            self.boundaries.append((\n                train_slice['date_id_copy'].iloc[0],\n                train_slice['date_id_copy'].iloc[-1],\n                test_slice['date_id_copy'].iloc[0],\n                test_slice['date_id_copy'].iloc[-1]\n            ))\n            yield X_train, y_train, X_test, y_test\n            start_of_train = max(start_of_train - rolling_increment, 0)\n            end_of_train -= rolling_increment\n            start_of_test -= rolling_increment\n            end_of_test -= rolling_increment\n\n    def _holdout_split(self, data):\n        # train_start ~ train_end : 학습 데이터 기간\n        # valid_start ~ valid_end : 검증 데이터 기간\n        # 학습 및 검증 데이터 분리\n        train_mask = (data['date_id_copy'] >= self.train_start) & (data['date_id_copy'] <= self.train_end)\n        valid_mask = (data['date_id_copy'] >= self.valid_start) & (data['date_id_copy'] <= self.valid_end)\n\n        train_slice = data[train_mask]\n        valid_slice = data[valid_mask]\n\n        X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n        y_train = train_slice[self.target]\n        X_valid = valid_slice.drop(columns=[self.target, 'date_id_copy'])\n        y_valid = valid_slice[self.target]\n\n        self.boundaries.append((\n            train_slice['date_id_copy'].iloc[0],\n            train_slice['date_id_copy'].iloc[-1],\n            valid_slice['date_id_copy'].iloc[0],\n            valid_slice['date_id_copy'].iloc[-1]\n        ))\n        yield X_train, y_train, X_valid, y_valid\n\n    def _purged_split(self, data, p_gap):\n        fold_size = 480 // self.n_splits\n        date_ids = data['date_id_copy'].values\n\n        for i in range(self.n_splits):\n            start = i * fold_size\n            end = start + fold_size\n            if i < self.n_splits - 1:  # No need to purge after the last fold\n                purged_start = end - 2\n                purged_end = end + p_gap + 2\n                train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n            else:\n                train_indices = (date_ids >= start) & (date_ids < end)\n\n            test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n\n            import gc\n            gc.collect()\n\n            df_fold_train = data.drop(columns=[self.target, 'date_id_copy']).iloc[train_indices]\n            df_fold_train_target = data['target'][train_indices]\n            df_fold_valid = data.drop(columns=[self.target, 'date_id_copy']).iloc[test_indices]\n            df_fold_valid_target = data['target'][test_indices]\n\n            self.boundaries.append((\n                data['date_id_copy'].iloc[train_indices].iloc[0],\n                data['date_id_copy'].iloc[train_indices].iloc[-1],\n                data['date_id_copy'].iloc[test_indices].iloc[0],\n                data['date_id_copy'].iloc[test_indices].iloc[-1]\n            ))\n            yield df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n\n    def visualize_splits(self):\n        print(\"Visualizing Train/Test Split Boundaries\")\n\n        plt.figure(figsize=(15, 6))\n\n        for idx, (train_start, train_end, test_start, test_end) in enumerate(self.boundaries):\n            train_width = train_end - train_start + 1\n            plt.barh(y=idx, width=train_width, left=train_start, color='blue', edgecolor='black')\n            plt.text(train_start + train_width / 2, idx - 0.15, f'{train_start}-{train_end}', ha='center', va='center',\n                     color='black', fontsize=8)\n\n            test_width = test_end - test_start + 1\n            plt.barh(y=idx, width=test_width, left=test_start, color='red', edgecolor='black')\n            if test_width > 0:\n                plt.text(test_start + test_width / 2, idx + 0.15, f'{test_start}-{test_end}', ha='center', va='center',\n                         color='black', fontsize=8)\n\n        plt.yticks(range(len(self.boundaries)), [f\"split {i + 1}\" for i in range(len(self.boundaries))])\n        plt.xticks(self.all_dates[::int(len(self.all_dates) / 10)])\n        plt.xlabel(\"date_id_copy\")\n        plt.title(\"Train/Test Split Boundaries\")\n        plt.grid(axis='x')\n\n        plt.tight_layout()\n        plt.show()","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.43886Z","start_time":"2023-12-13T13:27:42.375278Z"},"collapsed":false,"papermill":{"duration":0.060516,"end_time":"2023-11-09T03:16:26.914353","exception":false,"start_time":"2023-11-09T03:16:26.853837","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.786543Z","iopub.execute_input":"2023-12-19T23:29:31.787388Z","iopub.status.idle":"2023-12-19T23:29:31.826733Z","shell.execute_reply.started":"2023-12-19T23:29:31.787349Z","shell.execute_reply":"2023-12-19T23:29:31.825244Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing Class","metadata":{}},{"cell_type":"code","source":"class DataPreprocessor:\n    \"\"\"\n    데이터 전처리 클래스\n    \n    Attributes\n    ----------\n    data : pandas.DataFrame\n        전처리할 데이터\n        \n    Methods\n    -------\n    handle_missing_data()\n        결측치 처리\n    handle_outliers()\n        이상치 처리\n    normalize()\n        정규화\n    custom_preprocessing()\n        사용자 정의 전처리\n    transform()\n        전처리 수행\n    \"\"\"\n\n    def __init__(self, data, infer=False):\n        self.data = data  # reduce_mem_usage(data) # reduce_mem_usage 정밀도 훼손함 \n        self.infer = infer\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def handle_missing_data(self):\n        # 결측치 처리 코드\n        self.data = self.data.dropna(subset=[\"target\"]) if self.infer == False else self.data\n        self.data = self.data.reset_index(drop=True) if self.infer == False else self.data\n        # self.data.reset_index(drop=True, inplace=True)\n        return self.data\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def handle_outliers(self):\n        # 이상치 처리 코드\n        return self.data\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def normalize(self):\n        # 정규화 코드\n        return self.data\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def custom_preprocessing(self):\n        # 사용자 정의 전처리 코드\n        return self.data\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def transform(self):\n        # 전처리 수행 코드 (위의 메소드 활용 가능)\n        self.handle_missing_data()\n        # self.handle_outliers()\n        # self.normalize()\n        # self.custom_preprocessing()\n        return self.data","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.43902Z","start_time":"2023-12-13T13:27:42.423474Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.828982Z","iopub.execute_input":"2023-12-19T23:29:31.829370Z","iopub.status.idle":"2023-12-19T23:29:31.839894Z","shell.execute_reply.started":"2023-12-19T23:29:31.829331Z","shell.execute_reply":"2023-12-19T23:29:31.838732Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering Class","metadata":{}},{"cell_type":"code","source":"global_features = {}","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.439096Z","start_time":"2023-12-13T13:27:42.423656Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.841724Z","iopub.execute_input":"2023-12-19T23:29:31.842019Z","iopub.status.idle":"2023-12-19T23:29:31.855848Z","shell.execute_reply.started":"2023-12-19T23:29:31.841996Z","shell.execute_reply":"2023-12-19T23:29:31.854423Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class FeatureEngineer:\n    \"\"\"\n    이 클래스는 데이터 세트에 대한 피처 엔지니어링을 수행합니다.\n    클래스의 주요 목적은 데이터 세트에 대한 다양한 변환 및 가공을 통해 머신 러닝 모델에 적합한 형태의 피처를 생성하는 것입니다.\n\n    클래스에는 다음과 같은 메서드들이 포함됩니다:\n    1. feature_version_n: 피처 엔지니어링의 다양한 버전을 구현합니다. \n       이 메서드들은 데이터에 대한 고유한 변환을 적용하며, 다른 피처 엔지니어링 버전의 결과를 결합할 수도 있습니다.\n    2. transform: 모든 피처 엔지니어링 버전의 결과를 결합하여 최종적으로 통합된 데이터 세트를 생성하고 반환합니다.\n\n    feature_version_n 메서드의 'args' 매개변수에 대한 설명:\n    - 'args'는 가변 인자로, 다른 피처 엔지니어링 버전의 결과를 전달하는 데 사용됩니다.\n    - 예를 들어, feature_version_2 메서드가 feature_version_0의 결과를 필요로 하는 경우, \n      feature_version_2(feature_version_0()) 형태로 호출할 수 있습니다.\n    - 이런 방식으로 'args'를 사용하면, 하나의 피처 엔지니어링 버전이 다른 버전의 결과를 참조하고 활용할 수 있습니다.\n\n    주의: 이 클래스는 원본 데이터를 직접 수정하지 않습니다. 모든 변환은 새로운 데이터 프레임에 적용되며, \n    transform 메서드는 최종적으로 통합된 데이터 세트를 반환합니다.\n    \"\"\"\n\n    def __init__(self, data, infer=False, feature_versions=None, dependencies=None,\n                 base_directory=\"./data/fe_versions\"):\n        self.data = data\n        self.infer = infer\n        self.feature_versions = feature_versions or []\n        self.dependencies = dependencies or {}  # 피처 버전 간 의존성을 정의하는 딕셔너리\n        self.base_directory = base_directory\n        if not os.path.exists(self.base_directory):\n            os.makedirs(self.base_directory)\n\n    def _save_to_parquet(self, df, version_name):\n        file_path = os.path.join(self.base_directory, f\"{version_name}.parquet\")\n        df.to_parquet(file_path)\n        print(f\"Saved {version_name} to {file_path}\")\n\n    def _load_from_parquet(self, version_name):\n        file_path = os.path.join(self.base_directory, f\"{version_name}.parquet\")\n        if os.path.exists(file_path):\n            return pq.read_table(file_path).to_pandas()\n        else:\n            raise FileNotFoundError(f\"File {file_path} not found.\")\n\n    \"\"\"\n    def feature_version_n(self, *args):\n        \"\"\"\"\"\"\n        이 메서드는 피처 엔지니어링을 위한 예제 버전으로, 새로운 피처들을 데이터 프레임에 추가합니다.\n    \n        과정은 다음과 같습니다:\n        1. 빈 데이터 프레임 생성: 원본 데이터(self.data)의 인덱스를 기반으로 빈 데이터 프레임(df)을 생성합니다.\n        2. 새 피처 추가: 원본 데이터의 'feature' 컬럼에 1을 더하여 새 피처를 'new_feature'라는 이름으로 df에 추가합니다.\n        3. 다른 피처 의존성 추가: args[0]에서 'feature' 값을 가져와 원본 데이터의 'feature_2'에 더하고, 이를 'new_feature_2'라는 이름으로 df에 추가합니다.\n           이 단계에서 args[0]는 이 메서드의 첫 번째 인자로, 다른 피처 엔지니어링 버전의 결과를 나타냅니다.\n        4. 결측값 처리: df의 모든 결측값을 -999로 채웁니다.\n    \n        이 메서드는 데이터 변환 및 가공 과정에서 다른 피처 엔지니어링 메서드와 함께 사용될 수 있으며, \n        머신 러닝 모델의 입력으로 사용될 수 있는 가공된 피처 세트를 생성합니다.\n    \n        반환 값: 가공된 피처를 포함하는 데이터 프레임(df)\n        \"\"\"\"\"\"\n        # create empty dataframe\n        df = pd.DataFrame(index=self.data.index)\n        # add new feature to df\n        df[\"new_feature\"] = self.data[\"feature\"] + 1\n        # add new feature that depends on other feature\n        df[\"new_feature_2\"] = self.data[\"feature_2\"] + args[0][\"feature\"]\n        # fill nan values with -999\n        df = df.fillna(-999)\n        return df\n    \"\"\"\n\n    @staticmethod\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def generate_global_features(data):\n        global_features[\"version_0\"] = {\n            \"median_size\": data.groupby(\"stock_id\")[\"bid_size\"].median() + data.groupby(\"stock_id\")[\n                \"ask_size\"].median(),\n            \"std_size\": data.groupby(\"stock_id\")[\"bid_size\"].std() + data.groupby(\"stock_id\")[\"ask_size\"].std(),\n            \"ptp_size\": data.groupby(\"stock_id\")[\"bid_size\"].max() - data.groupby(\"stock_id\")[\"bid_size\"].min(),\n            \"median_price\": data.groupby(\"stock_id\")[\"bid_price\"].median() + data.groupby(\"stock_id\")[\n                \"ask_price\"].median(),\n            \"std_price\": data.groupby(\"stock_id\")[\"bid_price\"].std() + data.groupby(\"stock_id\")[\"ask_price\"].std(),\n            \"ptp_price\": data.groupby(\"stock_id\")[\"bid_price\"].max() - data.groupby(\"stock_id\")[\"ask_price\"].min(),\n            # \"min_size\": data.groupby('stock_id')['bid_size'].min() + data.groupby('stock_id')['ask_size'].min(),\n            # \"max_size\": data.groupby('stock_id')['bid_size'].max() + data.groupby('stock_id')['ask_size'].max()\n        }\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_selection(self, data, exclude_columns):\n        # 제외할 컬럼을 뺀 나머지로 구성된 새로운 DataFrame을 생성합니다.\n        selected_columns = [c for c in data.columns if c not in exclude_columns]\n        data = data[selected_columns]\n        return data\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_time(self, *args, version_name=\"feature_version_time\"):\n        # feature engineering version 0\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n\n        df[\"dow\"] = data[\"date_id\"] % 5\n        # df[\"dom\"] = data[\"date_id\"] % 20\n        df[\"seconds\"] = data[\"seconds_in_bucket\"] % 60\n        df[\"minute\"] = data[\"seconds_in_bucket\"] // 60\n        df['time_to_market_close'] = 540 - data['seconds_in_bucket']\n\n        for key, value in global_features[\"version_0\"].items():\n            df[f\"global_{key}\"] = data[\"stock_id\"].map(value.to_dict())\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_1(self, *args, version_name=\"feature_version_alvin_1\"):\n        # feature engineering version 1\n        # create empty dataframe\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n\n        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n        df[\"volume\"] = data.eval(\"ask_size + bid_size\")\n        df[\"mid_price\"] = data.eval(\"(ask_price + bid_price) / 2\")\n        df[\"liquidity_imbalance\"] = data.eval(f\"(bid_size-ask_size)/(bid_size+ask_size+{EPS})\")\n        df[\"matched_imbalance\"] = data.eval(f\"(imbalance_size-matched_size)/(matched_size+imbalance_size+{EPS})\")\n        df[\"size_imbalance\"] = data.eval(f\"bid_size / ask_size+{EPS}\")\n\n        for c in combinations(prices, 2):\n            # df[f'{c[0]}_minus_{c[1]}'] = (data[f'{c[0]}'] - data[f'{c[1]}']).astype(np.float32)  # added\n            df[f\"{c[0]}_{c[1]}_imb\"] = data.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]} + {EPS})\")\n\n        for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n            triplet_feature = calculate_triplet_imbalance_numba(c, data)\n            df[triplet_feature.columns] = triplet_feature.values\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_2_0(self, *args, version_name=\"feature_version_alvin_2_0\"):\n        # feature engineering version 2_0\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n\n        df[\"imbalance_momentum\"] = data.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / (data[\n                                                                                                       'matched_size'] + EPS)\n        df[\"price_spread\"] = data[\"ask_price\"] - data[\"bid_price\"]\n        # temporary concat for groupby(stock_id)\n        temp_df = pd.concat([data, df], axis=1)\n        df[\"spread_intensity\"] = temp_df.groupby(['stock_id'])['price_spread'].diff()\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    #  dependency: \"feature_version_imbalance_1\", \"feature_version_imbalance_2_0\"\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_2_1(self, *args, version_name=\"feature_version_alvin_2_1\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n        df['price_pressure'] = self.data['imbalance_size'] * (self.data['ask_price'] - self.data['bid_price'])\n        df['market_urgency'] = args[1]['price_spread'] * args[0]['liquidity_imbalance']\n        df['depth_pressure'] = (self.data['ask_size'] - self.data['bid_size']) * (\n                self.data['far_price'] - self.data['near_price'])\n\n        for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n            df[f\"all_prices_{func}\"] = self.data[prices].agg(func, axis=1)\n            df[f\"all_sizes_{func}\"] = self.data[sizes].agg(func, axis=1)\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_3(self, *args, version_name=\"feature_version_alvin_3\"):\n        # feature engineering version 3\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n            for window in [1, 3, 5, 10]:\n                df[f\"{col}_shift_{window}\"] = data.groupby('stock_id')[col].shift(window)\n                # df[f\"{col}_ret_{window}\"] = data.groupby('stock_id')[col].pct_change(window)\n                df[f\"{col}_ret_{window}\"] = data.groupby('stock_id')[col].pct_change(window)\n                # df[f\"{col}_ret_{window}\"] = data.groupby('stock_id')[col].transform(\n                #     lambda x: custom_pct_change(x, window))\n\n        for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n            for window in [1, 3, 5, 10]:\n                df[f\"{col}_diff_{window}\"] = data.groupby(\"stock_id\")[col].diff(window)\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_4_0(self, *args, version_name=\"feature_version_alvin_4_0\"):\n        # feature engineering version 4\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n            for window in [1, 2, 3, 10]:\n                # SMA 1, 2, 3, 10 - 이동 평균\n                df[f\"{col}_sma_{window}\"] = data.groupby('stock_id')[col].rolling(window=window).mean().values\n                # EMA 1, 2, 3, 10 - 지수 이동 평균\n                df[f\"{col}_ema_{window}\"] = data.groupby('stock_id')[col].ewm(span=window).mean().values\n                # WMA 1, 2, 3, 10 - 가중 이동 평균\n                weights = np.arange(1, window + 1)\n                df[f\"{col}_wma_{window}\"] = data.groupby('stock_id')[col].rolling(window=window).apply(\n                    lambda x: np.dot(x, weights) / weights.sum(), raw=True).values\n                # Volatility 1, 2, 3, 10 - 변동성\n                df[f\"{col}_volatility_{window}\"] = data.groupby('stock_id')[col].rolling(\n                    window=window).std().reset_index(\n                    level=0, drop=True)\n                # Price Range 1, 2, 3, 10 - 가격 범위\n                rolling_max = data.groupby('stock_id')[col].rolling(window=window).max().reset_index(level=0,\n                                                                                                     drop=True)\n                rolling_min = data.groupby('stock_id')[col].rolling(window=window).min().reset_index(level=0,\n                                                                                                     drop=True)\n                df[f\"{col}_range_{window}\"] = rolling_max - rolling_min\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_4_1(self, *args, version_name=\"feature_version_alvin_4_1\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n            for window in [1, 2, 3, 10]:\n                # SMA 1, 2, 3, 10 - 이동 평균\n                df[f\"{col}_sma_{window}\"] = data.groupby('stock_id')[col].rolling(window=window).mean().values\n                # EMA 1, 2, 3, 10 - 지수 이동 평균\n                df[f\"{col}_ema_{window}\"] = data.groupby('stock_id')[col].ewm(span=window).mean().values\n                # WMA 1, 2, 3, 10 - 가중 이동 평균 \n                weights = np.arange(1, window + 1)\n                df[f\"{col}_wma_{window}\"] = data.groupby('stock_id')[col].rolling(window=window).apply(\n                    lambda x: np.dot(x, weights) / weights.sum(), raw=True).values\n                # Volatility 1, 2, 3, 10 - 변동성\n                df[f\"{col}_volatility_{window}\"] = data.groupby('stock_id')[col].rolling(\n                    window=window).std().reset_index(\n                    level=0, drop=True)\n                # Price Range 1, 2, 3, 10 - 가격 범위\n                rolling_max = data.groupby('stock_id')[col].rolling(window=window).max().reset_index(level=0,\n                                                                                                     drop=True)\n                rolling_min = data.groupby('stock_id')[col].rolling(window=window).min().reset_index(level=0,\n                                                                                                     drop=True)\n                df[f\"{col}_range_{window}\"] = rolling_max - rolling_min\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_4_2(self, *args, version_name=\"feature_version_alvin_4_2\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        for col in ['far_price', 'near_price', 'wap']:\n            for window in [1, 2, 3, 10]:\n                # SMA 1, 2, 3, 10 - 이동 평균\n                df[f\"{col}_sma_{window}\"] = data.groupby('stock_id')[col].rolling(window=window).mean().values\n                # EMA 1, 2, 3, 10 - 지수 이동 평균\n                df[f\"{col}_ema_{window}\"] = data.groupby('stock_id')[col].ewm(span=window).mean().values\n                # WMA 1, 2, 3, 10 - 가중 이동 평균\n                weights = np.arange(1, window + 1)\n                df[f\"{col}_wma_{window}\"] = data.groupby('stock_id')[col].rolling(window=window).apply(\n                    lambda x: np.dot(x, weights) / weights.sum(), raw=True).values\n                # Volatility 1, 2, 3, 10 - 변동성\n                df[f\"{col}_volatility_{window}\"] = data.groupby('stock_id')[col].rolling(\n                    window=window).std().reset_index(\n                    level=0, drop=True)\n                # Price Range 1, 2, 3, 10 - 가격 범위\n                rolling_max = data.groupby('stock_id')[col].rolling(window=window).max().reset_index(level=0,\n                                                                                                     drop=True)\n                rolling_min = data.groupby('stock_id')[col].rolling(window=window).min().reset_index(level=0,\n                                                                                                     drop=True)\n                df[f\"{col}_range_{window}\"] = rolling_max - rolling_min\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_5(self, *args, version_name=\"feature_version_alvin_5\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        for window in [1, 2, 3, 10]:\n            delta = data.groupby('stock_id')['wap'].diff()\n\n            gain = (delta.where(delta > 0, 0))\n            gain = gain.groupby(data['stock_id']).rolling(window=window).mean().reset_index(level=0, drop=True)\n\n            loss = (-delta.where(delta < 0, 0))\n            loss = loss.groupby(data['stock_id']).rolling(window=window).mean().reset_index(level=0, drop=True)\n\n            rs = gain / loss\n            df[f'RSI_{window}'] = 100 - (100 / (1 + rs))\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_6_0(self, *args, version_name=\"feature_version_imbalance_6\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        df[\"stock_weights\"] = self.data[\"stock_id\"].map(weights)\n\n        return df\n\n    # dependency: feature_version_imbalance_6_0 : df['stock_weights']\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_6_1(self, *args, version=\"feature_version_imbalance_6_1\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        df[\"weighted_wap\"] = args[0][\"stock_weights\"] * self.data[\"wap\"]\n        df['wap_momentum'] = df.groupby(self.data['stock_id'])['weighted_wap'].pct_change(periods=6)\n\n        return df\n\n    # dependency: feature_version_imbalance_1: df['mid_price']\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_7(self, *args, version_name=\"feature_version_imbalance_7\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        # 데이터 복사 제거 및 직접 self.data 사용\n        df['spread_depth_ratio'] = (self.data['ask_price'] - self.data['bid_price']) / (\n                self.data['bid_size'] + self.data['ask_size'])\n        df['mid_price_movement'] = args[0]['mid_price'].diff(periods=5).apply(\n            lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n        df['micro_price'] = ((self.data['bid_price'] * self.data['ask_size']) + (\n                self.data['ask_price'] * self.data['bid_size'])) / (self.data['bid_size'] + self.data['ask_size'])\n        df['relative_spread'] = (self.data['ask_price'] - self.data['bid_price']) / self.data['wap']\n\n        return df\n\n    # dependency: feature_version_imbalance_3 : df['ask_price_diff_*'] and df['bid_price_diff_*'] and df['ask_size_diff_*'] and df['bid_size_diff_*']\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_8(self, *args, version_name=\"feature_version_imbalance_8\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        for window in [3, 5, 10]:\n            bid_price_diff = args[0][f'bid_price_diff_{window}']\n            ask_price_diff = args[0][f'ask_price_diff_{window}']\n            bid_size_diff = args[0][f'bid_size_diff_{window}']\n            ask_size_diff = args[0][f'ask_size_diff_{window}']\n\n            df[f'price_change_diff_{window}'] = bid_price_diff - ask_price_diff\n            df[f'size_change_diff_{window}'] = bid_size_diff - ask_size_diff\n\n        return df\n\n    # dependency: feature_version_imbalance_3 : df['ask_price_diff_*'] and df['bid_price_diff_*'] and df['ask_size_diff_*'] and df['bid_size_diff_*']\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_9(self, *args, version_name=\"feature_version_imbalance_9\"):\n        # Convert from pandas to Polars\n        data = self.data.copy()\n        for col in args[0].columns:\n            data[col] = args[0][col]\n        pl_df = pl.from_pandas(data)\n\n        #Define the windows and columns for which you want to calculate the rolling statistics\n        windows = [3, 5, 10]\n        columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n\n        # prepare the operations for each column and window\n        group = [\"stock_id\"]\n        expressions = []\n\n        # Loop over each window and column to create the rolling mean and std expressions\n        for window in windows:\n            for col in columns:\n                rolling_mean_expr = (\n                    pl.col(f\"{col}_diff_{window}\")\n                    .rolling_mean(window)\n                    .over(group)\n                    .alias(f'rolling_diff_{col}_{window}')\n                )\n\n                rolling_std_expr = (\n                    pl.col(f\"{col}_diff_{window}\")\n                    .rolling_std(window)\n                    .over(group)\n                    .alias(f'rolling_std_diff_{col}_{window}')\n                )\n\n                expressions.append(rolling_mean_expr)\n                expressions.append(rolling_std_expr)\n\n        # Run the operations using Polars' lazy API\n        lazy_df = pl_df.lazy().with_columns(expressions)\n\n        # Execute the lazy expressions and overwrite the pl_df variable\n        pl_df = lazy_df.collect()\n\n        # Convert back to pandas if necessary\n        pl_to_df = pl_df.to_pandas()\n        import gc\n        gc.collect()\n\n        # get the columns that we want rolling_diff_* and rolling_std_diff_* and drop the rest\n        rolling_diff_columns = [col for col in pl_to_df.columns if 'rolling_diff_' in col]\n        rolling_std_diff_columns = [col for col in pl_to_df.columns if 'rolling_std_diff_' in col]\n        df = pl_to_df[rolling_diff_columns + rolling_std_diff_columns]\n\n        del data\n        gc.collect()\n\n        return df\n\n    # dependency: feature_version_imbalance_1:  df['volume'] ,feature_version_imbalance_7 : df['mid_price_movement']\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_10(self, *args, version_name=\"feature_version_imbalance_10\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        # 데이터 복사 생략 및 벡터화된 연산 사용\n        df['mid_price*volume'] = args[1]['mid_price_movement'] * args[0]['volume']\n        df['harmonic_imbalance'] = 2 / ((1 / self.data['bid_size']) + (1 / self.data['ask_size']))\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_11(self, *args, version_name=\"feature_version_imbalance_11\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        # 데이터 복사 생략\n        df['bid_plus_ask_sizes'] = self.data['bid_size'] + self.data['ask_size']\n\n        # map 연산을 하나로 통합\n        size_features = global_features[\"version_0\"]\n        for feature in ['median_size', 'std_size', 'min_size', 'max_size']:\n            df[feature] = self.data['stock_id'].map(size_features[feature].to_dict())\n\n        df['high_volume'] = np.where(df['bid_plus_ask_sizes'] > df['median_size'], 1, 0)\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_imbalance_12(self, *args, version_name=\"feature_version_imbalance_12\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        # Perform calculations using vectorized operations\n        df['imb_s1'] = (self.data['bid_size'] - self.data['ask_size']) / (self.data['bid_size'] + self.data['ask_size'])\n        df['imb_s2'] = (self.data['imbalance_size'] - self.data['matched_size']) / (\n                self.data['matched_size'] + self.data['imbalance_size'])\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_order_flow(self, *args, version_name=\"feature_version_imbalance_order_flow\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        # Perform calculations using vectorized operations\n        bid_price_diff = self.data['bid_price'].diff()\n        ask_price_diff = self.data['ask_price'].diff()\n        bid_size_diff = self.data['bid_size'].diff()\n        ask_size_diff = self.data['ask_size'].diff()\n\n        ofi = ((bid_price_diff > 0) | (ask_price_diff < 0)) * bid_size_diff \\\n              - ((bid_price_diff < 0) | (ask_price_diff > 0)) * ask_size_diff\n\n        # Assign OFI to the DataFrame and fill NaN values with 0\n        df['OFI'] = ofi.fillna(0)\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_custom_weight(self, *args, version_name=\"feature_version_custom_weight\"):\n        def get_stock_weight(data_batch):\n            matched_volume = data_batch['matched_size'] * data_batch['wap']\n            total_vol = matched_volume.sum()\n            weights = matched_volume / total_vol\n            data_batch['weights'] = weights\n            return data_batch\n\n        if MODE != \"inference\":\n            # Group by 'time_id' with as_index=False to keep 'time_id' as a column\n            data = self.data.groupby('time_id', as_index=False).apply(get_stock_weight)\n\n            # Reset index after groupby operation\n            data.reset_index(drop=True, inplace=True)\n            return data['weights'].to_frame()\n        else:\n            data = self.data.copy()\n            return_value = get_stock_weight(data)['weights'].to_frame()\n\n            del data\n            import gc\n            gc.collect()\n\n            return return_value\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_ta_indicators_1(self, *args, version_name=\"feature_version_ta_indicators_1\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        # Price Relative to Matched Size\n        df['price_relative_to_matched_size'] = data['reference_price'] / data['matched_size']\n        # Price Volatility\n        df['price_volatility'] = args[1]['price_spread'] / args[0]['mid_price']\n        # Price Change per Second\n        df['price_change_per_second'] = data.groupby(\"stock_id\")['reference_price'].diff() / \\\n                                        data.groupby(\"stock_id\")['seconds_in_bucket'].diff()\n        # WAP to Reference Price Ratio\n        df['wap_to_reference_price_ratio'] = data['wap'] / data['reference_price']\n        # Matched Size as % of Total Volume\n        df['matched_size_percent_of_total_volume'] = data['matched_size'] / (\n                data['bid_size'] + data['ask_size']) * 100\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_ta_indicators_2(self, *args, version_name=\"feature_version_ta_indicators_2\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        # Weighted Price Spread\n        df['weighted_price_spread'] = args[0]['price_spread'] * (data['bid_size'] + data['ask_size']) / (\n                data['bid_size'] * data['ask_size'])\n        # Imbalance Ratio\n        df['imbalance_ratio'] = data['imbalance_size'] / data['matched_size']\n        # Price Movement Indicator\n        df['price_movement_indicator'] = data['reference_price'] - data['wap']\n        # Bid-Ask Volume Ratio\n        df['bid_ask_volume_ratio'] = data['bid_size'] / data['ask_size']\n        # Relative Size to WAP\n        df['relative_size_to_wap'] = data['matched_size'] / data['wap']\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_version_ta_indicators_3(self, *args, version_name=\"feature_version_ta_indicators_3\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        # Order Book Imbalance\n        df['order_book_imbalance'] = data['bid_size'] - data['ask_size']\n        # Relative Imbalance\n        df['relative_imbalance'] = df['order_book_imbalance'] / (data['bid_size'] + data['ask_size'])\n        # Inventory Risk - Change in imbalance size (simple difference)\n        df['inventory_risk'] = data.groupby('stock_id')['imbalance_size'].diff().fillna(0)\n        # Market Order Likelihood - Placeholder calculation as an example\n        # In a real-world scenario, this would require a more sophisticated model\n        df['market_order_likelihood'] = data['imbalance_buy_sell_flag'] * (\n                data['bid_size'] + data['ask_size'])\n        # Limit Order Partial Execution Risk\n        # Assuming a higher risk when matched size is lower than imbalance size\n        df['limit_order_partial_exec_risk'] = data['imbalance_size'] - data['matched_size']\n        # Poissonian Trade Model Feature - Placeholder\n        # In reality, this would require fitting a Poisson model to historical trade data\n        # Here, we use a simple ratio of bid and ask sizes as a proxy\n        df['poissonian_trade_model'] = (data['bid_size'] + data['ask_size']) / data[\n            'seconds_in_bucket'].replace(0, 1)\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_market_cap(self, *args, version_name=\"feature_stock_info\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        df = get_stock_info(df, data, \"Market Cap\")\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_sector(self, *args, version_name=\"feature_stock_info\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        df = get_stock_info(df, data, \"Sector\")\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_industry(self, *args, version_name=\"feature_stock_info\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        data = self.data.copy()\n        df = get_stock_info(df, data, \"Industry\")\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def feature_real_price(self, *args, version_name=\"feature_real_price\"):\n        df = pd.DataFrame(index=self.data.index)\n\n        df['real_price'] = pd.NA\n        df['estimated_price_diff'] = pd.NA\n        df['estimated_price'] = pd.NA\n        df['tick_size_min'] = pd.NA\n        df['quantity'] = pd.NA\n\n        data = self.data.copy()\n\n        data[\"bid_price_diff\"] = data.groupby([\"date_id\", \"stock_id\"]).bid_price.diff().abs()\n        data[\"ask_price_diff\"] = data.groupby([\"date_id\", \"stock_id\"]).ask_price.diff().abs()\n        tick_size_est_1 = data[data.bid_price_diff > 0].groupby(\n            [\"date_id\", \"stock_id\"]).bid_price_diff.min().rename(\"tick_est_bid\").to_frame()\n        tick_size_est_2 = data[data.ask_price_diff > 0].groupby(\n            [\"date_id\", \"stock_id\"]).ask_price_diff.min().rename(\"tick_est_ask\")\n        tick_size_est = tick_size_est_1.join(tick_size_est_2)\n        tick_size_est[\"tick_size_min\"] = tick_size_est.min(1)\n\n        data = data.reset_index().merge(tick_size_est, on=[\"date_id\", \"stock_id\"], how=\"left\").set_index('index')\n\n        for (stock_id, date_id), group in tqdm(data.groupby(['stock_id', 'date_id']), desc=\"Processing stock_id\",\n                                               disable=MODE == \"inference\"):\n            try:\n                tick_size_min = group[\"tick_size_min\"].iloc[0]\n                estimated_price = group['ask_price'] / tick_size_min * 0.01\n                estimated_price_diff = estimated_price.diff().abs().fillna(0) * 100\n                results = group.apply(\n                    lambda x: find_exact_two_decimal_divisors(x['ask_size'], estimated_price[x.name]), axis=1)\n                quantity, integer_validated_value = zip(*results)\n\n                df.loc[group.index, 'tick_size_min'] = tick_size_min\n                df.loc[group.index, 'estimated_price'] = estimated_price\n                df.loc[group.index, 'estimated_price_diff'] = estimated_price_diff\n                df.loc[group.index, 'quantity'] = quantity\n                df.loc[group.index, 'integer_validated_value'] = integer_validated_value\n\n                for i in range(1, len(group)):\n                    prev_val = integer_validated_value[i - 1]\n                    curr_val = integer_validated_value[i]\n                    estimated_diff = int(estimated_price_diff.iloc[i]) * 0.01\n\n                    if abs(curr_val - prev_val) > estimated_diff:\n                        if abs((curr_val + estimated_diff) - prev_val) < abs(\n                                (curr_val - estimated_diff) - prev_val):\n                            df.at[group.index[i], 'integer_validated_value'] = curr_val + estimated_diff\n                        else:\n                            df.at[group.index[i], 'integer_validated_value'] = curr_val - estimated_diff\n\n            except Exception as e:\n                # print(e)\n                continue\n\n        for col in df.columns:\n            if df[col].dtype not in ['float64', 'int64', 'bool']:\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n\n        del data\n        import gc\n        gc.collect()\n\n        return df\n\n    # you can add more feature engineering version like above\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def execute_feature_versions(self, save=False, load=False):\n        results = {}\n\n        for version in self.feature_versions:\n            if load:\n                df = self._load_from_parquet(version)\n            else:\n                method = getattr(self, version, None)\n                if callable(method):\n                    args = []\n                    for dep in self.dependencies.get(version, []):\n                        dep_result = results.get(dep)\n                        if isinstance(dep_result, pd.DataFrame):\n                            args.append(dep_result)\n                        elif dep_result is None and hasattr(self, dep):\n                            dep_method = getattr(self, dep)\n                            dep_result = dep_method()\n                            results[dep] = dep_result\n                            args.append(dep_result)\n                        else:\n                            args.append(None)\n                    df = method(*args)\n                    if save:\n                        self._save_to_parquet(df, version)\n            results[version] = df\n\n        # return that was in self.feature_versions\n        return {k: v for k, v in results.items() if k in self.feature_versions}\n\n    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n    def transform(self, save=False, load=False):\n        feature_versions_results = self.execute_feature_versions(save=save, load=load)\n        if not self.infer:\n            self.data[\"date_id_copy\"] = self.data[\"date_id\"]\n        concat_df = pd.concat([self.data] + list(feature_versions_results.values()), axis=1)\n\n        exclude_columns = [\"row_id\", \"time_id\", \"date_id\"]\n        final_data = self.feature_selection(concat_df, exclude_columns)\n        return final_data\n","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.465696Z","start_time":"2023-12-13T13:27:42.423745Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:31.857950Z","iopub.execute_input":"2023-12-19T23:29:31.858899Z","iopub.status.idle":"2023-12-19T23:29:32.351158Z","shell.execute_reply.started":"2023-12-19T23:29:31.858856Z","shell.execute_reply":"2023-12-19T23:29:32.349700Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Model Class","metadata":{"papermill":{"duration":0.017633,"end_time":"2023-11-09T03:16:26.985144","exception":false,"start_time":"2023-11-09T03:16:26.967511","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n\nclass OptunaWeights:\n    def __init__(self, random_state, n_trials=5000):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n        self.n_trials = n_trials\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        #         weights = [trial.suggest_float(f\"weight{n}\", -2, 3) for n in range(len(y_preds))]\n        weights = [max(0, trial.suggest_float(f\"weight{n}\", -2, 3)) for n in range(len(y_preds))]\n        # Calculate the weighted prediction\n        if sum(weights) == 0:\n            num_models = len(y_preds)\n            weights = [1 / num_models] * num_models\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n        auc_score = mean_absolute_error(y_true, weighted_pred)\n        #         log_loss_score=log_loss(y_true, weighted_pred)\n        return auc_score  #/log_loss_score\n\n    def fit(self, y_true, y_preds):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        pruner = optuna.pruners.HyperbandPruner()\n        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\",\n                                         direction='maximize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds):\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)\n\n    def weights(self):\n        return self.weights","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.483753Z","start_time":"2023-12-13T13:27:42.444605Z"},"collapsed":false,"papermill":{"duration":0.04399,"end_time":"2023-11-09T03:16:27.048011","exception":false,"start_time":"2023-11-09T03:16:27.004021","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:32.353702Z","iopub.execute_input":"2023-12-19T23:29:32.354143Z","iopub.status.idle":"2023-12-19T23:29:32.370405Z","shell.execute_reply.started":"2023-12-19T23:29:32.354112Z","shell.execute_reply":"2023-12-19T23:29:32.369108Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# V3 ModlePipeline\nclass ModelPipeline:\n    def __init__(self, lgb_model_weights=None):\n        self.models = []\n        self.models_list = []\n\n        self.predictions = []\n        self.predictions_list = []  # for single stacking\n\n        self.inference_prediction = None\n\n        self.optuna_weights = []\n\n        self.lgb_model_weights = lgb_model_weights\n\n    @staticmethod\n    def _lgb_train(model, X_train, y_train, X_valid, y_valid):\n        fit_params = {\n            \"callbacks\": [lgb.callback.log_evaluation(period=100)]\n        }\n        if X_valid is not None and y_valid is not None:\n            fit_params[\"eval_set\"] = [(X_valid, y_valid)]\n            fit_params[\"callbacks\"].append(lgb.callback.early_stopping(stopping_rounds=100))\n    \n        model.fit(X_train, y_train, **fit_params)\n        return model\n\n\n    @staticmethod\n    def _xgb_train(model, X_train, y_train, X_valid, y_valid):\n        fit_params = {\n            \"eval_set\": [(X_valid, y_valid)],\n            \"eval_metric\": \"mae\",\n            \"verbose\": 100,\n            \"early_stopping_rounds\": 100\n        }\n        model.fit(X_train, y_train, **fit_params)\n        return model\n\n    @staticmethod\n    def _cnn_train(model, X_train, y_train, X_valid, y_valid):\n        pass\n\n    def train(self, idx, X_train, y_train, X_valid, y_valid):\n        self.models = []  # each fold 에서 모델 저장\n        for model_name in config[\"model_name\"]:\n            model_cls = models_config[model_name][\"model\"]\n            params = models_config[model_name][\"params\"]\n            model = model_cls(**params)\n            print(f\"\\n\\n================== Training {model_name} ({idx}/{config['n_splits']})==================\")\n            if \"lgb\" in model_name:\n                trained_model = self._lgb_train(model, X_train, y_train, X_valid, y_valid)\n            elif \"xgb\" in model_name:\n                trained_model = self._xgb_train(model, X_train, y_train, X_valid, y_valid)\n            elif model_name == \"pytorch_cnn\":\n                trained_model = self._cnn_train(model, X_train, y_train, X_valid, y_valid)\n            else:\n                raise ValueError(\"Invalid model name\")\n            self.models.append(trained_model)\n            print(f\"Successfully trained {model_name} ({idx}/{config['n_splits']})\")\n        self.models_list.append(self.models)  # 전체 fold 에서 모델 저장\n\n    def predict(self, idx, X_test):\n        self.predictions = []  # each fold 에서 모델 예측값 저장\n        self.inference_prediction = None\n        self.models = self.models_list[idx]  # 각 fold 에서 모델 불러 오기\n        for model_name, model in zip(config[\"model_name\"], self.models):\n            print(\n                f\"\\n\\n================== Predict each model {model_name} ({idx}/{config['n_splits']})==================\") if MODE != \"inference\" else None\n            prediction = model.predict(X_test)  # 각 모델 예측\n            self.predictions.append(prediction)\n        self.inference_prediction = np.mean(self.predictions, axis=0) * lgb_model_weights[idx]  # non stacking\n        self.predictions_list.append(self.predictions[0]) if config[\"stacking_mode\"] and len(\n            config[\"model_name\"]) == 1 else None  # for single stacking\n        print(f\"Successfully predicted {model_name} ({idx}/{config['n_splits']})\") if MODE != \"inference\" else None\n\n    def _optuna_stacking(self, idx, y_test, X_test, infer):\n        optuna = OptunaWeights(random_state=config[\"optuna_random_state\"])\n        if infer:\n            self.inference_prediction = None\n            optuna.weights = self.optuna_weights[idx]\n            if idx == -1:\n                self.inference_prediction = optuna.predict(self.predictions_list)\n                self.predictions_list = []\n            else:\n                self.inference_prediction = optuna.predict(self.predictions)\n        else:\n            if idx == -1:  # single model stacking\n                #  predict each model and add to self.prediction with y_test]\n                self.predictions = []\n                for idx, models in enumerate(self.models_list):\n                    for model_name, model in zip(config[\"model_name\"], models):\n                        print(\n                            f\"\\n\\n================== Predict each model for stacking {model_name} ({idx}/{config['n_splits']})==================\")\n                        self.predictions.append(model.predict(X_test))\n                        print(f\"Successfully predicted for stacking {model_name} ({idx}/{config['n_splits']})\")\n                idx = -1\n            print(f\"\\n\\n================== Stacking ({idx}/{config['n_splits']})==================\")\n            y_test_pred = optuna.fit_predict(y_test.values, self.predictions)\n            score = mean_absolute_error(y_test, y_test_pred)\n            print(f\"Score for stacking ({idx}/{config['n_splits']}): {score}\")\n            self.optuna_weights.append(optuna.weights)\n            print(f\"Successfully stacking ({idx}/{config['n_splits']})\")\n\n    def stacking(self, idx, y_test=None, X_test=None, infer=False):\n        if config[\"stacking_algorithm\"] == \"optuna\":\n            self._optuna_stacking(idx, y_test, X_test, infer=infer)\n        else:\n            raise ValueError(\"Invalid stacking algorithm\")\n\n    def save_models(self):\n        if MODE == \"train\":\n            for idx in range(config[\"n_splits\"] + 1):\n                for n_model, model_name in enumerate(config[\"model_name\"]):\n                    model = self.models_list[idx][n_model]\n                    joblib.dump(model, f\"{config['model_dir']}/{idx}_{model_name}.pkl\")\n                    print(f\"Successfully saved model ({config['model_dir']}/{idx}_{model_name}.pkl)\")\n\n    def save_optuna_weights(self):\n        if MODE == \"train\":\n            if config[\"stacking_mode\"]:\n                joblib.dump(self.optuna_weights, f\"{config['model_dir']}/optuna_weights.pkl\")\n                print(f\"Successfully saved optuna weights ({config['model_dir']}/optuna_weights.pkl)\")\n\n    def load_models(self):  # both 이면 안해도됨\n        if MODE == \"inference\":\n            for idx in range(config[\"n_splits\"] + 1):\n                self.models = []\n                for model_name in config[\"model_name\"]:\n                    model = joblib.load(f\"{config['model_dir']}/{idx}_{model_name}.pkl\")\n                    self.models.append(model)\n                    print(f\"Successfully loaded model ({config['model_dir']}/{idx}_{model_name}.pkl)\")\n                self.models_list.append(self.models)\n\n    def load_optuna_weights(self):\n        if MODE == \"inference\":\n            if config[\"stacking_mode\"]:\n                self.optuna_weights = joblib.load(f\"{config['model_dir']}/optuna_weights.pkl\")\n                print(f\"Successfully loaded optuna weights ({config['model_dir']}/optuna_weights.pkl)\")\n","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.488068Z","start_time":"2023-12-13T13:27:42.467257Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:32.372109Z","iopub.execute_input":"2023-12-19T23:29:32.372485Z","iopub.status.idle":"2023-12-19T23:29:32.402576Z","shell.execute_reply.started":"2023-12-19T23:29:32.372450Z","shell.execute_reply":"2023-12-19T23:29:32.401318Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.014815,"end_time":"2023-11-09T03:16:27.148528","exception":false,"start_time":"2023-11-09T03:16:27.133713","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# ## Main\n## import data","metadata":{"papermill":{"duration":0.016047,"end_time":"2023-11-09T03:16:27.17927","exception":false,"start_time":"2023-11-09T03:16:27.163223","status":"completed"},"tags":[]}},{"cell_type":"code","source":"dependencies = {\n    \"feature_version_imbalance_2_1\": [\"feature_version_imbalance_1\", \"feature_version_imbalance_2_0\"],\n    \"feature_version_imbalance_6_1\": [\"feature_version_imbalance_6_0\"],\n    \"feature_version_imbalance_7\": [\"feature_version_imbalance_1\"],\n    \"feature_version_imbalance_8\": [\"feature_version_imbalance_3\"],\n    \"feature_version_imbalance_9\": [\"feature_version_imbalance_3\"],\n    \"feature_version_imbalance_10\": [\"feature_version_imbalance_1\", \"feature_version_imbalance_7\"],\n}","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.530589Z","start_time":"2023-12-13T13:27:42.47895Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:32.408843Z","iopub.execute_input":"2023-12-19T23:29:32.409919Z","iopub.status.idle":"2023-12-19T23:29:32.420760Z","shell.execute_reply.started":"2023-12-19T23:29:32.409881Z","shell.execute_reply":"2023-12-19T23:29:32.419453Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# # FOR VISUALIZE\n# df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n#\n# # 데이터 전처리\n# data_processor = DataPreprocessor(df)\n# df = data_processor.transform()\n# # Save할 피쳐 엔지니어링 함수 선택\n# feature_engineer = FeatureEngineer(df, feature_versions=[\n# ],\n#                                    dependencies=dependencies)\n# feature_engineer.generate_global_features(df)\n# df = feature_engineer.transform()","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.530761Z","start_time":"2023-12-13T13:27:42.518817Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:32.422471Z","iopub.execute_input":"2023-12-19T23:29:32.422983Z","iopub.status.idle":"2023-12-19T23:29:32.430272Z","shell.execute_reply.started":"2023-12-19T23:29:32.422831Z","shell.execute_reply":"2023-12-19T23:29:32.429441Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# splitter = Splitter(method=config[\"split_method\"], n_splits=config[\"n_splits\"], correct=config[\"correct\"],\n#                     initial_fold_size_ratio=config[\"initial_fold_size_ratio\"],\n#                         train_test_ratio=config[\"train_test_ratio\"], gap=config[\"gap\"])\n# for idx, (X_train, y_train, X_test, y_test) in enumerate(splitter.split(df)):\n#     print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n# splitter.visualize_splits()","metadata":{"ExecuteTime":{"end_time":"2023-12-13T13:27:42.530838Z","start_time":"2023-12-13T13:27:42.51894Z"},"collapsed":false,"papermill":{"duration":0.022352,"end_time":"2023-11-09T03:16:27.254351","exception":false,"start_time":"2023-11-09T03:16:27.231999","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:32.431384Z","iopub.execute_input":"2023-12-19T23:29:32.431737Z","iopub.status.idle":"2023-12-19T23:29:32.440820Z","shell.execute_reply.started":"2023-12-19T23:29:32.431708Z","shell.execute_reply":"2023-12-19T23:29:32.439518Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"lgb_model_weights = weighted_average(config[\"n_splits\"] + 1, equal_weight=False)\nmodel_pipeline = ModelPipeline(lgb_model_weights)\nif config[\"train_mode\"]:\n    # 데이터 불러오기\n\n    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n\n    # 데이터 전처리\n    data_processor = DataPreprocessor(data=df)\n    df = data_processor.transform()\n\n    # 사용할 피쳐 엔지니어링 함수 선택\n    feature_engineer = FeatureEngineer(data=df, feature_versions=[\n        'feature_version_time',\n        'feature_version_imbalance_1',\n        'feature_version_imbalance_2_0',\n        'feature_version_imbalance_2_1',\n        'feature_version_imbalance_3',\n        'feature_version_imbalance_6_0',\n        'feature_version_imbalance_6_1',\n        'feature_version_imbalance_7',\n        'feature_version_imbalance_8',\n        'feature_version_imbalance_9',\n        # 'feature_version_imbalance_10',\n        # 'feature_version_imbalance_11',\n        # 'feature_version_imbalance_12',\n        # 'feature_version_custom_weight',\n        # 'feature_version_order_flow',\n    ],\n                                       dependencies=dependencies)\n    feature_engineer.generate_global_features(data=df)\n    df = feature_engineer.transform(save=True)  # 맨 처음에는 save=True 돌렸으면, 다음부턴 transform(load=True)로 바꾸면된\n    df_copy = df.copy()\n    splitter = Splitter(method=config[\"split_method\"], n_splits=config[\"n_splits\"], correct=config[\"correct\"],\n                        initial_fold_size_ratio=config[\"initial_fold_size_ratio\"],\n                        train_test_ratio=config[\"train_test_ratio\"], gap=config[\"gap\"])\n    for idx, (X_train, y_train, X_test, y_test) in enumerate(splitter.split(data=df, p_gap=5)):\n        print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n        model_pipeline.train(idx=idx, X_train=X_train, y_train=y_train, X_valid=X_test, y_valid=y_test)\n        model_pipeline.predict(idx=idx, X_test=X_test)\n        if config[\"stacking_mode\"] and len(config[\"model_name\"]) > 1:  # 각 폴드마다 stacking\n            model_pipeline.stacking(idx=idx, y_test=y_test)\n    if config[\"stacking_mode\"] and len(config[\"model_name\"]) == 1:  # single model 에 대한 stacking\n        model_pipeline.stacking(idx=-1, y_test=y_test,\n                                X_test=X_test)  # stacking with last fold. if you want you can stacking with all folds\n    # print(df_copy.shape, df_copy[\"target\"].shape)\n    model_pipeline.train(idx=config[\"n_splits\"] + 1, X_train=df_copy.drop(columns=['target', 'date_id_copy']), y_train=df_copy[\"target\"], X_valid=None, y_valid=None)\n    model_pipeline.save_models()\n    model_pipeline.save_optuna_weights()\n    splitter.visualize_splits()","metadata":{"ExecuteTime":{"end_time":"2023-12-13T15:27:36.274698Z","start_time":"2023-12-13T13:27:42.519011Z"},"collapsed":false,"papermill":{"duration":0.026853,"end_time":"2023-11-09T03:16:27.296794","exception":false,"start_time":"2023-11-09T03:16:27.269941","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:29:32.443402Z","iopub.execute_input":"2023-12-19T23:29:32.444083Z","iopub.status.idle":"2023-12-19T23:29:32.457646Z","shell.execute_reply.started":"2023-12-19T23:29:32.444043Z","shell.execute_reply":"2023-12-19T23:29:32.456349Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#far_price 및 near_price의 누락된 값을 채우고 세 가지 결과를 반환\ndef imputer(df):\n    far_price_mean = df['far_price'].mean()\n    near_price_mean = df['near_price'].mean()\n    df['far_price'] = df['far_price'].fillna(far_price_mean)\n    df['near_price'] = df['near_price'].fillna(near_price_mean)\n\n    return df, far_price_mean, near_price_mean\n\n# 결측치 채우기\ndef add_missing_data(df):\n    all_stock_ids = set(range(200))\n    all_missed_data_list = []\n\n    #데이터를 미리 그룹화하여 각 time_id에 관련된 데이터에 빠르게 접근할 수 있도록 한다\n    grouped = df.groupby('time_id')\n\n    for t, group in grouped:\n        current_stock_ids = set(group['stock_id'].to_list())\n        missed_stock_id = list(all_stock_ids - current_stock_ids)\n        \n        date_id = group['date_id'].iloc[-1]\n        seconds_in_bucket = group['seconds_in_bucket'].iloc[-1]\n        \n        missed_stock_id_num = len(missed_stock_id)\n        missed_date_id = [date_id] * missed_stock_id_num\n        missed_seconds_in_bucket = [seconds_in_bucket] * missed_stock_id_num\n        missed_time_id = [t] * missed_stock_id_num\n        \n        missed_data = pd.DataFrame({\n            'stock_id': missed_stock_id,\n            'date_id': missed_date_id,\n            'seconds_in_bucket': missed_seconds_in_bucket,\n            'time_id': missed_time_id\n        })\n        \n        all_missed_data_list.append(missed_data)\n\n    all_missed_data = pd.concat(all_missed_data_list, axis=0).reset_index(drop=True).astype(int)\n\n    df = pd.concat([df, all_missed_data], axis=0)\n    df = df.sort_values(by=['time_id', 'stock_id']).reset_index(drop=True)\n    df = df.groupby('stock_id').apply(lambda x: x.fillna(method='bfill')).reset_index(drop=True)\n\n    return df\n\ndef sizesum_and_pricestd(df):\n    #업데이트 후 10개의 특성 추가\n    price_ftrs = ['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'wap'] # std\n    size_ftrs = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size'] # sum\n    \n    rolled = df[['stock_id'] + size_ftrs].groupby('stock_id').rolling(window=8, min_periods=1).sum()\n    rolled = rolled.reset_index(level=0, drop=True)\n    for col in size_ftrs:\n        df[f'{col}_rolled_sum'] = rolled[col]\n\n    rolled = df[['stock_id'] + price_ftrs].groupby('stock_id').rolling(window=8, min_periods=1).std().fillna(0)\n    rolled = rolled.reset_index(level=0, drop=True)\n    for col in price_ftrs:\n        df[f'{col}_rolled_std'] = rolled[col]\n\n    return df\n\n#리스트 요소 삭제\ndef remove_element(input_list, drop_list):\n    return [e for e in input_list if e not in drop_list]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:29:32.459461Z","iopub.execute_input":"2023-12-19T23:29:32.459869Z","iopub.status.idle":"2023-12-19T23:29:32.473888Z","shell.execute_reply.started":"2023-12-19T23:29:32.459828Z","shell.execute_reply":"2023-12-19T23:29:32.473060Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(f\"{config['data_dir']}/train.csv\")\ntrain = train.loc[train['target'].notna()]\n\ntrain, far_price_mean, near_price_mean = imputer(train)\ntrain = add_missing_data(train)\nprint('결측치：', train.isnull().sum().sum())\n\ntrain = sizesum_and_pricestd(train)\n\nno_feature_cols = ['date_id', 'row_id', 'time_id', 'target', 'currently_scored']\n\nfeature_cols = remove_element(train.columns, no_feature_cols)\ntarget_col = 'target'\n\nprint('피처 수：', len(feature_cols))\n\n#표준화\navg = train[feature_cols].mean()\nstd = train[feature_cols].std()\n\ntrain[feature_cols] = (train[feature_cols] - avg)/std\n\n#데이터를 float32 데이터 타입으로 변환\ntrain = train.astype('float32')\n\nseq_len = 8\n\n# Grouping by time_id\ngrouped_by_time = train.groupby('stock_id')\n\ndef generate_data(grouped_by_time, seq_len):\n    for _, group in grouped_by_time:\n        # Sorting by stock_id to maintain consistency across images\n        group_sorted = group.sort_values(by='time_id')\n        features = group_sorted[feature_cols].values\n        # print('features',features.shape)\n        windows = []\n        ############################################ \n        for t in range(0, seq_len - 1):\n            copy_0 = np.stack([features[0]] * (seq_len - 1 - t))\n            cut_0 = features[: t + 1]\n            windows.append(np.vstack((copy_0, cut_0)))\n            \n        for t in range(0, features.shape[0] - seq_len + 1):\n            windows.append(features[t: t+seq_len, :])\n        ############################################\n        # stock n의 0일~480일 0초 ~540초를 time_id기준으로 정렬했을 때\n        # seq_len 길이만큼을 하나의 시퀀스 데이터로 만들되, 자기 시점 이전의 데이터 행 개수가 seq_len보다 작은 경우\n        # 첫번째 행을 복사.\n        # 예를 들어 seq_len이 5인데 데이터가 100개 있는 경우, 1번 데이터의 시퀀스는 11111\n        # 예를 들어 seq_len이 5인데 데이터가 100개 있는 경우, 2번 데이터의 시퀀스는 11112\n        # 예를 들어 seq_len이 5인데 데이터가 100개 있는 경우, 3번 데이터의 시퀀스는 11123\n        # 이런 식으로 데이터 포인트를 시퀀스로 변환하고 시퀀스로 라벨을 맞추는 모델 만들기\n        \n        # Convert list of windows to numpy array\n        features_array = np.stack(windows)\n        \n        # 시퀀스 형태 확인\n        # print(len(windows),windows[0].shape,windows[0][0].shape)\n        \n        target = group_sorted['target'].values\n        # Yield the result for this group to avoid storing all results in memory\n        yield features_array, target\n\n# Use generator to iterate over data\ndata_generator = generate_data(grouped_by_time, seq_len=seq_len)\n\n# If you need to store results in arrays:\ndatas, labels = zip(*data_generator)\n\nprint(len(datas),datas[0].shape,datas[1].shape)\n\ndata = np.array(datas).reshape(-1, seq_len, len(feature_cols))\nlabel = np.array(labels).reshape(-1,)\nprint('data_seq_to_reshaped', data.shape, 'label_shape', label.shape)\n#del train, datas, labels, grouped_by_time\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('device：', device)\n\n\ndata = torch.tensor(data, dtype=torch.float32).to(device)\nlabel = torch.tensor(label, dtype=torch.float32).to(device)\n\nprint('데이터 형태', data.shape)\nprint('라벨 형태', label.shape)\n\n#train,val set 분리\ntorch.manual_seed(42)\n\ndataset = TensorDataset(data, label)\n\ntrain_ratio = 0.8\ntrain_size = int(train_ratio * len(dataset))\nvalid_size = len(dataset) - train_size\ntrain_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n\nbatch_size = 4096\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n\nprint('batch size：', next(iter(train_loader))[0].shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:29:32.475773Z","iopub.execute_input":"2023-12-19T23:29:32.476126Z","iopub.status.idle":"2023-12-19T23:30:28.489863Z","shell.execute_reply.started":"2023-12-19T23:29:32.476094Z","shell.execute_reply":"2023-12-19T23:30:28.489186Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"결측치： 0\n피처 수： 23\n200 (26455, 8, 23) (26455, 8, 23)\ndata_seq_to_reshaped (5291000, 8, 23) label_shape (5291000,)\ndevice： cpu\n데이터 형태 torch.Size([5291000, 8, 23])\n라벨 형태 torch.Size([5291000])\nbatch size： torch.Size([4096, 8, 23])\n","output_type":"stream"}]},{"cell_type":"code","source":"class MyModel(nn.Module):\n    def __init__(self, feature_num, d_model, nhead, num_layers):\n        super(MyModel, self).__init__()\n        self.embedding = nn.Linear(feature_num, d_model)\n        self.tf0 = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=2, batch_first=True)\n        self.tf1 = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, batch_first=True)\n        # [batch_size, seq_len, d_model] nn.Transformer가 입력받는 포멧\n        self.fc = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(0.5)\n        self.tf2 = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, batch_first=True)\n        self.decoder_0 = nn.Linear(d_model, d_model//2)\n        self.decoder_1 = nn.Linear(d_model//2 , 1)                            \n        self.decoder = nn.Linear(d_model, 1)\n\n        self.ffnn = nn.Sequential(\n            nn.Linear(d_model, 2*d_model),  \n            nn.ReLU(),                  \n            nn.Linear(2*d_model, d_model)  \n        )\n\n    def forward(self, x):\n        # x = self.embedding(x)\n        # x = self.tf1.encoder(x)\n        # x = x[:, -1, :]\n        # x = self.fc(x) \n        # x = self.dropout(x)\n        # x = self.tf2.encoder(x)\n        # x = self.decoder(x)\n\n        x = self.embedding(x)\n        x = self.tf1.encoder(x)\n        x = x[:, -1, :]\n        # x = self.fc(x) \n        x = self.dropout(x)\n        x = self.tf2.encoder(x)\n        x = self.decoder(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:30:28.491449Z","iopub.execute_input":"2023-12-19T23:30:28.491760Z","iopub.status.idle":"2023-12-19T23:30:28.500182Z","shell.execute_reply.started":"2023-12-19T23:30:28.491730Z","shell.execute_reply":"2023-12-19T23:30:28.499273Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"if is_train:\n    input_size = data.shape[-1]\n    print(input_size)\n    n_epochs = 50\n    lr = 1e-03\n\n    # pre mae init\n    pre_epoch_valid_mae = np.inf\n\n    # MAE가 두 번의 에폭에서 감소하지 않으면 학습률을 절반으로 줄인다\"\n    patience_counter = 0\n\n    model = MyModel(feature_num=input_size, d_model=64, nhead=8, num_layers=1).to(device)\n    \n    # optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    loss = nn.L1Loss().to(device)\n\n    # out_path = \"model/\"\n    # if not os.path.exists(out_path):\n    #     os.makedirs(out_path)\n    best_mae = np.inf\n\n    print(f'Train start...')\n    for epoch in range(n_epochs):\n        model.train()\n        train_maes = []\n        batch_num = len(train_loader)\n\n        # 훈련\n        for X, y in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X).squeeze()\n            l = loss(outputs, y)\n            l.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n            optimizer.step()\n            mae = l.item()\n            train_maes.append(mae)\n        epoch_train_mae = np.mean(train_maes)\n        print(f'Epoch [{epoch+1}/{n_epochs}] Training average MAE: {epoch_train_mae:.4f}')\n        train_maes = []\n\n        # 검증\n        model.eval()\n        with torch.no_grad():\n            valid_maes = []\n            for X_v, y_v in valid_loader:\n                preds = model(X_v).squeeze()\n                mae = torch.abs(preds - y_v).mean().item()\n                valid_maes.append(mae)\n            epoch_valid_mae = np.mean(valid_maes)\n            print(f'Epoch [{epoch+1}/{n_epochs}] Validation average MAE: {epoch_valid_mae:.4f}')\n            \n            if epoch_valid_mae < best_mae:\n                best_mae = epoch_valid_mae\n                # torch.save(model, os.path.join(out_path, f\"model_epoch_{epoch+1}.pt\"))\n    \n                now = datetime.datetime.now()\n                time_string = now.strftime(\"%Y%m%d_%H%M\")\n                torch.save(model, f\"{config['model_dir']}/model_epoch_{epoch+1}.pt\")\n                # torch.save(model, f\"transformer_model.pt\")\n                \n        #이전 라운드의 MAE가 현재 MAE보다 개선되지 않으면 학습률을 절반으로 줄인다\"\n        if epoch_valid_mae - pre_epoch_valid_mae > 0:\n            patience_counter += 1\n\n            if patience_counter == 2:\n                lr = lr * 0.75\n                patience_counter = 0\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = lr  # 학습률 업데이트 \n                    print(f'renew lr to {lr}')\n\n        # MAE 업데이트\n        pre_epoch_valid_mae = epoch_valid_mae\n\n        # 분기가 0.03을 초과하거나 학습률이 1e-7보다 낮을 때 훈련을 중단\n        if (epoch_valid_mae - epoch_train_mae > 0.03) or (lr <1e-7):\n            print('Early stop now.')\n            break\n    print(f'Train over.')","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:30:28.501294Z","iopub.execute_input":"2023-12-19T23:30:28.501619Z","iopub.status.idle":"2023-12-19T23:30:28.520412Z","shell.execute_reply.started":"2023-12-19T23:30:28.501591Z","shell.execute_reply":"2023-12-19T23:30:28.518676Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n#整体特征\nmedian_sizes = train.groupby('stock_id')['bid_size'].median() + train.groupby('stock_id')['ask_size'].median()\nstd_sizes = train.groupby('stock_id')['bid_size'].std() + train.groupby('stock_id')['ask_size'].std()\nmax_sizes = train.groupby('stock_id')['bid_size'].max() + train.groupby('stock_id')['ask_size'].max()\nmin_sizes = train.groupby('stock_id')['bid_size'].min() + train.groupby('stock_id')['ask_size'].min()\nmean_sizes = train.groupby('stock_id')['bid_size'].mean() + train.groupby('stock_id')['ask_size'].mean()\nfirst_sizes = train.groupby('stock_id')['bid_size'].first() + train.groupby('stock_id')['ask_size'].first()\nlast_sizes = train.groupby('stock_id')['bid_size'].last() + train.groupby('stock_id')['ask_size'].last()\n#可以再做日期的（好像没看到drop掉日期列）\n\ntrain = train.dropna(subset=['target'])\ntrain.reset_index(drop=True, inplace=True)\n\ndef feature_eng(df):\n    cols = [c for c in df.columns if c not in ['row_id', 'time_id']]\n    df = df[cols]\n    \n    #匹配失败数量和匹配成功数量的比率\n    df['imbalance_ratio'] = df['imbalance_size'] / df['matched_size']\n    #供需市场的差额\n    df['bid_ask_volume_diff'] = df['ask_size'] - df['bid_size']\n    #供需市场总和\n    df['bid_plus_ask_sizes'] = df['bid_size'] + df['ask_size']\n    \n    #供需价格的均值\n    df['mid_price'] = (df['ask_price'] + df['bid_price']) / 2\n    \n    #整体数据情况\n    df['median_size'] = df['stock_id'].map(median_sizes.to_dict())\n    df['std_size'] = df['stock_id'].map(std_sizes.to_dict())\n    df['max_size'] = df['stock_id'].map(max_sizes.to_dict())\n    df['min_size'] = df['stock_id'].map(min_sizes.to_dict())\n    df['mean_size'] = df['stock_id'].map(mean_sizes.to_dict())\n    df['first_size'] = df['stock_id'].map(first_sizes.to_dict())    \n    df['last_size'] = df['stock_id'].map(last_sizes.to_dict())       \n    \n    #整体市场规模和当前的市场规模比较\n    df['high_volume'] = np.where(df['bid_plus_ask_sizes'] > df['median_size'], 1, 0)\n    \n    prices = ['reference_price', 'far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n    \n    #价格之间做差，做差/求和\n    for c in combinations(prices, 2):\n        df[f'{c[0]}_minus_{c[1]}'] = (df[f'{c[0]}'] - df[f'{c[1]}']).astype(np.float32)\n        df[f'{c[0]}_{c[1]}_imb'] = df.eval(f'({c[0]} - {c[1]})/({c[0]} + {c[1]})')\n        \n    for c in combinations(prices, 3):\n        max_ = df[list(c)].max(axis=1)\n        min_ = df[list(c)].min(axis=1)\n        mid_ = df[list(c)].sum(axis=1) - min_ - max_\n        \n        df[f'{c[0]}_{c[1]}_{c[2]}_imb2'] = (max_-mid_)/(mid_-min_ + 1e-4)\n    \n    gc.collect()\n    \n    return df\n\ny = train['target'].values\nX = feature_eng(train.drop(columns='target'))\nX_date_id = X['date_id']\nX = X.drop('date_id',axis=1)\n\ny_min = np.min(y)\ny_max = np.max(y)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:30:28.521596Z","iopub.execute_input":"2023-12-19T23:30:28.521855Z","iopub.status.idle":"2023-12-19T23:31:36.816523Z","shell.execute_reply.started":"2023-12-19T23:30:28.521833Z","shell.execute_reply":"2023-12-19T23:31:36.814570Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def feat_eng_nn(df: pd.DataFrame):\n    # change seconds_in_bucket to 9 categories (9 min) & make a new col\n    df[\"stage\"] = np.where(df[\"seconds_in_bucket\"] > 300, 1, 0)\n    df[\"min_in_bucket\"] = df[\"seconds_in_bucket\"]\n    for i in range(9):\n        t1, t2 = i * 60, ((i+1) * 60 if i < 8 else 541 )\n        df.loc[(df[\"min_in_bucket\"] >= t1) & (df[\"min_in_bucket\"] < t2), \"min_in_bucket\"] = i \n\n    # create discrete feature\n    int_feat = df.dtypes[df.dtypes == \"int64\"].to_dict().keys()\n    \n    # handle invaild values\n    X_dsc = df[int_feat]\n    for f in int_feat:\n        mv = np.min(X_dsc[f])\n        if mv < 0:\n            X_dsc[f] += 0 - mv\n    X_dsc = X_dsc.drop(columns=\"seconds_in_bucket\")\n    assert not X_dsc.isnull().any().any()\n    cat_num = X_dsc.nunique()\n    \n    X_ctg = df.drop(columns=int_feat)\n    X_ctg = X_ctg.fillna(0)\n    \n    \n    return X_ctg, X_dsc, cat_num\n\nX_ctg, X_dsc, cat_num = feat_eng_nn(X)\nprint(X_ctg.dtypes)\nprint(X_dsc.dtypes)\nprint(X_dsc.head())\nprint(X_dsc.min())\nprint(cat_num)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:31:36.818389Z","iopub.execute_input":"2023-12-19T23:31:36.818814Z","iopub.status.idle":"2023-12-19T23:31:39.372837Z","shell.execute_reply.started":"2023-12-19T23:31:36.818782Z","shell.execute_reply":"2023-12-19T23:31:39.371921Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"imbalance_size                               float64\nreference_price                              float64\nmatched_size                                 float64\nfar_price                                    float64\nnear_price                                   float64\nbid_price                                    float64\nbid_size                                     float64\nask_price                                    float64\nask_size                                     float64\nwap                                          float64\nimbalance_ratio                              float64\nbid_ask_volume_diff                          float64\nbid_plus_ask_sizes                           float64\nmid_price                                    float64\nmedian_size                                  float64\nstd_size                                     float64\nmax_size                                     float64\nmin_size                                     float64\nmean_size                                    float64\nfirst_size                                   float64\nlast_size                                    float64\nreference_price_minus_far_price              float32\nreference_price_far_price_imb                float64\nreference_price_minus_near_price             float32\nreference_price_near_price_imb               float64\nreference_price_minus_ask_price              float32\nreference_price_ask_price_imb                float64\nreference_price_minus_bid_price              float32\nreference_price_bid_price_imb                float64\nreference_price_minus_wap                    float32\nreference_price_wap_imb                      float64\nfar_price_minus_near_price                   float32\nfar_price_near_price_imb                     float64\nfar_price_minus_ask_price                    float32\nfar_price_ask_price_imb                      float64\nfar_price_minus_bid_price                    float32\nfar_price_bid_price_imb                      float64\nfar_price_minus_wap                          float32\nfar_price_wap_imb                            float64\nnear_price_minus_ask_price                   float32\nnear_price_ask_price_imb                     float64\nnear_price_minus_bid_price                   float32\nnear_price_bid_price_imb                     float64\nnear_price_minus_wap                         float32\nnear_price_wap_imb                           float64\nask_price_minus_bid_price                    float32\nask_price_bid_price_imb                      float64\nask_price_minus_wap                          float32\nask_price_wap_imb                            float64\nbid_price_minus_wap                          float32\nbid_price_wap_imb                            float64\nreference_price_far_price_near_price_imb2    float64\nreference_price_far_price_ask_price_imb2     float64\nreference_price_far_price_bid_price_imb2     float64\nreference_price_far_price_wap_imb2           float64\nreference_price_near_price_ask_price_imb2    float64\nreference_price_near_price_bid_price_imb2    float64\nreference_price_near_price_wap_imb2          float64\nreference_price_ask_price_bid_price_imb2     float64\nreference_price_ask_price_wap_imb2           float64\nreference_price_bid_price_wap_imb2           float64\nfar_price_near_price_ask_price_imb2          float64\nfar_price_near_price_bid_price_imb2          float64\nfar_price_near_price_wap_imb2                float64\nfar_price_ask_price_bid_price_imb2           float64\nfar_price_ask_price_wap_imb2                 float64\nfar_price_bid_price_wap_imb2                 float64\nnear_price_ask_price_bid_price_imb2          float64\nnear_price_ask_price_wap_imb2                float64\nnear_price_bid_price_wap_imb2                float64\nask_price_bid_price_wap_imb2                 float64\ndtype: object\nstock_id                   int64\nimbalance_buy_sell_flag    int64\nhigh_volume                int64\nstage                      int64\nmin_in_bucket              int64\ndtype: object\n   stock_id  imbalance_buy_sell_flag  high_volume  stage  min_in_bucket\n0         0                        2            1      0              0\n1         1                        0            0      0              0\n2         2                        0            1      0              0\n3         3                        0            1      0              0\n4         4                        0            0      0              0\nstock_id                   0\nimbalance_buy_sell_flag    0\nhigh_volume                0\nstage                      0\nmin_in_bucket              0\ndtype: int64\nstock_id                   200\nimbalance_buy_sell_flag      3\nhigh_volume                  2\nstage                        2\nmin_in_bucket                9\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"scaler = QuantileTransformer(output_distribution='normal', n_quantiles=30000, subsample=500000)\nscaled_data = scaler.fit_transform(X_ctg)\nX_ctg = pd.DataFrame(scaled_data, columns=X_ctg.columns)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:32:40.484891Z","iopub.execute_input":"2023-12-19T23:32:40.486123Z","iopub.status.idle":"2023-12-19T23:34:35.333785Z","shell.execute_reply.started":"2023-12-19T23:32:40.486081Z","shell.execute_reply":"2023-12-19T23:34:35.332634Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class NNDataset(Dataset):\n    def __init__(self, X_c, X_d, y_nn):\n        self.X_c = X_c\n        self.X_d = X_d\n        self.y_nn = y_nn\n    \n    def __len__(self):\n        return len(self.X_d)\n    \n    def __getitem__(self, idx):\n        if self.y_nn is None:\n            return torch.tensor(self.X_c.iloc[idx]).float(), torch.tensor(self.X_d.iloc[idx]).long()\n        return torch.tensor(self.X_c.iloc[idx]).float(), torch.tensor(self.X_d.iloc[idx]).long(), torch.tensor(self.y_nn[idx]).float()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:34:35.335807Z","iopub.execute_input":"2023-12-19T23:34:35.336273Z","iopub.status.idle":"2023-12-19T23:34:35.345319Z","shell.execute_reply.started":"2023-12-19T23:34:35.336233Z","shell.execute_reply":"2023-12-19T23:34:35.343414Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self,\n                 num_features: int,\n                 hidden_size: int,\n                 n_categories: List[int],\n                 emb_dim: int = 10,\n                 dropout_cat: float = 0.2,\n                 channel_1: int = 256,\n                 channel_2: int = 512,\n                 channel_3: int = 512,\n                 dropout_top: float = 0.1,\n                 dropout_mid: float = 0.3,\n                 dropout_bottom: float = 0.2,\n                 weight_norm: bool = True,\n                 two_stage: bool = True,\n                 celu: bool = True,\n                 kernel1: int = 5,\n                 leaky_relu: bool = False):\n        super().__init__()\n\n        num_targets = 1\n\n        cha_1_reshape = int(hidden_size / channel_1)\n        cha_po_1 = int(hidden_size / channel_1 / 2)\n        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n\n        self.cat_dim = emb_dim * len(n_categories)\n        self.cha_1 = channel_1\n        self.cha_2 = channel_2\n        self.cha_3 = channel_3\n        self.cha_1_reshape = cha_1_reshape\n        self.cha_po_1 = cha_po_1\n        self.cha_po_2 = cha_po_2\n        self.two_stage = two_stage\n\n        self.expand = nn.Sequential(\n            nn.BatchNorm1d(num_features + self.cat_dim),\n            nn.Dropout(dropout_top),\n            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n            nn.CELU(0.06) if celu else nn.ReLU()\n        )\n\n        def _norm(layer, dim=None):\n            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n\n        self.conv1 = nn.Sequential(\n            nn.BatchNorm1d(channel_1),\n            nn.Dropout(dropout_top),\n            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n            nn.BatchNorm1d(channel_2),\n            nn.Dropout(dropout_top),\n            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n            nn.ReLU()\n        )\n\n        if self.two_stage:\n            self.conv2 = nn.Sequential(\n                nn.BatchNorm1d(channel_2),\n                nn.Dropout(dropout_mid),\n                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n                nn.ReLU(),\n                nn.BatchNorm1d(channel_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n                nn.ReLU()\n            )\n\n        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        if leaky_relu:\n            self.dense = nn.Sequential(\n                nn.BatchNorm1d(cha_po_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n                nn.LeakyReLU()\n            )\n        else:\n            self.dense = nn.Sequential(\n                nn.BatchNorm1d(cha_po_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n            )\n\n        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n        self.cat_dim = emb_dim * len(n_categories)\n        self.dropout_cat = nn.Dropout(dropout_cat)\n\n    def forward(self, x_num, x_cat):\n        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n        x = torch.cat([x_num, x_cat_emb], 1)\n\n        x = self.expand(x)\n\n        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n\n        x = self.conv1(x)\n\n        if self.two_stage:\n            x = self.conv2(x) * x\n\n        x = self.max_po_c2(x)\n        x = self.flt(x)\n        x = self.dense(x)\n\n        return torch.squeeze(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:34:35.348109Z","iopub.execute_input":"2023-12-19T23:34:35.348587Z","iopub.status.idle":"2023-12-19T23:34:35.370922Z","shell.execute_reply.started":"2023-12-19T23:34:35.348551Z","shell.execute_reply":"2023-12-19T23:34:35.369339Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:34:35.374189Z","iopub.execute_input":"2023-12-19T23:34:35.374567Z","iopub.status.idle":"2023-12-19T23:34:35.394878Z","shell.execute_reply.started":"2023-12-19T23:34:35.374540Z","shell.execute_reply":"2023-12-19T23:34:35.393711Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"N_Folds = 5\n#ts = TimeSeriesSplit(n_splits=4)\ngkf = PurgedGroupTimeSeriesSplit(n_splits = 5, group_gap = 5)\n\nis_train = False\n\nparams_nn = {\n    \"batch_size\": 1200,\n    \"min_lr\": 1e-7,\n    \"lr\": 1e-3,\n    \"epochs\": 35,\n    \"val_iter\": 1500,\n    \"train_log_step\": 500,\n    \"scheduler_factor\":0.5,\n    \"scd_patience\": 2,\n    \"patience\": 5,\n    \"weight_decay\": 6.5e-6\n}\n\nN, D_c = X_ctg.shape\nprint(N, D_c)\nembed_dims = list(cat_num.to_dict().values())\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"device: {device}\")\noutput_dir = \"cnn_models_ScalerVer_Quantile_30000_1200\"\n# os.system(f'mkdir {output_dir}')\n\nif is_train:\n    for fold_i, (train_idx, valid_idx) in enumerate(gkf.split(X, y, X_date_id)):\n\n        if fold_i == 0:\n            continue\n        \n        # data\n        tr_X_ctg, tr_X_dsc, tr_y = X_ctg.iloc[train_idx], X_dsc.iloc[train_idx], y[train_idx]\n        valid_idx_small = valid_idx[:495000]\n        val_X_ctg, val_X_dsc, val_y = X_ctg.iloc[valid_idx_small], X_dsc.iloc[valid_idx_small], y[valid_idx_small]\n        \n        # build torch dataset and dataloader \n        dataset_tr = NNDataset(tr_X_ctg, tr_X_dsc, tr_y)\n        dataset_val = NNDataset(val_X_ctg, val_X_dsc, val_y)\n        \n        loader_tr = DataLoader(dataset_tr, batch_size=params_nn[\"batch_size\"], shuffle=False, num_workers=10)\n        loader_val = DataLoader(dataset_val, batch_size=params_nn[\"batch_size\"], shuffle=False, num_workers=10)\n        \n        # build model and related modules\n        model = CNN(num_features=D_c,\n                    n_categories=embed_dims,\n                    hidden_size=8*200,\n                    emb_dim=30,\n                    dropout_cat=0,\n                    channel_1=200,\n                    channel_2=3*200,\n                    channel_3=3*200,\n                    dropout_top=0.35,\n                    dropout_mid=0.35,\n                    dropout_bottom=0.35,\n                    weight_norm=True,\n                    two_stage=False,\n                    celu=False,\n                    kernel1=5,\n                    leaky_relu=False)\n        \n        model.to(device)\n        criterion = nn.L1Loss()\n        optimizer = optim.Adam(model.parameters(), lr=params_nn[\"lr\"], weight_decay=params_nn[\"weight_decay\"] )\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", min_lr = params_nn[\"min_lr\"],\n                                                         factor=params_nn[\"scheduler_factor\"],\n                                                         patience=params_nn[\"patience\"],verbose = True)\n        \n        # begin training\n        n_iter = np.ceil(len(dataset_tr) / params_nn[\"batch_size\"])\n        best_loss = np.inf\n        last_epoch_loss = np.inf\n        p = 0\n        for epoch in range(params_nn[\"epochs\"]):\n            loss_epoch = []\n            for i, (x_c, x_d, y_tr) in enumerate(loader_tr):\n                optimizer.zero_grad()\n                x_c, x_d, y_tr = x_c.to(device), x_d.to(device), y_tr.to(device)\n                y_hat = model(x_c, x_d)\n                loss = criterion(y_hat, y_tr)\n                loss.backward()\n                optimizer.step()\n                \n                loss_epoch.append(loss.detach().item())\n                \n                # log\n                if i != 0 and i % params_nn[\"train_log_step\"] == 0:\n                    print(f\"Fold {fold_i:1}, \"\n                          f\"Epoch {epoch:2}/{params_nn['epochs']:2}, \"\n                          f\"iter {i:5}/{n_iter:5}, \"\n                          f\"loss {np.mean(loss_epoch[-params_nn['train_log_step']:]):8.4f}\")\n                \n                # validate\n                if i % params_nn[\"val_iter\"] == 0 and i != 0 or i == n_iter - 1:\n                    loss_val = []\n                    bar = tqdm.tqdm(total=np.ceil(len(dataset_val) / params_nn[\"batch_size\"]))\n                    with torch.no_grad():\n                        for i, (x_c, x_d, y_val) in enumerate(loader_val):\n                            x_c, x_d, y_val = x_c.to(device), x_d.to(device), y_val.to(device)\n\n                            y_hat = model(x_c, x_d)\n                            loss = criterion(y_hat, y_val)\n\n                            loss_val.append(loss.item())\n                            bar.update()\n                    \n                    loss_val_avg = np.mean(loss_val)\n                    print(f\"==> Val current best loss {best_loss:8.4f}\")\n                    print(f\"Fold {fold_i:1}, \"\n                          f\"Epoch {epoch:2}/{params_nn['epochs']:2}, \"\n                          f\"Valid loss {loss_val_avg:8.4f}\")\n                    if loss_val_avg < best_loss:\n                        best_loss = loss_val_avg\n                        print(f\"Loss decreases! current best loss {best_loss:8.4f}\")\n                        torch.save(model, os.path.join(output_dir, f\"fold{fold_i}.pt\"))\n                    print()\n            \n            # early stop\n            if loss_val_avg >= last_epoch_loss:\n                p += 1\n                print(f\"Best loss doesn't decrease in this epoch, patience {p}/{params_nn['patience']}\")\n                if p >= params_nn[\"patience\"]:\n                    print(f\"Reach patience, quit training of fold {fold_i}\")\n                    print()\n                    break\n            else:\n                print(f\"Epoch loss decreases from {last_epoch_loss:8.4f} to {loss_val_avg:8.4f}\")\n                last_epoch_loss = loss_val_avg\n                p = 0\n            \n            # scheduler\n            scheduler.step(loss_val_avg)\n            print()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:34:35.397353Z","iopub.execute_input":"2023-12-19T23:34:35.397684Z","iopub.status.idle":"2023-12-19T23:34:35.420179Z","shell.execute_reply.started":"2023-12-19T23:34:35.397659Z","shell.execute_reply":"2023-12-19T23:34:35.419087Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"5237892 71\ndevice: cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### upload kaggle dataset","metadata":{"papermill":{"duration":0.014762,"end_time":"2023-11-09T03:16:27.326796","exception":false,"start_time":"2023-11-09T03:16:27.312034","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# # want to see feature importance plot for each fold\n# for idx, models in enumerate(model_pipeline.models_list):\n#     for model_name, model in zip(config[\"model_name\"], models):\n#         if \"lgb\" in model_name:\n#             lgb.plot_importance(model, importance_type=\"gain\", figsize=(20, 20))\n#         elif \"xgb\" in model_name:\n#             xgb.plot_importance(model, importance_type=\"gain\", figsize=(20, 20))\n#         else:\n#             raise ValueError(\"Invalid model name\")\n#         plt.title(f\"Feature Importance ({model_name})\")\n#         # plt.savefig(f\"{config['model_dir']}/{idx}_{model_name}_feature_importance.png\")\n#         plt.show()\n","metadata":{"ExecuteTime":{"end_time":"2023-12-13T15:27:36.277817Z","start_time":"2023-12-13T15:27:36.273419Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:34:35.421687Z","iopub.execute_input":"2023-12-19T23:34:35.421995Z","iopub.status.idle":"2023-12-19T23:34:35.434332Z","shell.execute_reply.started":"2023-12-19T23:34:35.421969Z","shell.execute_reply":"2023-12-19T23:34:35.433130Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"\n#### dataset init\n! /home/username/.local/bin/kaggle datasets init -p {config['model_dir']}\n#### dataset create \n! /home/username/.local/bin/kaggle datasets create -p {config['model_dir']}","metadata":{}},{"cell_type":"code","source":"# KAGGLE_DATASET_NAME = \"model-version-31\"","metadata":{"ExecuteTime":{"end_time":"2023-12-13T15:27:36.280484Z","start_time":"2023-12-13T15:27:36.274052Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:34:35.435877Z","iopub.execute_input":"2023-12-19T23:34:35.436203Z","iopub.status.idle":"2023-12-19T23:34:35.451888Z","shell.execute_reply.started":"2023-12-19T23:34:35.436177Z","shell.execute_reply":"2023-12-19T23:34:35.450402Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"if MODE == \"train\":\n    ! /usr/local/bin/kaggle datasets init -p {config['model_dir']}\n    import json\n\n    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"r\") as file:\n        data = json.load(file)\n\n    data[\"title\"] = data[\"title\"].replace(\"INSERT_TITLE_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n    data[\"id\"] = data[\"id\"].replace(\"INSERT_SLUG_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n\n    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"w\") as file:\n        json.dump(data, file, indent=2)\n\n    ! /usr/local/bin/kaggle datasets create -p {config['model_dir']}\n\n    # !/usr/local/bin/kaggle datasets version -p {config['model_dir']} -m 'Updated data'","metadata":{"ExecuteTime":{"end_time":"2023-12-13T15:28:36.783375Z","start_time":"2023-12-13T15:27:36.277689Z"},"collapsed":false,"papermill":{"duration":0.02825,"end_time":"2023-11-09T03:16:27.370494","exception":false,"start_time":"2023-11-09T03:16:27.342244","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:34:35.453458Z","iopub.execute_input":"2023-12-19T23:34:35.453815Z","iopub.status.idle":"2023-12-19T23:34:35.466734Z","shell.execute_reply.started":"2023-12-19T23:34:35.453784Z","shell.execute_reply":"2023-12-19T23:34:35.465410Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class TestStack:\n    #time_id 추가\n    def __init__(self, window_size=6):\n        self.window_size = window_size * 2\n        self.stock_cache = []  # Dictionary to hold cache for each stock\n\n    def test_stack(self, test, time_id):\n        # Convert batch_data to DataFrame if it's a list of dicts\n        if isinstance(test, list):\n            test = pd.DataFrame(test)\n            \n        test['time_id'] = time_id\n        \n        #단일 데이터 추가\n        self.stock_cache.append(test)\n        \n        if len(self.stock_cache) > self.window_size:\n            # 현재 데이터가 n개를 초과하면 n개 이후 데이터는 버림 \n            self.stock_cache = self.stock_cache[-self.window_size:]\n            test = pd.concat(self.stock_cache, axis=0).reset_index(drop=True)\n        else:\n            # 초기화, n개의 데이터를 이미 수집했다면 현재 데이터를 6번 복사\n            self.stock_cache = []\n            for t in range(self.window_size): # [0, 1, 2, 3, 4, 5]\n                test['time_id'] = t - self.window_size + 1 # [-5, -4, -3, -2, -1, 0]\n                test_add = test.copy()\n                self.stock_cache.append(test_add)\n            test = pd.concat(self.stock_cache, axis=0).reset_index(drop=True).sort_values(by='time_id')\n            \n        return test.sort_values(['time_id', 'stock_id'])\n\ntest_cols = None\ndef df_to_seq(test, seq_len):\n    grouped_by_stock = test.groupby('stock_id')\n    datas = []\n\n    for _, group in grouped_by_stock:\n        group_sorted = group.sort_values(by='time_id')\n        cols = remove_element(test.columns, no_feature_cols)\n        \n        features = group_sorted[cols].values # [12, 23]\n        \n        features = features[-seq_len:, ]\n        datas.append(features)\n\n    return np.stack(datas)\n\ndef zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices)/np.sum(std_error)\n    out = prices-std_error*step\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:34:35.468493Z","iopub.execute_input":"2023-12-19T23:34:35.468783Z","iopub.status.idle":"2023-12-19T23:34:35.480853Z","shell.execute_reply.started":"2023-12-19T23:34:35.468756Z","shell.execute_reply":"2023-12-19T23:34:35.479837Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"if is_infer:\n    model_names = ['model_epoch_50_3.pt']\n    \n    models = []\n    for model_name in model_names:\n        models.append(torch.load(f\"/kaggle/input/model-epoch-50-3/{model_name}\", map_location=device))","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:34:35.484105Z","iopub.execute_input":"2023-12-19T23:34:35.484484Z","iopub.status.idle":"2023-12-19T23:34:36.209835Z","shell.execute_reply.started":"2023-12-19T23:34:35.484458Z","shell.execute_reply":"2023-12-19T23:34:36.208926Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"if is_pre_test:\n    # 제출 전 테스트 \n    main_dir = '/kaggle/input/optiver-trading-at-the-close/'\n    \n    test_df = pd.read_csv(main_dir + 'example_test_files/test.csv')\n    #test_df = test_df.drop(columns=['target'])\n    test_group = test_df.groupby(['time_id'])\n    tdp = TestStack(window_size=seq_len)\n\n    counter = 0\n    for test in test_group:\n        test = test[1]\n        test = test.drop(columns=['time_id'])\n\n        # zerosum\n        volumes = test.loc[:,'bid_size'] + test.loc[:,'ask_size']\n\n        # 결측치를 평균값으로 채우기 \n        test['far_price'] = test['far_price'].fillna(far_price_mean)\n        test['near_price'] = test['near_price'].fillna(near_price_mean)\n\n        # 데이터 쌓기 \n        test_stack = tdp.test_stack(test, counter)\n\n        # FE\n        test = sizesum_and_pricestd(test_stack)\n\n        # 정규화\n        test_cols = remove_element(test.columns, no_feature_cols)\n        test[test_cols] = (test[test_cols] - avg)/std\n\n        # 직렬화\n        test = df_to_seq(test, seq_len)\n    #     print(test.shape)\n\n        # 예측 \n        predictions = np.zeros((test.shape[0],))\n        for model in models:\n            test = torch.tensor(test, dtype=torch.float32).squeeze().to(device)\n            predictions_tmp = model(test).squeeze().cpu()\n            predictions_tmp = predictions_tmp.detach().numpy()\n            predictions += predictions_tmp\n        \n        predictions /= len(models)\n        # zero sum조정\n        predictions = zero_sum(predictions, volumes)\n\n        print(predictions.shape)\n\n        counter += 1","metadata":{"execution":{"iopub.status.busy":"2023-12-19T23:34:36.211408Z","iopub.execute_input":"2023-12-19T23:34:36.211943Z","iopub.status.idle":"2023-12-19T23:34:36.221424Z","shell.execute_reply.started":"2023-12-19T23:34:36.211913Z","shell.execute_reply":"2023-12-19T23:34:36.220446Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"dependencies = {\n    \"feature_version_imbalance_2_1\": [\"feature_version_imbalance_1\", \"feature_version_imbalance_2_0\"],\n    \"feature_version_imbalance_6_1\": [\"feature_version_imbalance_6_0\"],\n    \"feature_version_imbalance_7\": [\"feature_version_imbalance_1\"],\n    \"feature_version_imbalance_8\": [\"feature_version_imbalance_3\"],\n    \"feature_version_imbalance_9\": [\"feature_version_imbalance_3\"],\n    \"feature_version_imbalance_10\": [\"feature_version_imbalance_1\", \"feature_version_imbalance_7\"],\n}","metadata":{"ExecuteTime":{"end_time":"2023-12-13T15:28:36.788995Z","start_time":"2023-12-13T15:28:36.784189Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:34:36.222599Z","iopub.execute_input":"2023-12-19T23:34:36.223290Z","iopub.status.idle":"2023-12-19T23:34:36.236163Z","shell.execute_reply.started":"2023-12-19T23:34:36.223262Z","shell.execute_reply":"2023-12-19T23:34:36.234317Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"if config[\"infer_mode\"]:\n    import optiver2023\n\n    optiver2023.make_env.func_dict['__called__'] = False\n    env = optiver2023.make_env()\n    iter_test = env.iter_test()\n\n    y_min, y_max = -64, 64\n    qps = []\n    counter = 0\n    cache = pd.DataFrame()\n\n    model_pipeline.load_models()\n    model_pipeline.load_optuna_weights()\n    \n    batch_size_test = 200\n    \n    tdp = TestStack(window_size=seq_len)\n    \n    lgb_model_weights = weighted_average(config[\"n_splits\"] + 1, equal_weight=False)\n\n    # This is for the generate_global_features (only need to run once)\n    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n    data_processor = DataPreprocessor(data=df)\n    df = data_processor.transform()\n    feature_engineer = FeatureEngineer(data=df)\n    feature_engineer.generate_global_features(data=df)\n\n    for (test, revealed_targets, sample_prediction) in iter_test:\n        \n        now_time = time.time()\n        \n        if not test.currently_scored.iloc[0]:\n            sample_prediction['target'] = 0\n            env.predict(sample_prediction)\n            counter += 1\n            qps.append(time.time() - now_time)\n            if counter % 10 == 0:\n                print(counter, 'qps:', np.mean(qps))\n            continue\n        \n        clipped_predictionss = []\n        \n        test_ = test.copy()\n        \n        if INFER_USE_TF:\n            volumes = test.loc[:,'bid_size'] + test.loc[:,'ask_size']\n\n             # 결측치를 평균값으로 채우기 \n            test['far_price'] = test['far_price'].fillna(far_price_mean)\n            test['near_price'] = test['near_price'].fillna(near_price_mean)\n\n            test_stack = tdp.test_stack(test, counter)\n            # print(counter, test_stack.shape)\n\n            test = sizesum_and_pricestd(test_stack)\n\n            test_cols = remove_element(test.columns, no_feature_cols)\n            test[test_cols] = (test[test_cols] - avg)/std\n\n            testseq = df_to_seq(test, seq_len)\n\n            predictions = np.zeros((testseq.shape[0],))\n            # print('predictions shape', predictions.shape, 'test shape', test.shape)\n            for model in models:\n                test2 = torch.tensor(testseq, dtype=torch.float32).squeeze().to(device)\n                predictions_tmp = model(test2).squeeze().cpu()\n                predictions_tmp = predictions_tmp.detach().numpy()\n                predictions += predictions_tmp\n            predictions /= len(models)\n\n            predictions = zero_sum(predictions, volumes)\n            \n            clipped_predictions = predictions.values\n\n            clipped_predictionss.append(clipped_predictions)\n        \n        if INFER_USE_CNN:\n            \n            test2 = test_.copy()\n            \n            test2.drop('currently_scored',axis=1,inplace=True)\n            feat2 = feature_eng(test2)\n            feat2.drop('date_id',axis=1,inplace=True)\n            \n            X_ctg, X_dsc, cat_num = feat_eng_nn(feat2)\n            scaled_ctg = scaler.transform(X_ctg)\n            X_ctg = pd.DataFrame(scaled_ctg, columns = X_ctg.columns)\n\n            test_dataset = NNDataset(X_ctg, X_dsc, None)\n            test_loader = DataLoader(test_dataset,batch_size=batch_size_test,shuffle=False)\n\n            fold_prediction = np.zeros((test2.shape[0],))\n            for fold in range(0, N_Folds):\n                model_filename = f\"/kaggle/input/cnn-save-world/fold{fold}.pt\"\n                m = torch.load(model_filename, map_location=\"cpu\")\n                for data_i, (x_c, x_d) in enumerate(test_loader):\n                    y_hat = m(x_c, x_d)\n                    fold_prediction[data_i * batch_size_test: (data_i + 1) * batch_size_test] += y_hat.detach().cpu().numpy()\n\n            fold_prediction /= N_Folds\n            fold_prediction = zero_sum(fold_prediction, test2.loc[:,'bid_size'] + test2.loc[:,'ask_size'])\n            clipped_predictions = np.clip(fold_prediction, y_min, y_max)\n            \n            clipped_predictionss.append(clipped_predictions)\n            \n        if INFER_USE_ML:\n            test = test_.copy()\n            \n            cache = pd.concat([cache, test], ignore_index=True, axis=0)\n            if counter > 0:\n                cache = cache.groupby(['stock_id']).tail(21).sort_values(\n                    by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n\n            # preprocessing\n            data_processor = DataPreprocessor(data=cache, infer=True)\n            cache_df = data_processor.transform()\n\n            # feature engineering\n            feature_engineer = FeatureEngineer(data=cache_df, infer=True,\n                                               feature_versions=['feature_version_time',\n                                                                 'feature_version_imbalance_1',\n                                                                 'feature_version_imbalance_2_0',\n                                                                 'feature_version_imbalance_2_1',\n                                                                 'feature_version_imbalance_3',\n                                                                 'feature_version_imbalance_6_0',\n                                                                 'feature_version_imbalance_6_1',\n                                                                 'feature_version_imbalance_7',\n                                                                 'feature_version_imbalance_8',\n                                                                 'feature_version_imbalance_9',\n                                                                 # 'feature_version_imbalance_10',\n                                                                 # 'feature_version_imbalance_11',\n                                                                 # 'feature_version_imbalance_12',\n                                                                 # 'feature_version_custom_weight',\n                                                                 # 'feature_version_order_flow',\n                                                                 ],\n                                               dependencies=dependencies)\n            cache_df = feature_engineer.transform()\n\n            feat = cache_df[-len(test):]\n\n            feat = feat.drop(columns=[\"currently_scored\"])\n\n            # feat = generate_all_features(cache)[-len(test):]\n            test_predss = np.zeros(feat.shape[0])\n            # prediction\n            for i in range(config[\"n_splits\"]):\n                model_pipeline.predict(idx=i, X_test=feat)\n                if config[\"stacking_mode\"] and len(config[\"model_name\"]) > 1:\n                    model_pipeline.stacking(idx=i, infer=True)\n                test_predss += model_pipeline.inference_prediction  #  * lgb_model_weights[i]\n            if config[\"stacking_mode\"] and len(config[\"model_name\"]) == 1:  # single model 에 대한 stacking\n                model_pipeline.stacking(idx=-1, infer=True)\n                test_predss = model_pipeline.inference_prediction\n            # whole data trained model\n            model_pipeline.predict(idx=config[\"n_splits\"], X_test=feat)\n            test_predss += model_pipeline.inference_prediction\n            # test_predss = zero_sum(test_predss, test['bid_size'] + test['ask_size'])\n            clipped_predictions = np.clip(test_predss, y_min, y_max)\n            clipped_predictionss.append(clipped_predictions)\n            \n        sample_prediction['target'] = clipped_predictionss[0]*0.1+clipped_predictionss[1]*0.2+clipped_predictions[2]*0.7\n        \n#         if sample_prediction['target'].isna().sum() != 0:\n#             print(counter)\n#             print('oh there is a problem here!')\n\n        env.predict(sample_prediction)\n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n\n    time_cost = 1.146 * np.mean(qps)\n    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")","metadata":{"ExecuteTime":{"end_time":"2023-12-13T15:28:36.803574Z","start_time":"2023-12-13T15:28:36.793705Z"},"collapsed":false,"papermill":{"duration":53.62895,"end_time":"2023-11-09T03:17:21.014741","exception":false,"start_time":"2023-11-09T03:16:27.385791","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-19T23:52:46.276905Z","iopub.execute_input":"2023-12-19T23:52:46.277489Z","iopub.status.idle":"2023-12-19T23:53:02.762766Z","shell.execute_reply.started":"2023-12-19T23:52:46.277451Z","shell.execute_reply":"2023-12-19T23:53:02.761285Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Successfully loaded model (/kaggle/input/model-version-52/0_lgb_b.pkl)\nSuccessfully loaded model (/kaggle/input/model-version-52/1_lgb_b.pkl)\nSuccessfully loaded model (/kaggle/input/model-version-52/2_lgb_b.pkl)\nSuccessfully loaded model (/kaggle/input/model-version-52/3_lgb_b.pkl)\nSuccessfully loaded model (/kaggle/input/model-version-52/4_lgb_b.pkl)\nSuccessfully loaded model (/kaggle/input/model-version-52/5_lgb_b.pkl)\n\n----------------------------------------------------------------------------------------------------\nExecuted handle_missing_data, Elapsed time: 0.51 seconds, shape((5237892, 17))\n----------------------------------------------------------------------------------------------------\n\n\n----------------------------------------------------------------------------------------------------\nExecuted transform, Elapsed time: 0.51 seconds, shape((5237892, 17))\n----------------------------------------------------------------------------------------------------\n\n\n----------------------------------------------------------------------------------------------------\nExecuted generate_global_features, Elapsed time: 1.53 seconds\n----------------------------------------------------------------------------------------------------\n\nThis version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n10 qps: 0.00015516281127929686\n20 qps: 0.00015619993209838867\n30 qps: 0.00014913082122802734\n40 qps: 0.00014804601669311525\n50 qps: 0.00014525890350341797\n60 qps: 0.00014322201410929363\n70 qps: 0.00014560563223702566\n80 qps: 0.00014385879039764405\n90 qps: 0.00014309353298611112\n100 qps: 0.00014745235443115235\n110 qps: 0.0001500216397372159\n120 qps: 0.00015115737915039062\n130 qps: 0.00014967734997089091\n140 qps: 0.00014855521065848214\n150 qps: 0.00014771302541097004\n160 qps: 0.00014694333076477052\nThe code will take approximately 0.0002 hours to reason about\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"ExecuteTime":{"end_time":"2023-12-13T15:28:36.809365Z","start_time":"2023-12-13T15:28:36.804543Z"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]}]}