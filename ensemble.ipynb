{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60891,"databundleVersionId":6622892,"sourceType":"competition"},{"sourceId":3989074,"sourceType":"datasetVersion","datasetId":2367101},{"sourceId":149116262,"sourceType":"kernelVersion"},{"sourceId":149393795,"sourceType":"kernelVersion"},{"sourceId":149552339,"sourceType":"kernelVersion"},{"sourceId":150293926,"sourceType":"kernelVersion"},{"sourceId":150329906,"sourceType":"kernelVersion"},{"sourceId":150339002,"sourceType":"kernelVersion"},{"sourceId":150418632,"sourceType":"kernelVersion"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. INTRODUCTION\n<center>\n<img src=\"https://www.verywellmind.com/thmb/Mcp09WZWDZyDcyEFXu4JOtV4hNc=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/GettyImages-1173760696-d7d8a2da3fb04d2faf3c5de3a22becec.jpg\" width=1300 height=800 />\n</center>","metadata":{}},{"cell_type":"markdown","source":"**PROBLEM STATEMENT: PREDICT CESSATION IN SMOKERS**\n\n<font size=\"3\">\nSmoking is a well-established cause of various health issues and is a leading contributor to preventable diseases and deaths worldwide. It is projected that smoking-related deaths will reach 10 million by 2030. Efforts have been made to help people quit smoking, but success rates are relatively low, partly due to the complexity of factors influencing smoking cessation.</font>\n\n<font size=\"3\">To improve the effectiveness of smoking cessation, let us use ML to predict better. Our aim is to create a model that can predict an individual's smoking status using bio-signals. This model would consider various factors such as nicotine dependence, carbon monoxide levels, daily cigarette consumption, age of smoking initiation, previous quit attempts, emotional well-being, personality traits, and motivation to quit. By developing such a predictive model, healthcare professionals and patients can better understand the likelihood of successfully quitting smoking. This approach holds promise for improving smoking cessation outcomes.</font>\n\n**Data Description:**\n\n1. Age (5-year gap)\n2. Height (cm)\n3. Weight (kg)\n4. Waist circumference (cm)\n5. Eyesight (left)\n6. Eyesight (right)\n7. Hearing (left)\n8. Hearing (right)\n9. Systolic blood pressure\n10. Diastolic blood pressure (relaxation)\n11. Fasting blood sugar\n12. Total Cholesterol\n13. Triglyceride\n14. HDL cholesterol\n15. LDL cholesterol\n16. Hemoglobin\n17. Urine protein\n18. Serum creatinine\n19. AST (glutamic oxaloacetic transaminase)\n20. ALT (glutamic oxaloacetic transaminase)\n21. GTP (Î³-GTP)\n22. Dental caries\n23. Smoking status\n\n**Metric of Evalutaion:** ROC-AUC","metadata":{}},{"cell_type":"markdown","source":"# 2. IMPORTS","metadata":{}},{"cell_type":"code","source":"import sklearn\nimport numpy as np\nimport os\nimport datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom prettytable import PrettyTable\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_notebook\ntqdm_notebook.get_lock().locks = []\n# !pip install sweetviz\n# import sweetviz as sv\nimport concurrent.futures\nfrom copy import deepcopy       \nfrom functools import partial\nfrom itertools import combinations\nimport random\nfrom random import randint, uniform\nimport gc\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom itertools import combinations\nfrom sklearn.impute import SimpleImputer\nimport xgboost as xg\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\nfrom sklearn.cluster import KMeans\n!pip install yellowbrick\nfrom yellowbrick.cluster import KElbowVisualizer\n!pip install gap-stat\nfrom gap_statistic.optimalK import OptimalK\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import boxcox\nimport math\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.base import BaseEstimator, TransformerMixin\n!pip install optuna\nimport optuna\nimport xgboost as xgb\n!pip install catboost\n!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\nimport lightgbm as lgb\n!pip install category_encoders\nfrom category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\n!pip install -U imbalanced-learn\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.impute import KNNImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import Pool\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.pandas.set_option('display.max_columns',None)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-13T15:23:35.068812Z","iopub.execute_input":"2023-11-13T15:23:35.069066Z","iopub.status.idle":"2023-11-13T15:25:36.101909Z","shell.execute_reply.started":"2023-11-13T15:23:35.069040Z","shell.execute_reply":"2023-11-13T15:25:36.100776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Data","metadata":{}},{"cell_type":"code","source":"global device\ndevice = 'cpu'\n\n\ntrain=pd.read_csv('/kaggle/input/playground-series-s3e24/train.csv')\ntest=pd.read_csv('/kaggle/input/playground-series-s3e24/test.csv')\noriginal=pd.read_csv(\"/kaggle/input/smoker-status-prediction-using-biosignals/train_dataset.csv\")\n\ntrain.drop(columns=[\"id\"],inplace=True)\ntest.drop(columns=[\"id\"],inplace=True)\n\ntrain_copy=train.copy()\ntest_copy=test.copy()\noriginal_copy=original.copy()\n\noriginal[\"original\"]=1\n\ntrain[\"original\"]=0\ntest[\"original\"]=0\n\ntrain=pd.concat([train,original],axis=0)\ntrain.reset_index(inplace=True,drop=True)\ntrain.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-13T15:25:36.104428Z","iopub.execute_input":"2023-11-13T15:25:36.105593Z","iopub.status.idle":"2023-11-13T15:25:37.143895Z","shell.execute_reply.started":"2023-11-13T15:25:36.105563Z","shell.execute_reply":"2023-11-13T15:25:37.142918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Missing Values","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\n\ntable.field_names = ['Feature', 'Data Type', 'Train Missing %', 'Test Missing %',\"Original Missing%\"]\nfor column in train_copy.columns:\n    data_type = str(train_copy[column].dtype)\n    non_null_count_train= np.round(100-train_copy[column].count()/train_copy.shape[0]*100,1)\n    if column!='smoking':\n        non_null_count_test = np.round(100-test_copy[column].count()/test_copy.shape[0]*100,1)\n    else:\n        non_null_count_test=\"NA\"\n    non_null_count_orig= np.round(100-original_copy[column].count()/original_copy.shape[0]*100,1)\n    table.add_row([column, data_type, non_null_count_train,non_null_count_test,non_null_count_orig])\nprint(table)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-13T15:25:37.144985Z","iopub.execute_input":"2023-11-13T15:25:37.145279Z","iopub.status.idle":"2023-11-13T15:25:37.172040Z","shell.execute_reply.started":"2023-11-13T15:25:37.145253Z","shell.execute_reply":"2023-11-13T15:25:37.171116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='3'>We don't have any missing values in any of the datasets</font>","metadata":{}},{"cell_type":"markdown","source":"# 3. EXPLORATORY DATA ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Target Distribution","metadata":{}},{"cell_type":"code","source":"def plot_pie_chart(data, title, ax):\n    data_counts = data['smoking'].value_counts()\n    labels = data_counts.index\n    sizes = data_counts.values\n    colors = [ (0.3, 0.6, 0.6), 'crimson']  \n    explode = (0.1, 0)  \n\n    ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n    ax.axis('equal') \n    ax.set_title(title)\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))  # Create three subplots in a row\n\nplot_pie_chart(train_copy, \"Train smoking status Distribution\", axes[0])\nplot_pie_chart(original, \"Original smoking status Distribution\", axes[1])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-13T15:25:37.175009Z","iopub.execute_input":"2023-11-13T15:25:37.175583Z","iopub.status.idle":"2023-11-13T15:25:37.863533Z","shell.execute_reply.started":"2023-11-13T15:25:37.175555Z","shell.execute_reply":"2023-11-13T15:25:37.862614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">The smoking status distribution is similar</font>","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Numerical Feature Distributions","metadata":{}},{"cell_type":"code","source":"cont_cols = [f for f in train.columns if train[f].dtype != 'O' and train[f].nunique() > 2]\nn_rows = len(cont_cols)\nfig, axs = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\nsns.set_palette(\"Set3\")\nfor i, col in enumerate(cont_cols):\n    sns.violinplot(x='smoking', y=col, data=train_copy, ax=axs[i, 0])\n    axs[i, 0].set_title(f'{col.title()} Distribution by Target (Train)', fontsize=14)\n    axs[i, 0].set_xlabel('smoking', fontsize=12)\n    axs[i, 0].set_ylabel(col.title(), fontsize=12)\n    sns.despine()\n\n    sns.violinplot(x='smoking', y=col, data=original, ax=axs[i, 1])\n    axs[i, 1].set_title(f'{col.title()} Distribution by Target (Original)', fontsize=14)\n    axs[i, 1].set_xlabel('smoking', fontsize=12)\n    axs[i, 1].set_ylabel(col.title(), fontsize=12)\n    sns.despine()\n\nfig.tight_layout()\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-13T15:25:37.864937Z","iopub.execute_input":"2023-11-13T15:25:37.865365Z","iopub.status.idle":"2023-11-13T15:25:59.694011Z","shell.execute_reply.started":"2023-11-13T15:25:37.865332Z","shell.execute_reply":"2023-11-13T15:25:59.693138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**INFERENCES**\n1. <font size=\"3\">Very interesting spike shaped distribution for height, weight, & age. I think the numbers are rounded or maybe experimental error that lead to numbers being close to 5</font>\n2. <font size=\"3\">Serum creatinine looks like distingushing feature atleast in the train dataset</font>","metadata":{}},{"cell_type":"markdown","source":"  ## 3.3 Numerical Pair Plots - Original","metadata":{}},{"cell_type":"code","source":"pair_plot_cols=[f for f in cont_cols if original[f].nunique()>50]\n\nsns.set(font_scale=1)\nplt.figure(figsize=(18, 10))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(data=original, vars=pair_plot_cols,diag_kind='kde', \n        kind='scatter', palette='muted', \n        plot_kws={'s': 20}, hue='smoking')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-13T15:25:59.695378Z","iopub.execute_input":"2023-11-13T15:25:59.695998Z","iopub.status.idle":"2023-11-13T15:31:30.217537Z","shell.execute_reply.started":"2023-11-13T15:25:59.695968Z","shell.execute_reply":"2023-11-13T15:31:30.216142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**INFERENCES:**\n1. <font size=\"3\">Hemoglobin couppled with Waist, Systolic, Relaxation, Fasting Blood Sugar, Cholestrol,triglyceride, & HDL.</font>","metadata":{}},{"cell_type":"markdown","source":"# 4. FEATURE ENGINEERING","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">The data processing ideas are referenced from [@paddykb](https://www.kaggle.com/code/paddykb/pg-s3e24-brute-force-and-ignorance)</font>","metadata":{}},{"cell_type":"code","source":"def create_extra_features(df):\n    best = np.where(df['hearing(left)'] < df['hearing(right)'], \n                    df['hearing(left)'],  df['hearing(right)'])\n    worst = np.where(df['hearing(left)'] < df['hearing(right)'], \n                     df['hearing(right)'],  df['hearing(left)'])\n    df['hearing(left)'] = best - 1\n    df['hearing(right)'] = worst - 1\n    \n    df['eyesight(left)'] = np.where(df['eyesight(left)'] > 9, 0, df['eyesight(left)'])\n    df['eyesight(right)'] = np.where(df['eyesight(right)'] > 9, 0, df['eyesight(right)'])\n    best = np.where(df['eyesight(left)'] < df['eyesight(right)'], \n                    df['eyesight(left)'],  df['eyesight(right)'])\n    worst = np.where(df['eyesight(left)'] < df['eyesight(right)'], \n                     df['eyesight(right)'],  df['eyesight(left)'])\n    df['eyesight(left)'] = best\n    df['eyesight(right)'] = worst\n    ##\n    df['Gtp'] = np.clip(df['Gtp'], 0, 300)\n    df['HDL'] = np.clip(df['HDL'], 0, 110)\n    df['LDL'] = np.clip(df['LDL'], 0, 200)\n    df['ALT'] = np.clip(df['ALT'], 0, 150)\n    df['AST'] = np.clip(df['AST'], 0, 100)\n    df['serum creatinine'] = np.clip(df['serum creatinine'], 0, 3)  \n    \n    return df\ntrain=create_extra_features(train)\ntest=create_extra_features(test)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T15:31:30.219553Z","iopub.execute_input":"2023-11-13T15:31:30.220017Z","iopub.status.idle":"2023-11-13T15:31:30.275575Z","shell.execute_reply.started":"2023-11-13T15:31:30.219972Z","shell.execute_reply":"2023-11-13T15:31:30.274589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Basic Functions","metadata":{}},{"cell_type":"code","source":"def min_max_scaler(train, test, column):\n    '''\n    Min Max just based on train might have an issue if test has extreme values, hence changing the denominator uding overall min and max\n    '''\n    sc=MinMaxScaler()\n    \n    max_val=max(train[column].max(),test[column].max())\n    min_val=min(train[column].min(),test[column].min())\n\n    train[column]=(train[column]-min_val)/(max_val-min_val)\n    test[column]=(test[column]-min_val)/(max_val-min_val)\n    \n    return train,test  \n\ndef OHE(train_df,test_df,cols,target):\n    '''\n    Function for one hot encoding, it first combines the data so that no category is missed and\n    the category with least frequency can be dropped because of redundancy\n    '''\n    combined = pd.concat([train_df, test_df], axis=0)\n    for col in cols:\n        one_hot = pd.get_dummies(combined[col])\n        counts = combined[col].value_counts()\n        min_count_category = counts.idxmin()\n        one_hot = one_hot.drop(min_count_category, axis=1)\n        one_hot.columns=[str(f)+col+\"_OHE\" for f in one_hot.columns]\n        combined = pd.concat([combined, one_hot], axis=\"columns\")\n        combined = combined.loc[:, ~combined.columns.duplicated()]\n    \n    # split back to train and test dataframes\n    train_ohe = combined[:len(train_df)]\n    test_ohe = combined[len(train_df):]\n    test_ohe.reset_index(inplace=True,drop=True)\n    test_ohe.drop(columns=[target],inplace=True)\n    return train_ohe, test_ohe\n\nlgb_params = {\n            'n_estimators': 100,\n            'max_depth': 6,\n            \"num_leaves\": 16,\n            'learning_rate': 0.05,\n            'subsample': 0.7,\n            'colsample_bytree': 0.8,\n            #'reg_alpha': 0.25,\n            'reg_lambda': 5e-07,\n            'objective': 'regression_l2',\n            'metric': 'mean_squared_error',\n            'boosting_type': 'gbdt',\n            'random_state': 42,\n            'device': device,\n        }\ndef rmse(y1,y2):\n    ''' RMSE Evaluator'''\n    return(np.sqrt(mean_squared_error(np.array(y1),np.array(y2))))\n\ndef store_missing_rows(df, features):\n    '''Function stores where missing values are located for given set of features'''\n    missing_rows = {}\n    \n    for feature in features:\n        missing_rows[feature] = df[df[feature].isnull()]\n    \n    return missing_rows\n\ndef fill_missing_numerical(train,test,target, max_iterations=10):\n    '''Iterative Missing Imputer: Updates filled missing values iteratively using CatBoost Algorithm'''\n    train_temp=train.copy()\n    if target in train_temp.columns:\n        train_temp=train_temp.drop(columns=target)\n        \n    \n    df=pd.concat([train_temp,test],axis=\"rows\")\n    df=df.reset_index(drop=True)\n    features=[ f for f in df.columns if df[f].isna().sum()>0]\n    if len(features)>0:\n        # Step 1: Store the instances with missing values in each feature\n        missing_rows = store_missing_rows(df, features)\n\n        # Step 2: Initially fill all missing values with \"Missing\"\n        for f in features:\n            df[f]=df[f].fillna(df[f].mean())\n\n        cat_features=[f for f in df.columns if not pd.api.types.is_numeric_dtype(df[f])]\n        dictionary = {feature: [] for feature in features}\n\n        for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n            for feature in features:\n                # Skip features with no missing values\n                rows_miss = missing_rows[feature].index\n\n                missing_temp = df.loc[rows_miss].copy()\n                non_missing_temp = df.drop(index=rows_miss).copy()\n                y_pred_prev=missing_temp[feature]\n                missing_temp = missing_temp.drop(columns=[feature])\n\n\n                # Step 3: Use the remaining features to predict missing values using Random Forests\n                X_train = non_missing_temp.drop(columns=[feature])\n                y_train = non_missing_temp[[feature]]\n\n                model= lgb.LGBMRegressor(**lgb_params)\n                model.fit(X_train, y_train, verbose=False)\n\n                # Step 4: Predict missing values for the feature and update all N features\n                y_pred = model.predict(missing_temp)\n                df.loc[rows_miss, feature] = y_pred\n                error_minimize=rmse(y_pred,y_pred_prev)\n                dictionary[feature].append(error_minimize)  # Append the error_minimize value\n\n#         for feature, values in dictionary.items():\n#             iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n#             plt.plot(iterations, values, label=feature)  # plot the values\n#             plt.xlabel('Iterations')\n#             plt.ylabel('RMSE')\n#             plt.title('Minimization of RMSE with iterations')\n#             plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n#         plt.show()\n        train[features] = np.array(df.iloc[:train.shape[0]][features])\n        test[features] = np.array(df.iloc[train.shape[0]:][features])\n\n    return train,test","metadata":{"execution":{"iopub.status.busy":"2023-11-13T15:31:30.276955Z","iopub.execute_input":"2023-11-13T15:31:30.277322Z","iopub.status.idle":"2023-11-13T15:31:30.303589Z","shell.execute_reply.started":"2023-11-13T15:31:30.277296Z","shell.execute_reply":"2023-11-13T15:31:30.302687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.1 Numerical Transformations","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">We're going to see what transformation works better for each feature and select them, the idea is to compress the data. There could be situations where you will have to stretch the data. These are the methods applied:</font>\n\n1. **Log Transformation**: <font size=\"3\">This transformation involves taking the logarithm of each data point. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                y = log(x)\n\n2. **Square Root Transformation**: <font size=\"3\">This transformation involves taking the square root of each data point. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                y = sqrt(x)\n\n3. **Box-Cox Transformation**: <font size=\"3\">This transformation is a family of power transformations that includes the log and square root transformations as special cases. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                y = [(x^lambda) - 1] / lambda if lambda != 0\n                y = log(x) if lambda = 0\n\n4. **Yeo-Johnson Transformation**: <font size=\"3\">This transformation is similar to the Box-Cox transformation, but it can be applied to both positive and negative values. It is useful when the data is highly skewed and the variance increases with the mean.</font>\n                y = [(|x|^lambda) - 1] / lambda if x >= 0, lambda != 0\n                y = log(|x|) if x >= 0, lambda = 0\n                y = -[(|x|^lambda) - 1] / lambda if x < 0, lambda != 2\n                y = -log(|x|) if x < 0, lambda = 2\n\n5. **Power Transformation**: <font size=\"3\">This transformation involves raising each data point to a power. It is useful when the data is highly skewed and the variance increases with the mean. The power can be any value, and is often determined using statistical methods such as the Box-Cox or Yeo-Johnson transformations.</font>\n                y = [(x^lambda) - 1] / lambda if method = \"box-cox\" and lambda != 0\n                y = log(x) if method = \"box-cox\" and lambda = 0\n                y = [(x + 1)^lambda - 1] / lambda if method = \"yeo-johnson\" and x >= 0, lambda != 0\n                y = log(x + 1) if method = \"yeo-johnson\" and x >= 0, lambda = 0\n                y = [-(|x| + 1)^lambda - 1] / lambda if method = \"yeo-johnson\" and x < 0, lambda != 2\n                y = -log(|x| + 1) if method = \"yeo-johnson\" and x < 0, lambda = 2","metadata":{}},{"cell_type":"code","source":"cont_cols = [f for f in train.columns if pd.api.types.is_numeric_dtype(train[f]) and train[f].nunique() >2]\ncat_cols = [f for f in train.columns if train[f].nunique()!=2and f not in ['smoking']]\n\nsc=MinMaxScaler()\n\nglobal unimportant_features\nglobal overall_best_score\nglobal overall_best_col\nunimportant_features=[]\noverall_best_score=0\noverall_best_col='none'\n\nfor col in cont_cols:\n     train, test=min_max_scaler(train, test, col)\n\ndef transformer(train, test,cont_cols, target):\n    '''\n    Algorithm applies multiples transformations on selected columns and finds the best transformation using a single variable model performance\n    '''\n    global unimportant_features\n    global overall_best_score\n    global overall_best_col\n    train_copy = train.copy()\n    test_copy = test.copy()\n    table = PrettyTable()\n    table.field_names = ['Feature', 'Initial ROC_AUC', 'Transformation', 'Tranformed ROC_AUC']\n\n    for col in cont_cols:\n        \n        for c in [\"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col, \"log_sqrt\"+col, \"pow_\"+col, \"pow2_\"+col]:\n            if c in train_copy.columns:\n                train_copy = train_copy.drop(columns=[c])\n        \n        # Log Transformation after MinMax Scaling (keeps data between 0 and 1)\n        train_copy[\"log_\"+col] = np.log1p(train_copy[col])\n        test_copy[\"log_\"+col] = np.log1p(test_copy[col])\n        \n        # Square Root Transformation\n        train_copy[\"sqrt_\"+col] = np.sqrt(train_copy[col])\n        test_copy[\"sqrt_\"+col] = np.sqrt(test_copy[col])\n        \n        # Box-Cox transformation\n        combined_data = pd.concat([train_copy[[col]], test_copy[[col]]], axis=0)\n        epsilon = 1e-5\n        transformer = PowerTransformer(method='box-cox')\n        scaled_data = transformer.fit_transform(combined_data + epsilon)\n\n        train_copy[\"bx_cx_\" + col] = scaled_data[:train_copy.shape[0]]\n        test_copy[\"bx_cx_\" + col] = scaled_data[train_copy.shape[0]:]\n        # Yeo-Johnson transformation\n        transformer = PowerTransformer(method='yeo-johnson')\n        train_copy[\"y_J_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"y_J_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Power transformation, 0.25\n        power_transform = lambda x: np.power(x + 1 - np.min(x), 0.25)\n        transformer = FunctionTransformer(power_transform)\n        train_copy[\"pow_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"pow_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Power transformation, 2\n        power_transform = lambda x: np.power(x + 1 - np.min(x), 2)\n        transformer = FunctionTransformer(power_transform)\n        train_copy[\"pow2_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"pow2_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Log to power transformation\n        train_copy[\"log_sqrt\"+col] = np.log1p(train_copy[\"sqrt_\"+col])\n        test_copy[\"log_sqrt\"+col] = np.log1p(test_copy[\"sqrt_\"+col])\n        \n        temp_cols = [col, \"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col,  \"pow_\"+col , \"pow2_\"+col,\"log_sqrt\"+col]\n        \n        train_copy,test_copy = fill_missing_numerical(train_copy,test_copy,\"defects\",5)\n#         train_copy[temp_cols] = train_copy[temp_cols].fillna(0)\n#         test_copy[temp_cols] = test_copy[temp_cols].fillna(0)\n        \n        pca = TruncatedSVD(n_components=1)\n        x_pca_train = pca.fit_transform(train_copy[temp_cols])\n        x_pca_test = pca.transform(test_copy[temp_cols])\n        x_pca_train = pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb\"])\n        x_pca_test = pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb\"])\n        temp_cols.append(col+\"_pca_comb\")\n        \n        test_copy = test_copy.reset_index(drop=True)\n        \n        train_copy = pd.concat([train_copy, x_pca_train], axis='columns')\n        test_copy = pd.concat([test_copy, x_pca_test], axis='columns')\n        \n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n        \n        auc_scores = []\n        \n        for f in temp_cols:\n            X = train_copy[[f]].values\n            y = train_copy[target].values\n            \n            auc = []\n            for train_idx, val_idx in kf.split(X, y):\n                X_train, y_train = X[train_idx], y[train_idx]\n                x_val, y_val = X[val_idx], y[val_idx]\n#                 model =   SVC(gamma=\"auto\", probability=True, random_state=42)\n                model =   LogisticRegression() # since it is a large dataset, Logistic Regression would be a good option to save time\n                model.fit(X_train,y_train)\n                y_pred = model.predict_proba(x_val)[:,1]\n                auc.append(roc_auc_score(y_val, y_pred))\n            auc_scores.append((f, np.mean(auc)))\n            \n            if overall_best_score < np.mean(auc):\n                overall_best_score = np.mean(auc)\n                overall_best_col = f\n\n            if f == col:\n                orig_auc = np.mean(auc)\n                \n        best_col, best_auc = sorted(auc_scores, key=lambda x: x[1], reverse=True)[0]\n        cols_to_drop = [f for f in temp_cols if f != best_col]\n        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n        \n        if cols_to_drop:\n            unimportant_features = unimportant_features+cols_to_drop\n        table.add_row([col,orig_auc,best_col ,best_auc])\n    print(table)   \n    print(\"overall best CV ROC AUC score: \",overall_best_score)\n    return train_copy, test_copy\n\ntrain, test= transformer(train, test,cont_cols, \"smoking\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T15:31:30.305155Z","iopub.execute_input":"2023-11-13T15:31:30.305464Z","iopub.status.idle":"2023-11-13T15:35:29.447692Z","shell.execute_reply.started":"2023-11-13T15:31:30.305438Z","shell.execute_reply":"2023-11-13T15:35:29.446709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2 Discrete Feature-->Categorical","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">For each categorical/discrete variable, perform the following encoding techniques:</font>\n\n\n* **Count/Frequency Encoding**: Count the number of occurrences of each category and replace the category with its count.\n* **Count Labeling**: Assign a label to each category based on its count, with higher counts receiving higher labels.\n* **Target-Guided Mean Encoding**: Rank the categories based on the mean of target column across each category\n* **One-Hot Encoding**: Apply OHE if the unique  value is less than N (avoid creating so many features)\n\nPlease note that a particular encoding technique is not selected only if it has superior technique and the correlation with that is high","metadata":{}},{"cell_type":"code","source":"selected_cols=[]\nfor col in cat_cols:\n    train['cat_'+col]=train[col]\n    test['cat_'+col]=test[col]\n#     cat_list=test['cat_'+col].unique()\n#     train['cat_'+col]=train['cat_'+col].apply(lambda x: x if x in cat_list else np.nan)\n    selected_cols.append('cat_'+col)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T15:35:29.449027Z","iopub.execute_input":"2023-11-13T15:35:29.449659Z","iopub.status.idle":"2023-11-13T15:35:29.503138Z","shell.execute_reply.started":"2023-11-13T15:35:29.449623Z","shell.execute_reply":"2023-11-13T15:35:29.501864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n    '''\n    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied, \n    where it takes a maximum of n columns and drops the rest of them treating as rare categories\n    '''\n    train_copy=train.copy()\n    test_copy=test.copy()\n    ohe_cols=[]\n    for col in extra_cols:\n        dict1=train_copy[col].value_counts().to_dict()\n        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n        rare_keys=list([*ordered.keys()][n_limit:])\n#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n        \n        train_copy[col]=train_copy[col].replace(rare_key_map)\n        test_copy[col]=test_copy[col].replace(rare_key_map)\n    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n    train_copy=train_copy.drop(columns=drop_cols)\n    test_copy=test_copy.drop(columns=drop_cols)\n    \n    return train_copy, test_copy\n\ndef cat_encoding(train, test,cat_cols, target):\n    '''Takes in a list of features and applied different categorical encoding techniques including One-hot and return the best one using \n    a single var model and other encoders if they do not have high correlation'''\n    global overall_best_score\n    global overall_best_col\n    table = PrettyTable()\n    table.field_names = ['Feature', 'Encoded Feature', 'ROC AUC Score']\n    train_copy=train.copy()\n    test_copy=test.copy()\n    train_dum = train.copy()\n    for feature in cat_cols:\n        cat_labels = train_copy.groupby([feature])[target].mean().sort_values().index\n        cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n        train_copy[feature + \"_target\"] = train_copy[feature].map(cat_labels2)\n        test_copy[feature + \"_target\"] = test_copy[feature].map(cat_labels2)\n\n        dic = train_copy[feature].value_counts().to_dict()\n        train_copy[feature + \"_count\"] =train_copy[feature].map(dic)\n        test_copy[feature + \"_count\"] = test_copy[feature].map(dic)\n\n        dic2=train_copy[feature].value_counts().to_dict()\n        list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n        # list1=np.arange(len(dic2.values())) # Higher rank for low count\n        dic3=dict(zip(list(dic2.keys()),list1))\n        train_copy[feature+\"_count_label\"]=train_copy[feature].replace(dic3).astype(float)\n        test_copy[feature+\"_count_label\"]=test_copy[feature].replace(dic3).astype(float)\n\n        temp_cols = [feature + \"_count\", feature + \"_count_label\"]\n        if train_copy[feature].dtype=='O':\n            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n            train_copy=train_copy.drop(columns=[feature])\n            test_copy=test_copy.drop(columns=[feature])\n        else:\n            if train_copy[feature].nunique()<=50:\n                train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n                test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n                train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n                train_copy=train_copy.drop(columns=[feature])\n                test_copy=test_copy.drop(columns=[feature])\n#                 temp_cols.append(feature)\n            else:\n                train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=10)\n            \n\n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n        auc_scores = []\n\n        for f in temp_cols:\n            X = train_copy[[f]].values\n            y = train_copy[target].astype(int).values\n\n            auc = []\n            for train_idx, val_idx in kf.split(X, y):\n                X_train, y_train = X[train_idx], y[train_idx]\n                x_val, y_val = X[val_idx], y[val_idx]\n                model =  HistGradientBoostingClassifier (max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n                model.fit(X_train, y_train)\n                y_pred = model.predict_proba(x_val)[:,1]\n                auc.append(roc_auc_score(y_val,  y_pred))\n            auc_scores.append((f, np.mean(auc)))\n            if overall_best_score < np.mean(auc):\n                overall_best_score = np.mean(auc)\n                overall_best_col = f\n        best_col, best_auc = sorted(auc_scores, key=lambda x: x[1], reverse=True)[0]\n\n        corr = train_copy[temp_cols].corr(method='pearson')\n        corr_with_best_col = corr[best_col]\n        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n        if cols_to_drop:\n            train_copy = train_copy.drop(columns=cols_to_drop)\n            test_copy = test_copy.drop(columns=cols_to_drop)\n\n        table.add_row([feature, best_col, best_auc])\n        print(feature)\n    print(table)\n    print(\"overall best CV score: \", overall_best_score)\n    return train_copy, test_copy\n\ntrain, test= cat_encoding(train, test,selected_cols, \"smoking\")\ntrain, test = fill_missing_numerical(train, test,\"smoking\",3)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T15:35:29.509761Z","iopub.execute_input":"2023-11-13T15:35:29.510637Z","iopub.status.idle":"2023-11-13T16:10:28.629406Z","shell.execute_reply.started":"2023-11-13T15:35:29.510604Z","shell.execute_reply":"2023-11-13T16:10:28.628406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.3 Numerical Clustering","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\ntable.field_names = ['Clustered Feature', 'ROC AUC (CV-TRAIN)']\nfor col in cont_cols:\n    sub_set=[f for f in unimportant_features if col in f]\n    temp_train=train[sub_set]\n    temp_test=test[sub_set]\n    sc=StandardScaler()\n    temp_train=sc.fit_transform(temp_train)\n    temp_test=sc.transform(temp_test)\n    model = KMeans()\n\n    # print(ideal_clusters)\n    kmeans = KMeans(n_clusters=10)\n    kmeans.fit(np.array(temp_train))\n    labels_train = kmeans.labels_\n\n    train[col+\"_unimp_cluster_WOE\"] = labels_train\n    test[col+\"_unimp_cluster_WOE\"] = kmeans.predict(np.array(temp_test))\n\n    \n    kf=KFold(n_splits=5, shuffle=True, random_state=42)\n    \n    X=train[[col+\"_unimp_cluster_WOE\"]].values\n    y=train[\"smoking\"].astype(int).values\n\n    auc=[]\n    for train_idx, val_idx in kf.split(X,y):\n        X_train,y_train=X[train_idx],y[train_idx]\n        x_val,y_val=X[val_idx],y[val_idx]\n        model = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(x_val)[:,1]\n        auc.append(roc_auc_score(y_val,y_pred))\n        \n    table.add_row([col+\"_unimp_cluster_WOE\",np.mean(auc)])\n    if overall_best_score<np.mean(auc):\n        overall_best_score=np.mean(auc)\n        overall_best_col=col+\"_unimp_cluster_WOE\"\n\nprint(table)\nprint(\"overall best CV score: \", overall_best_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:10:28.630996Z","iopub.execute_input":"2023-11-13T16:10:28.631767Z","iopub.status.idle":"2023-11-13T16:14:43.971734Z","shell.execute_reply.started":"2023-11-13T16:10:28.631728Z","shell.execute_reply":"2023-11-13T16:14:43.970640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.4 Arithmetic New Features","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Until now, I have saved the best overall column and the best overall score, a few feature can be created based on the below criteria:</font>\n* New features are based on the existing features by computing the arithmetic combinations\n* The best arithmetic function is selected based on the individual performance\n* If the best arithmetic feature has better AUC score than the overall best score or the correlation of this feature with the existing features is less than 0.9, then a new feature is added to the dataset. ","metadata":{}},{"cell_type":"code","source":"def better_features(train, test, target, cols, best_score):\n    new_cols = []\n    skf = KFold(n_splits=5, shuffle=True, random_state=42)  # Stratified k-fold object\n    best_list=[]\n    for i in tqdm(range(len(cols)), desc='Generating Columns'):\n        col1 = cols[i]\n        temp_df = pd.DataFrame()  # Temporary dataframe to store the generated columns\n        temp_df_test = pd.DataFrame()  # Temporary dataframe for test data\n\n        for j in range(i+1, len(cols)):\n            col2 = cols[j]\n            # Multiply\n            temp_df[col1 + '*' + col2] = train[col1] * train[col2]\n            temp_df_test[col1 + '*' + col2] = test[col1] * test[col2]\n\n            # Divide (col1 / col2)\n            temp_df[col1 + '/' + col2] = train[col1] / (train[col2] + 1e-5)\n            temp_df_test[col1 + '/' + col2] = test[col1] / (test[col2] + 1e-5)\n\n            # Divide (col2 / col1)\n            temp_df[col2 + '/' + col1] = train[col2] / (train[col1] + 1e-5)\n            temp_df_test[col2 + '/' + col1] = test[col2] / (test[col1] + 1e-5)\n\n            # Subtract\n            temp_df[col1 + '-' + col2] = train[col1] - train[col2]\n            temp_df_test[col1 + '-' + col2] = test[col1] - test[col2]\n\n            # Add\n            temp_df[col1 + '+' + col2] = train[col1] + train[col2]\n            temp_df_test[col1 + '+' + col2] = test[col1] + test[col2]\n\n        SCORES = []\n        for column in temp_df.columns:\n            scores = []\n            for train_index, val_index in skf.split(train, train[target]):\n                X_train, X_val = temp_df[column].iloc[train_index].values.reshape(-1, 1), temp_df[column].iloc[val_index].values.reshape(-1, 1)\n                y_train, y_val = train[target].astype(int).iloc[train_index], train[target].astype(int).iloc[val_index]\n                model = LogisticRegression()#HistGradientBoostingClassifier(max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n                model.fit(X_train, y_train)\n                y_pred = model.predict_proba(X_val)[:,1]\n                score = roc_auc_score( y_val, y_pred)\n                scores.append(score)\n            mean_score = np.mean(scores)\n            SCORES.append((column, mean_score))\n\n        if SCORES:\n            best_col, best_auc = sorted(SCORES, key=lambda x: x[1],reverse=True)[0]\n            corr_with_other_cols = train.drop([target] + new_cols, axis=1).corrwith(temp_df[best_col])\n            if (corr_with_other_cols.abs().max() < 0.9 or best_auc > best_score) and corr_with_other_cols.abs().max() !=1 :\n                train[best_col] = temp_df[best_col]\n                test[best_col] = temp_df_test[best_col]\n                new_cols.append(best_col)\n                print(f\"Added column '{best_col}' with ROC AUC Score: {best_auc:.4f} & Correlation {corr_with_other_cols.abs().max():.4f}\")\n\n    return train, test, new_cols","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:14:43.973313Z","iopub.execute_input":"2023-11-13T16:14:43.973614Z","iopub.status.idle":"2023-11-13T16:14:43.991355Z","shell.execute_reply.started":"2023-11-13T16:14:43.973586Z","shell.execute_reply":"2023-11-13T16:14:43.990442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">We don't have to run the above algorithm every time, just run it once to store the combinations and compute just the required columns</font>","metadata":{}},{"cell_type":"code","source":"# selected_features=[f for f in train.columns if train[f].nunique()>2 and f not in unimportant_features]\n# train, test,new_cols=better_features(train, test, 'smoking', selected_features, overall_best_score)\n# new_cols","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-13T16:14:43.992452Z","iopub.execute_input":"2023-11-13T16:14:43.992726Z","iopub.status.idle":"2023-11-13T16:14:44.007945Z","shell.execute_reply.started":"2023-11-13T16:14:43.992701Z","shell.execute_reply":"2023-11-13T16:14:44.007114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_cols=['height(cm)*hemoglobin',\n 'eyesight(left)*Gtp',\n 'Gtp/systolic',\n 'Gtp/Cholesterol',\n 'triglyceride+hemoglobin',\n 'HDL/Gtp',\n 'Gtp/LDL',\n 'hemoglobin+Gtp',\n 'Gtp/AST',\n 'ALT*Gtp',\n 'cat_Gtp/cat_relaxation',\n 'cat_fasting blood sugar*cat_Gtp',\n 'cat_triglyceride/cat_Gtp_count',\n 'cat_HDL*cat_Gtp_count',\n 'cat_AST*cat_Gtp',\n 'cat_Gtp*cat_height(cm)_count',\n 'cat_age_count/cat_Gtp_count',\n 'cat_Gtp_count/cat_height(cm)_count',\n 'cat_weight(kg)_count/cat_Gtp_count',\n 'cat_waist(cm)_count-cat_Gtp_count',\n 'cat_eyesight(left)_count/cat_Gtp_count',\n 'cat_Gtp_count/cat_eyesight(right)_count',\n 'cat_Gtp_count/cat_systolic_count',\n 'cat_relaxation_count_label/cat_Gtp_count',\n 'cat_Gtp_count/cat_HDL_count_label',\n 'cat_hemoglobin_count_label/cat_Gtp_count',\n 'cat_Urine protein_count/cat_Gtp_count',\n 'cat_serum creatinine_count/cat_Gtp_count',\n 'cat_ALT_count*cat_Gtp_count',\n 'hemoglobin_unimp_cluster_WOE/waist(cm)_unimp_cluster_WOE',\n 'systolic_unimp_cluster_WOE+Gtp_unimp_cluster_WOE',\n 'hemoglobin_unimp_cluster_WOE/relaxation_unimp_cluster_WOE',\n 'fasting blood sugar_unimp_cluster_WOE*hemoglobin_unimp_cluster_WOE',\n 'Cholesterol_unimp_cluster_WOE/hemoglobin_unimp_cluster_WOE',\n 'triglyceride_unimp_cluster_WOE+Gtp_unimp_cluster_WOE',\n 'HDL_unimp_cluster_WOE*hemoglobin_unimp_cluster_WOE',\n 'LDL_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n 'hemoglobin_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n 'AST_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n 'ALT_unimp_cluster_WOE+Gtp_unimp_cluster_WOE']","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:14:44.009004Z","iopub.execute_input":"2023-11-13T16:14:44.009343Z","iopub.status.idle":"2023-11-13T16:14:44.019158Z","shell.execute_reply.started":"2023-11-13T16:14:44.009312Z","shell.execute_reply":"2023-11-13T16:14:44.018225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_arithmetic_operations(train_df, test_df, expressions_list):\n    '''\n    We pass the selected arithmetic combinations\n    '''\n    for expression in expressions_list:\n        if expression not in train_df.columns:\n            # Split the expression based on operators (+, -, *, /)\n            parts = expression.split('+') if '+' in expression else \\\n                    expression.split('-') if '-' in expression else \\\n                    expression.split('*') if '*' in expression else \\\n                    expression.split('/')\n\n            # Get the DataFrame column names involved in the operation\n            cols = [col for col in parts]\n\n            # Perform the corresponding arithmetic operation based on the operator in the expression\n            if cols[0] in train_df.columns and cols[1] in train_df.columns:\n                if '+' in expression:\n                    train_df[expression] = train_df[cols[0]] + train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] + test_df[cols[1]]\n                elif '-' in expression:\n                    train_df[expression] = train_df[cols[0]] - train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] - test_df[cols[1]]\n                elif '*' in expression:\n                    train_df[expression] = train_df[cols[0]] * train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] * test_df[cols[1]]\n                elif '/' in expression:\n                    train_df[expression] = train_df[cols[0]] / (train_df[cols[1]]+1e-5)\n                    test_df[expression] = test_df[cols[0]] /( test_df[cols[1]]+1e-5)\n    \n    return train_df, test_df\n\ntrain, test = apply_arithmetic_operations(train, test, new_cols)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:14:44.020488Z","iopub.execute_input":"2023-11-13T16:14:44.021216Z","iopub.status.idle":"2023-11-13T16:14:44.131108Z","shell.execute_reply.started":"2023-11-13T16:14:44.021183Z","shell.execute_reply":"2023-11-13T16:14:44.130335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.5 Feature Elimination","metadata":{}},{"cell_type":"code","source":"first_drop=[ f for f in unimportant_features if f in train.columns]\ntrain=train.drop(columns=first_drop)\ntest=test.drop(columns=first_drop)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:14:44.132188Z","iopub.execute_input":"2023-11-13T16:14:44.132447Z","iopub.status.idle":"2023-11-13T16:14:44.314805Z","shell.execute_reply.started":"2023-11-13T16:14:44.132424Z","shell.execute_reply":"2023-11-13T16:14:44.314013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_drop_list=[]\n\ntable = PrettyTable()\ntable.field_names = ['Original', 'Final Transformation', 'ROV AUC CV']\nthreshold=0.95\n# It is possible that multiple parent features share same child features, so store selected features to avoid selecting the same feature again\nbest_cols=[]\n\nfor col in cont_cols:\n    sub_set=[f for f in train.columns if (str(col) in str(f)) and (train[f].nunique()>2)]\n#     print(sub_set)\n    if len(sub_set)>2:\n        correlated_features = []\n\n        for i, feature in enumerate(sub_set):\n            # Check correlation with all remaining features\n            for j in range(i+1, len(sub_set)):\n                correlation = np.abs(train[feature].corr(train[sub_set[j]]))\n                # If correlation is greater than threshold, add to list of highly correlated features\n                if correlation > threshold:\n                    correlated_features.append(sub_set[j])\n\n        # Remove duplicate features from the list\n        correlated_features = list(set(correlated_features))\n#         print(correlated_features)\n        if len(correlated_features)>=2:\n\n            temp_train=train[correlated_features]\n            temp_test=test[correlated_features]\n            #Scale before applying PCA\n            sc=StandardScaler()\n            temp_train=sc.fit_transform(temp_train)\n            temp_test=sc.transform(temp_test)\n\n            # Initiate PCA\n            pca=TruncatedSVD(n_components=1)\n            x_pca_train=pca.fit_transform(temp_train)\n            x_pca_test=pca.transform(temp_test)\n            x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_final\"])\n            x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_final\"])\n            train=pd.concat([train,x_pca_train],axis='columns')\n            test=pd.concat([test,x_pca_test],axis='columns')\n\n            # Clustering\n            model = KMeans()\n            kmeans = KMeans(n_clusters=10)\n            kmeans.fit(np.array(temp_train))\n            labels_train = kmeans.labels_\n\n            train[col+'_final_cluster'] = labels_train\n            test[col+'_final_cluster'] = kmeans.predict(np.array(temp_test))\n\n\n            correlated_features=correlated_features+[col+\"_pca_comb_final\",col+\"_final_cluster\"]\n\n            # See which transformation along with the original is giving you the best univariate fit with target\n            kf=KFold(n_splits=5, shuffle=True, random_state=42)\n\n            scores=[]\n\n            for f in correlated_features:\n                X=train[[f]].values\n                y=train[\"smoking\"].astype(int).values\n\n                auc=[]\n                for train_idx, val_idx in kf.split(X,y):\n                    X_train,y_train=X[train_idx],y[train_idx]\n                    X_val,y_val=X[val_idx],y[val_idx]\n\n                    model = HistGradientBoostingClassifier (max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n                    model.fit(X_train,y_train)\n                    y_pred = model.predict_proba(X_val)[:,1]\n                    score = roc_auc_score( y_val, y_pred)\n                    auc.append(score)\n                if f not in best_cols:\n                    scores.append((f,np.mean(auc)))\n            best_col, best_auc=sorted(scores, key=lambda x:x[1], reverse=True)[0]\n            best_cols.append(best_col)\n\n            cols_to_drop = [f for f in correlated_features if  f not in best_cols]\n            if cols_to_drop:\n                final_drop_list=final_drop_list+cols_to_drop\n            table.add_row([col,best_col ,best_auc])\n\nprint(table)      ","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:14:44.316002Z","iopub.execute_input":"2023-11-13T16:14:44.316318Z","iopub.status.idle":"2023-11-13T16:16:06.872461Z","shell.execute_reply.started":"2023-11-13T16:14:44.316292Z","shell.execute_reply":"2023-11-13T16:16:06.871547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. FEATURE SELECTION","metadata":{}},{"cell_type":"code","source":"final_features=[f for f in train.columns if f not in ['smoking']]\nfinal_features=[*set(final_features)]\n\nsc=StandardScaler()\n\ntrain_scaled=train.copy()\ntest_scaled=test.copy()\ntrain_scaled[final_features]=sc.fit_transform(train[final_features])\ntest_scaled[final_features]=sc.transform(test[final_features])","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:16:06.873988Z","iopub.execute_input":"2023-11-13T16:16:06.874360Z","iopub.status.idle":"2023-11-13T16:16:14.308550Z","shell.execute_reply.started":"2023-11-13T16:16:06.874326Z","shell.execute_reply":"2023-11-13T16:16:14.307684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_processor(train, test):\n    '''\n    After Scaling, some of the features may be the same and can be eliminated\n    '''\n    cols=[f for f in train.columns if \"smoking\" not in f and \"OHE\" not in f]\n    train_cop=train.copy()\n    test_cop=test.copy()\n    drop_cols=[]\n    for i, feature in enumerate(cols):\n        for j in range(i+1, len(cols)):\n            if sum(abs(train_cop[feature]-train_cop[cols[j]]))==0:\n                if cols[j] not in drop_cols:\n                    drop_cols.append(cols[j])\n    print(drop_cols)\n    train_cop.drop(columns=drop_cols,inplace=True)\n    test_cop.drop(columns=drop_cols,inplace=True)\n    \n    return train_cop, test_cop\n\n                    \ntrain_cop, test_cop=   post_processor(train_scaled, test_scaled)        \n\ntrain_cop.to_csv('train_processed.csv',index=False)\ntest_cop.to_csv('test_processed.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:16:14.309676Z","iopub.execute_input":"2023-11-13T16:16:14.309937Z","iopub.status.idle":"2023-11-13T16:23:24.179178Z","shell.execute_reply.started":"2023-11-13T16:16:14.309914Z","shell.execute_reply":"2023-11-13T16:23:24.178081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_cop.drop(columns=['smoking'])\ny_train = train['smoking'].astype(int)\n\nX_test = test_cop.copy()\n\nprint(X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:23:24.180536Z","iopub.execute_input":"2023-11-13T16:23:24.181271Z","iopub.status.idle":"2023-11-13T16:23:24.469526Z","shell.execute_reply.started":"2023-11-13T16:23:24.181234Z","shell.execute_reply":"2023-11-13T16:23:24.468585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_most_important_features(X_train, y_train, n,model_input):\n    xgb_params = {\n            'n_jobs': -1,\n            'eval_metric': 'logloss',\n            'objective': 'binary:logistic',\n            'tree_method': 'hist',\n            'verbosity': 0,\n            'random_state': 42,\n        }\n    if device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n    lgb_params = {\n            'objective': 'binary',\n            'metric': 'logloss',\n            'boosting_type': 'gbdt',\n            'random_state': 42,\n            'device': device,\n        }\n    cb_params = {\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'AUC',\n            'loss_function': 'Logloss',\n            'random_state': 42,\n            'task_type': device.upper(),\n        }\n    if 'xgb' in model_input:\n        model = xgb.XGBClassifier(**xgb_params)\n    elif 'cat' in model_input:\n        model=CatBoostClassifier(**cb_params)\n    else:\n        model=lgb.LGBMClassifier(**lgb_params)\n        \n    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n    auc_scores = []\n    feature_importances_list = []\n    \n    for train_idx, val_idx in kfold.split(X_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n        model.fit(X_train_fold, y_train_fold, verbose=False)\n        \n        y_pred = model.predict_proba(X_val_fold)[:,1]\n        auc_scores.append(roc_auc_score(y_val_fold, y_pred))\n        feature_importances = model.feature_importances_\n        feature_importances_list.append(feature_importances)\n\n    avg_auc= np.mean(auc_scores)\n    avg_feature_importances = np.mean(feature_importances_list, axis=0)\n\n    feature_importance_list = [(X_train.columns[i], importance) for i, importance in enumerate(avg_feature_importances)]\n    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n    top_n_features = [feature[0] for feature in sorted_features[:n]]\n\n    display_features=top_n_features[:10]\n    \n    sns.set_palette(\"Set2\")\n    plt.figure(figsize=(8, 6))\n    plt.barh(range(len(display_features)), [avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features])\n    plt.yticks(range(len(display_features)), display_features, fontsize=12)\n    plt.xlabel('Average Feature Importance', fontsize=14)\n    plt.ylabel('Features', fontsize=10)\n    plt.title(f'Top {10} of {n} Feature Importances with ROC AUC score {avg_auc}', fontsize=16)\n    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n    plt.grid(axis='x', linestyle='--', alpha=0.7)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n\n    # Add data labels on the bars\n    for index, value in enumerate([avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features]):\n        plt.text(value + 0.005, index, f'{value:.3f}', fontsize=12, va='center')\n\n    plt.tight_layout()\n    plt.show()\n\n    return top_n_features","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:23:24.471126Z","iopub.execute_input":"2023-11-13T16:23:24.471416Z","iopub.status.idle":"2023-11-13T16:23:24.487803Z","shell.execute_reply.started":"2023-11-13T16:23:24.471392Z","shell.execute_reply":"2023-11-13T16:23:24.486807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'cat')\nn_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'xgb')\nn_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'lgbm')","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:23:24.489047Z","iopub.execute_input":"2023-11-13T16:23:24.489366Z","iopub.status.idle":"2023-11-13T16:34:17.718202Z","shell.execute_reply.started":"2023-11-13T16:23:24.489343Z","shell.execute_reply":"2023-11-13T16:34:17.717376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_imp_features=[*set(n_imp_features_xgb+n_imp_features_lgbm+n_imp_features_cat)]#\nprint(f\"{len(n_imp_features)} features have been selected from three algorithms for the final model\")","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:17.719631Z","iopub.execute_input":"2023-11-13T16:34:17.720055Z","iopub.status.idle":"2023-11-13T16:34:17.725547Z","shell.execute_reply.started":"2023-11-13T16:34:17.720006Z","shell.execute_reply":"2023-11-13T16:34:17.724599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=X_train[n_imp_features]\nX_test=X_test[n_imp_features]","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:17.726657Z","iopub.execute_input":"2023-11-13T16:34:17.726931Z","iopub.status.idle":"2023-11-13T16:34:17.790320Z","shell.execute_reply.started":"2023-11-13T16:34:17.726908Z","shell.execute_reply":"2023-11-13T16:34:17.789232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. MODELING","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Class Weights","metadata":{}},{"cell_type":"code","source":"classes = np.unique(y_train)  \nclass_to_index = {cls: idx for idx, cls in enumerate(classes)}\ny_train_numeric = np.array([class_to_index[cls] for cls in y_train])\n\nclass_counts = np.bincount(y_train_numeric)\n\ntotal_samples = len(y_train_numeric)\n\nclass_weights = total_samples / (len(classes) * class_counts)\n\nclass_weights_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\nprint(\"Class counts:\", class_counts)\nprint(\"Total samples:\", total_samples)\nprint(\"Class weights:\", class_weights)\nprint(\"Class weights dictionary:\", class_weights_dict)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:17.791719Z","iopub.execute_input":"2023-11-13T16:34:17.792102Z","iopub.status.idle":"2023-11-13T16:34:17.887631Z","shell.execute_reply.started":"2023-11-13T16:34:17.792047Z","shell.execute_reply":"2023-11-13T16:34:17.886650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Models","metadata":{}},{"cell_type":"code","source":"import tensorflow\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.layers import Dropout","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:17.888834Z","iopub.execute_input":"2023-11-13T16:34:17.889215Z","iopub.status.idle":"2023-11-13T16:34:25.655051Z","shell.execute_reply.started":"2023-11-13T16:34:17.889182Z","shell.execute_reply":"2023-11-13T16:34:25.654158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd=tensorflow.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True)\nrms = tensorflow.keras.optimizers.RMSprop()\nnadam=tensorflow.keras.optimizers.Nadam(\n    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n)\nlrelu = lambda x: tensorflow.keras.activations.relu(x, alpha=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:25.656356Z","iopub.execute_input":"2023-11-13T16:34:25.657031Z","iopub.status.idle":"2023-11-13T16:34:28.182878Z","shell.execute_reply.started":"2023-11-13T16:34:25.657002Z","shell.execute_reply":"2023-11-13T16:34:28.181916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann = Sequential()\nann.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='he_uniform', activation=lrelu))\nann.add(Dropout(0.1))\nann.add(Dense(16,  kernel_initializer='he_uniform', activation=lrelu))\nann.add(Dropout(0.1))\nann.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))\nann.add(Dropout(0.1))\n\nann.add(Dense(1,  kernel_initializer='he_uniform', activation='sigmoid'))\nann.compile(loss=\"binary_crossentropy\", optimizer=sgd,metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:28.184188Z","iopub.execute_input":"2023-11-13T16:34:28.184538Z","iopub.status.idle":"2023-11-13T16:34:28.306560Z","shell.execute_reply.started":"2023-11-13T16:34:28.184506Z","shell.execute_reply":"2023-11-13T16:34:28.305647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n        self.test_size = test_size\n        self.kfold = kfold\n        self.n_splits = n_splits\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold:\n            for random_state in random_state_list:\n                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n\nclass Classifier:\n    def __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.models = self._define_model()\n        self.len_models = len(self.models)\n        \n    def _define_model(self):\n        xgb_params = {\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.1,\n            'max_depth': 4,\n            'subsample': 0.8,\n            'colsample_bytree': 0.1,\n            'n_jobs': -1,\n            'eval_metric': 'logloss',\n            'objective': 'binary:logistic',\n            'tree_method': 'hist',\n            'verbosity': 0,\n            'random_state': self.random_state,\n#             'class_weight':class_weights_dict,\n        }\n        if self.device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n            \n        xgb_params2=xgb_params.copy() \n        xgb_params2['subsample']= 0.3\n        xgb_params2['max_depth']=8\n        xgb_params2['learning_rate']=0.005\n        xgb_params2['colsample_bytree']=0.9\n\n        xgb_params3=xgb_params.copy() \n        xgb_params3['subsample']= 0.6\n        xgb_params3['max_depth']=6\n        xgb_params3['learning_rate']=0.02\n        xgb_params3['colsample_bytree']=0.7      \n        \n        lgb_params = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 8,\n            'learning_rate': 0.02,\n            'subsample': 0.20,\n            'colsample_bytree': 0.56,\n            'reg_alpha': 0.25,\n            'reg_lambda': 5e-08,\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state,\n#             'class_weight':class_weights_dict,\n        }\n        lgb_params2 = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 6,\n            'learning_rate': 0.05,\n            'subsample': 0.20,\n            'colsample_bytree': 0.56,\n            'reg_alpha': 0.25,\n            'reg_lambda': 5e-08,\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state,\n        }\n        lgb_params3=lgb_params.copy()  \n        lgb_params3['subsample']=0.9\n        lgb_params3['reg_lambda']=0.3461495211744402\n        lgb_params3['reg_alpha']=0.3095626288582237\n        lgb_params3['max_depth']=8\n        lgb_params3['learning_rate']=0.007\n        lgb_params3['colsample_bytree']=0.5\n\n        lgb_params4=lgb_params2.copy()  \n        lgb_params4['subsample']=0.7\n        lgb_params4['reg_lambda']=0.1\n        lgb_params4['reg_alpha']=0.2\n        lgb_params4['max_depth']=10\n        lgb_params4['learning_rate']=0.007\n        lgb_params4['colsample_bytree']=0.5\n        cb_params = {\n            'iterations': self.n_estimators,\n            'depth': 6,\n            'learning_rate': 0.1,\n            'l2_leaf_reg': 0.7,\n            'random_strength': 0.2,\n            'max_bin': 200,\n            'od_wait': 65,\n            'one_hot_max_size': 120,\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'AUC',\n            'loss_function': 'Logloss',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n        }\n        cb_sym_params = cb_params.copy()\n        cb_sym_params['grow_policy'] = 'SymmetricTree'\n        cb_loss_params = cb_params.copy()\n        cb_loss_params['grow_policy'] = 'Lossguide'\n        \n        cb_params2=  cb_params.copy()\n        cb_params2['learning_rate']=0.01\n        cb_params2['depth']=8\n        \n        cb_params3={\n            'iterations': self.n_estimators,\n            'random_strength': 0.1, \n            'one_hot_max_size': 70, \n            'max_bin': 100, \n            'learning_rate': 0.008, \n            'l2_leaf_reg': 0.3, \n            'grow_policy': 'Depthwise', \n            'depth': 10, \n            'max_bin': 200,\n            'od_wait': 65,\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'AUC',\n            'loss_function': 'Logloss',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n        }\n        cb_params4=  cb_params.copy()\n        cb_params4['learning_rate']=0.01\n        cb_params4['depth']=12\n        dt_params= {'min_samples_split': 30, 'min_samples_leaf': 10, 'max_depth': 8, 'criterion': 'gini'}\n        \n        models = {\n            'xgb': xgb.XGBClassifier(**xgb_params),\n#             'xgb2': xgb.XGBClassifier(**xgb_params2),\n            'xgb3': xgb.XGBClassifier(**xgb_params3),\n            'lgb': lgb.LGBMClassifier(**lgb_params),\n            'lgb2': lgb.LGBMClassifier(**lgb_params2),\n#             'lgb3': lgb.LGBMClassifier(**lgb_params3),\n#             'lgb4': lgb.LGBMClassifier(**lgb_params4),\n            'cat': CatBoostClassifier(**cb_params),\n#             'cat2': CatBoostClassifier(**cb_params2),\n#             'cat3': CatBoostClassifier(**cb_params2),\n#             'cat4': CatBoostClassifier(**cb_params2),\n#             \"cat_sym\": CatBoostClassifier(**cb_sym_params),\n#             \"cat_loss\": CatBoostClassifier(**cb_loss_params),\n#             'hist_gbm' : HistGradientBoostingClassifier (max_iter=300, learning_rate=0.001,  max_leaf_nodes=80,\n#                                                          max_depth=6,random_state=self.random_state),#class_weight=class_weights_dict, \n#             'gbdt': GradientBoostingClassifier(max_depth=6,  n_estimators=1000,random_state=self.random_state),\n            'lr': LogisticRegression(),\n#             'rf': RandomForestClassifier(max_depth= 9,max_features= 'auto',min_samples_split= 10,\n#                                                           min_samples_leaf= 4,  n_estimators=500,random_state=self.random_state),\n# #             'svc': SVC(gamma=\"auto\", probability=True),\n# #             'knn': KNeighborsClassifier(n_neighbors=5),\n#             'mlp': MLPClassifier(random_state=self.random_state, max_iter=1000),\n#              'etr':ExtraTreesClassifier(min_samples_split=55, min_samples_leaf= 15, max_depth=10,\n#                                        n_estimators=200,random_state=self.random_state),\n            'dt' :DecisionTreeClassifier(**dt_params,random_state=self.random_state),\n#             'ada': AdaBoostClassifier(random_state=self.random_state),\n            'ann':ann,\n                                       \n        }\n        return models","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:28.307927Z","iopub.execute_input":"2023-11-13T16:34:28.308290Z","iopub.status.idle":"2023-11-13T16:34:28.333091Z","shell.execute_reply.started":"2023-11-13T16:34:28.308258Z","shell.execute_reply":"2023-11-13T16:34:28.332211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 Optimize Ensemble Weights","metadata":{}},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state, n_trials=3000):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n        self.n_trials = n_trials\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", -1, 2) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n\n        auc_score = roc_auc_score(y_true, weighted_pred)\n        log_loss_score=log_loss(y_true, weighted_pred)\n        return auc_score#/log_loss_score\n\n    def fit(self, y_true, y_preds):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        pruner = optuna.pruners.HyperbandPruner()\n        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds):\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)\n    \n    def weights(self):\n        return self.weights","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:28.334247Z","iopub.execute_input":"2023-11-13T16:34:28.334613Z","iopub.status.idle":"2023-11-13T16:34:28.347363Z","shell.execute_reply.started":"2023-11-13T16:34:28.334582Z","shell.execute_reply":"2023-11-13T16:34:28.346563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Model Fit","metadata":{}},{"cell_type":"code","source":"kfold = True\nn_splits = 1 if not kfold else 5\nrandom_state = 2023\nrandom_state_list = [42] # used by split_data [71]\nn_estimators = 9999 # 9999\nearly_stopping_rounds = 300\nverbose = False\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits)\n\n# Initialize an array for storing test predictions\ntest_predss = np.zeros(X_test.shape[0])\nensemble_score = []\nweights = []\ntrained_models = {'xgb':[], 'lgb':[]}\n\n    \nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n            \n    # Get a set of Regressor models\n    classifier = Classifier(n_estimators, device, random_state)\n    models = classifier.models\n    \n    # Initialize lists to store oof and test predictions for each base model\n    oof_preds = []\n    test_preds = []\n    \n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    for name, model in models.items():\n        if ('cat' in name) or (\"lgb\" in name) or (\"xgb\" in name):\n            if 'lgb' == name: #categorical_feature=cat_features\n                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#,categorical_feature=cat_features,\n                          early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            elif 'cat' ==name:\n                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#cat_features=cat_features,\n                          early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            else:\n                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        elif name in 'ann':\n            model.fit(X_train_, y_train_, validation_data=(X_val, y_val),batch_size=4, epochs=5,verbose=verbose)\n        else:\n            model.fit(X_train_, y_train_)\n        \n        if name in 'ann':\n            test_pred = np.array(model.predict(X_test))[:, 0]\n            y_val_pred = np.array(model.predict(X_val))[:, 0]\n        else:\n            test_pred = model.predict_proba(X_test)[:, 1]\n            y_val_pred = model.predict_proba(X_val)[:, 1]\n\n        score = roc_auc_score(y_val, y_val_pred)\n#         score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] ROC AUC score: {score:.5f}')\n        \n        oof_preds.append(y_val_pred)\n        test_preds.append(test_pred)\n        \n        if name in trained_models.keys():\n            trained_models[f'{name}'].append(deepcopy(model))\n    # Use Optuna to find the best ensemble weights\n    optweights = OptunaWeights(random_state=random_state)\n    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n    \n    score = roc_auc_score(y_val, y_val_pred)\n#     score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] ------------------>  ROC AUC score {score:.5f}')\n    ensemble_score.append(score)\n    weights.append(optweights.weights)\n    \n    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T16:34:28.352905Z","iopub.execute_input":"2023-11-13T16:34:28.353188Z","iopub.status.idle":"2023-11-13T18:15:21.507395Z","shell.execute_reply.started":"2023-11-13T16:34:28.353164Z","shell.execute_reply":"2023-11-13T18:15:21.506465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean ROC AUC  score of the ensemble\nmean_score = np.mean(ensemble_score)\nstd_score = np.std(ensemble_score)\nprint(f'Ensemble ROC AUC score {mean_score:.5f} Â± {std_score:.5f}')\n\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name}: {mean_weight:.5f} Â± {std_weight:.5f}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-13T18:15:21.508883Z","iopub.execute_input":"2023-11-13T18:15:21.509605Z","iopub.status.idle":"2023-11-13T18:15:21.517104Z","shell.execute_reply.started":"2023-11-13T18:15:21.509570Z","shell.execute_reply":"2023-11-13T18:15:21.516012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.5 Feature Importance Visualization","metadata":{}},{"cell_type":"code","source":"def visualize_importance(models, feature_cols, title, head=15):\n    importances = []\n    feature_importance = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df[\"importance\"] = model.feature_importances_\n        _df[\"feature\"] = pd.Series(feature_cols)\n        _df[\"fold\"] = i\n        _df = _df.sort_values('importance', ascending=False)\n        _df = _df.head(head)\n        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n        \n    feature_importance = feature_importance.sort_values('importance', ascending=False)\n    # display(feature_importance.groupby([\"feature\"]).mean().reset_index().drop('fold', axis=1))\n    plt.figure(figsize=(18, 10))\n    sns.barplot(x='importance', y='feature', data=feature_importance, color= (0.4, 0.76, 0.65), errorbar='sd')\n    plt.xlabel('Importance', fontsize=14)\n    plt.ylabel('Feature', fontsize=14)\n    plt.title(f'{title} Feature Importance', fontsize=18)\n    plt.grid(True, axis='x')\n    plt.show()\n    \nfor name, models in trained_models.items():\n    visualize_importance(models, list(X_train.columns), name)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T18:15:21.518346Z","iopub.execute_input":"2023-11-13T18:15:21.518654Z","iopub.status.idle":"2023-11-13T18:15:22.375746Z","shell.execute_reply.started":"2023-11-13T18:15:21.518628Z","shell.execute_reply":"2023-11-13T18:15:22.374791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.6 Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/playground-series-s3e24/sample_submission.csv')\nsub['smoking'] =  test_predss\nsub.to_csv('submission_pure.csv',index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T18:15:22.377107Z","iopub.execute_input":"2023-11-13T18:15:22.377446Z","iopub.status.idle":"2023-11-13T18:15:22.805333Z","shell.execute_reply.started":"2023-11-13T18:15:22.377415Z","shell.execute_reply":"2023-11-13T18:15:22.804384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.7 Ensemble - Averaging","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Let's also take the best notebooks at the top compute porbabilistic averages after scaling. This will be combined with the above model predictions</font>\n* Notebook by [@cv13j0](https://www.kaggle.com/code/cv13j0/efficient-prediction-of-smoker-status)\n* Notebook by [@yaaangzhou](https://www.kaggle.com/code/yaaangzhou/pg-s3-e24-eda-modeling-ensemle-nn)\n* Notebook by [@paddykb](https://www.kaggle.com/code/paddykb/pg-s3e24-brute-force-and-ignorance)\n* Notebook by [@alexryzhkov](https://www.kaggle.com/code/alexryzhkov/lb-0-88048-simple-smoking-ensemble)","metadata":{}},{"cell_type":"code","source":"sub1=pd.read_csv('/kaggle/input/lb-0-88048-simple-smoking-ensemble/combo_50_35_15.csv')\nsub2=pd.read_csv(\"/kaggle/input/pg-s3e24-brute-force-and-ignorance/submission.csv\")\nsub3=pd.read_csv(\"/kaggle/input/ps-s3e24-smoker-status-predictions/submission.csv\")\nsub4=pd.read_csv(\"/kaggle/input/ps-s3-ep24-eda-modeling-submission/Voting_Stacker_full_cv_baseline_submission.csv\")\nsub5=pd.read_csv(\"/kaggle/input/efficient-prediction-of-smoker-status/xgb_pseudo_opt_submission.csv\")\nsub6=pd.read_csv(\"/kaggle/input/pg-s3-e24-eda-modeling-ensemle-nn/nn_submission.csv\")\n\n\ndef scale(df):\n    df['smoking']=(df['smoking']-df['smoking'].min())/(df['smoking'].max()-df['smoking'].min())\n    return df\n\nsub_combined=sub1.copy() \n\nsub1=scale(sub1)\nsub2=scale(sub2)\nsub3=scale(sub3)\nsub4=scale(sub4)\nsub5=scale(sub5)\nsub6=scale(sub6)\n\nsub=scale(sub)\n\nsub_combined['smoking']=(5*sub1['smoking'] + 4*sub2['smoking'] + 5*sub[\"smoking\"]+1/4*sub3[\"smoking\"]\n                         +1/16*sub4[\"smoking\"]+1/64*sub5[\"smoking\"]+1/256*sub6[\"smoking\"]) \n\nsub_combined=scale(sub_combined)\nsub_combined.to_csv('submission5.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T18:43:21.588097Z","iopub.execute_input":"2023-11-13T18:43:21.588998Z","iopub.status.idle":"2023-11-13T18:43:22.175099Z","shell.execute_reply.started":"2023-11-13T18:43:21.588962Z","shell.execute_reply":"2023-11-13T18:43:22.174302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}