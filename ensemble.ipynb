{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T15:23:35.069066Z",
     "iopub.status.busy": "2023-11-13T15:23:35.068812Z",
     "iopub.status.idle": "2023-11-13T15:25:36.101909Z",
     "shell.execute_reply": "2023-11-13T15:25:36.100776Z",
     "shell.execute_reply.started": "2023-11-13T15:23:35.069040Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from prettytable import PrettyTable\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', font_scale=1.4)\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "tqdm_notebook.get_lock().locks = []\n",
    "# !pip install sweetviz\n",
    "# import sweetviz as sv\n",
    "import concurrent.futures\n",
    "from copy import deepcopy       \n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "import random\n",
    "from random import randint, uniform\n",
    "import gc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from itertools import combinations\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xg\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\n",
    "from sklearn.cluster import KMeans\n",
    "!pip install yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "!pip install gap-stat\n",
    "from gap_statistic.optimalK import OptimalK\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import boxcox\n",
    "import math\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "!pip install optuna\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "!pip install catboost\n",
    "!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n",
    "import lightgbm as lgb\n",
    "!pip install category_encoders\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\n",
    "!pip install -U imbalanced-learn\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import Pool\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T15:25:36.105593Z",
     "iopub.status.busy": "2023-11-13T15:25:36.104428Z",
     "iopub.status.idle": "2023-11-13T15:25:37.143895Z",
     "shell.execute_reply": "2023-11-13T15:25:37.142918Z",
     "shell.execute_reply.started": "2023-11-13T15:25:36.105563Z"
    }
   },
   "outputs": [],
   "source": [
    "global device\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "train=pd.read_csv('/kaggle/input/playground-series-s3e24/train.csv')\n",
    "test=pd.read_csv('/kaggle/input/playground-series-s3e24/test.csv')\n",
    "original=pd.read_csv(\"/kaggle/input/smoker-status-prediction-using-biosignals/train_dataset.csv\")\n",
    "\n",
    "train.drop(columns=[\"id\"],inplace=True)\n",
    "test.drop(columns=[\"id\"],inplace=True)\n",
    "\n",
    "train_copy=train.copy()\n",
    "test_copy=test.copy()\n",
    "original_copy=original.copy()\n",
    "\n",
    "original[\"original\"]=1\n",
    "\n",
    "train[\"original\"]=0\n",
    "test[\"original\"]=0\n",
    "\n",
    "train=pd.concat([train,original],axis=0)\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T15:25:37.145279Z",
     "iopub.status.busy": "2023-11-13T15:25:37.144985Z",
     "iopub.status.idle": "2023-11-13T15:25:37.172040Z",
     "shell.execute_reply": "2023-11-13T15:25:37.171116Z",
     "shell.execute_reply.started": "2023-11-13T15:25:37.145253Z"
    }
   },
   "outputs": [],
   "source": [
    "table = PrettyTable()\n",
    "\n",
    "table.field_names = ['Feature', 'Data Type', 'Train Missing %', 'Test Missing %',\"Original Missing%\"]\n",
    "for column in train_copy.columns:\n",
    "    data_type = str(train_copy[column].dtype)\n",
    "    non_null_count_train= np.round(100-train_copy[column].count()/train_copy.shape[0]*100,1)\n",
    "    if column!='smoking':\n",
    "        non_null_count_test = np.round(100-test_copy[column].count()/test_copy.shape[0]*100,1)\n",
    "    else:\n",
    "        non_null_count_test=\"NA\"\n",
    "    non_null_count_orig= np.round(100-original_copy[column].count()/original_copy.shape[0]*100,1)\n",
    "    table.add_row([column, data_type, non_null_count_train,non_null_count_test,non_null_count_orig])\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T15:25:37.175583Z",
     "iopub.status.busy": "2023-11-13T15:25:37.175009Z",
     "iopub.status.idle": "2023-11-13T15:25:37.863533Z",
     "shell.execute_reply": "2023-11-13T15:25:37.862614Z",
     "shell.execute_reply.started": "2023-11-13T15:25:37.175555Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pie_chart(data, title, ax):\n",
    "    data_counts = data['smoking'].value_counts()\n",
    "    labels = data_counts.index\n",
    "    sizes = data_counts.values\n",
    "    colors = [ (0.3, 0.6, 0.6), 'crimson']  \n",
    "    explode = (0.1, 0)  \n",
    "\n",
    "    ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "    ax.axis('equal') \n",
    "    ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))  # Create three subplots in a row\n",
    "\n",
    "plot_pie_chart(train_copy, \"Train smoking status Distribution\", axes[0])\n",
    "plot_pie_chart(original, \"Original smoking status Distribution\", axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T15:25:37.865365Z",
     "iopub.status.busy": "2023-11-13T15:25:37.864937Z",
     "iopub.status.idle": "2023-11-13T15:25:59.694011Z",
     "shell.execute_reply": "2023-11-13T15:25:59.693138Z",
     "shell.execute_reply.started": "2023-11-13T15:25:37.865332Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_cols = [f for f in train.columns if train[f].dtype != 'O' and train[f].nunique() > 2]\n",
    "n_rows = len(cont_cols)\n",
    "fig, axs = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\n",
    "sns.set_palette(\"Set3\")\n",
    "for i, col in enumerate(cont_cols):\n",
    "    sns.violinplot(x='smoking', y=col, data=train_copy, ax=axs[i, 0])\n",
    "    axs[i, 0].set_title(f'{col.title()} Distribution by Target (Train)', fontsize=14)\n",
    "    axs[i, 0].set_xlabel('smoking', fontsize=12)\n",
    "    axs[i, 0].set_ylabel(col.title(), fontsize=12)\n",
    "    sns.despine()\n",
    "\n",
    "    sns.violinplot(x='smoking', y=col, data=original, ax=axs[i, 1])\n",
    "    axs[i, 1].set_title(f'{col.title()} Distribution by Target (Original)', fontsize=14)\n",
    "    axs[i, 1].set_xlabel('smoking', fontsize=12)\n",
    "    axs[i, 1].set_ylabel(col.title(), fontsize=12)\n",
    "    sns.despine()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T15:25:59.695998Z",
     "iopub.status.busy": "2023-11-13T15:25:59.695378Z",
     "iopub.status.idle": "2023-11-13T15:31:30.217537Z",
     "shell.execute_reply": "2023-11-13T15:31:30.216142Z",
     "shell.execute_reply.started": "2023-11-13T15:25:59.695968Z"
    }
   },
   "outputs": [],
   "source": [
    "pair_plot_cols=[f for f in cont_cols if original[f].nunique()>50]\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "plt.figure(figsize=(18, 10))\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.pairplot(data=original, vars=pair_plot_cols,diag_kind='kde', \n",
    "        kind='scatter', palette='muted', \n",
    "        plot_kws={'s': 20}, hue='smoking')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T15:31:30.220017Z",
     "iopub.status.busy": "2023-11-13T15:31:30.219553Z",
     "iopub.status.idle": "2023-11-13T15:31:30.275575Z",
     "shell.execute_reply": "2023-11-13T15:31:30.274589Z",
     "shell.execute_reply.started": "2023-11-13T15:31:30.219972Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_extra_features(df):\n",
    "    best = np.where(df['hearing(left)'] < df['hearing(right)'], \n",
    "                    df['hearing(left)'],  df['hearing(right)'])\n",
    "    worst = np.where(df['hearing(left)'] < df['hearing(right)'], \n",
    "                     df['hearing(right)'],  df['hearing(left)'])\n",
    "    df['hearing(left)'] = best - 1\n",
    "    df['hearing(right)'] = worst - 1\n",
    "    \n",
    "    df['eyesight(left)'] = np.where(df['eyesight(left)'] > 9, 0, df['eyesight(left)'])\n",
    "    df['eyesight(right)'] = np.where(df['eyesight(right)'] > 9, 0, df['eyesight(right)'])\n",
    "    best = np.where(df['eyesight(left)'] < df['eyesight(right)'], \n",
    "                    df['eyesight(left)'],  df['eyesight(right)'])\n",
    "    worst = np.where(df['eyesight(left)'] < df['eyesight(right)'], \n",
    "                     df['eyesight(right)'],  df['eyesight(left)'])\n",
    "    df['eyesight(left)'] = best\n",
    "    df['eyesight(right)'] = worst\n",
    "    ##\n",
    "    df['Gtp'] = np.clip(df['Gtp'], 0, 300)\n",
    "    df['HDL'] = np.clip(df['HDL'], 0, 110)\n",
    "    df['LDL'] = np.clip(df['LDL'], 0, 200)\n",
    "    df['ALT'] = np.clip(df['ALT'], 0, 150)\n",
    "    df['AST'] = np.clip(df['AST'], 0, 100)\n",
    "    df['serum creatinine'] = np.clip(df['serum creatinine'], 0, 3)  \n",
    "    \n",
    "    return df\n",
    "train=create_extra_features(train)\n",
    "test=create_extra_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T15:31:30.277322Z",
     "iopub.status.busy": "2023-11-13T15:31:30.276955Z",
     "iopub.status.idle": "2023-11-13T15:31:30.303589Z",
     "shell.execute_reply": "2023-11-13T15:31:30.302687Z",
     "shell.execute_reply.started": "2023-11-13T15:31:30.277296Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_max_scaler(train, test, column):\n",
    "    '''\n",
    "    Min Max just based on train might have an issue if test has extreme values, hence changing the denominator uding overall min and max\n",
    "    '''\n",
    "    sc=MinMaxScaler()\n",
    "    \n",
    "    max_val=max(train[column].max(),test[column].max())\n",
    "    min_val=min(train[column].min(),test[column].min())\n",
    "\n",
    "    train[column]=(train[column]-min_val)/(max_val-min_val)\n",
    "    test[column]=(test[column]-min_val)/(max_val-min_val)\n",
    "    \n",
    "    return train,test  \n",
    "\n",
    "def OHE(train_df,test_df,cols,target):\n",
    "    '''\n",
    "    Function for one hot encoding, it first combines the data so that no category is missed and\n",
    "    the category with least frequency can be dropped because of redundancy\n",
    "    '''\n",
    "    combined = pd.concat([train_df, test_df], axis=0)\n",
    "    for col in cols:\n",
    "        one_hot = pd.get_dummies(combined[col])\n",
    "        counts = combined[col].value_counts()\n",
    "        min_count_category = counts.idxmin()\n",
    "        one_hot = one_hot.drop(min_count_category, axis=1)\n",
    "        one_hot.columns=[str(f)+col+\"_OHE\" for f in one_hot.columns]\n",
    "        combined = pd.concat([combined, one_hot], axis=\"columns\")\n",
    "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
    "    \n",
    "    # split back to train and test dataframes\n",
    "    train_ohe = combined[:len(train_df)]\n",
    "    test_ohe = combined[len(train_df):]\n",
    "    test_ohe.reset_index(inplace=True,drop=True)\n",
    "    test_ohe.drop(columns=[target],inplace=True)\n",
    "    return train_ohe, test_ohe\n",
    "\n",
    "lgb_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 6,\n",
    "            \"num_leaves\": 16,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.8,\n",
    "            #'reg_alpha': 0.25,\n",
    "            'reg_lambda': 5e-07,\n",
    "            'objective': 'regression_l2',\n",
    "            'metric': 'mean_squared_error',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 42,\n",
    "            'device': device,\n",
    "        }\n",
    "def rmse(y1,y2):\n",
    "    ''' RMSE Evaluator'''\n",
    "    return(np.sqrt(mean_squared_error(np.array(y1),np.array(y2))))\n",
    "\n",
    "def store_missing_rows(df, features):\n",
    "    '''Function stores where missing values are located for given set of features'''\n",
    "    missing_rows = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        missing_rows[feature] = df[df[feature].isnull()]\n",
    "    \n",
    "    return missing_rows\n",
    "\n",
    "def fill_missing_numerical(train,test,target, max_iterations=10):\n",
    "    '''Iterative Missing Imputer: Updates filled missing values iteratively using CatBoost Algorithm'''\n",
    "    train_temp=train.copy()\n",
    "    if target in train_temp.columns:\n",
    "        train_temp=train_temp.drop(columns=target)\n",
    "        \n",
    "    \n",
    "    df=pd.concat([train_temp,test],axis=\"rows\")\n",
    "    df=df.reset_index(drop=True)\n",
    "    features=[ f for f in df.columns if df[f].isna().sum()>0]\n",
    "    if len(features)>0:\n",
    "        # Step 1: Store the instances with missing values in each feature\n",
    "        missing_rows = store_missing_rows(df, features)\n",
    "\n",
    "        # Step 2: Initially fill all missing values with \"Missing\"\n",
    "        for f in features:\n",
    "            df[f]=df[f].fillna(df[f].mean())\n",
    "\n",
    "        cat_features=[f for f in df.columns if not pd.api.types.is_numeric_dtype(df[f])]\n",
    "        dictionary = {feature: [] for feature in features}\n",
    "\n",
    "        for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n",
    "            for feature in features:\n",
    "                # Skip features with no missing values\n",
    "                rows_miss = missing_rows[feature].index\n",
    "\n",
    "                missing_temp = df.loc[rows_miss].copy()\n",
    "                non_missing_temp = df.drop(index=rows_miss).copy()\n",
    "                y_pred_prev=missing_temp[feature]\n",
    "                missing_temp = missing_temp.drop(columns=[feature])\n",
    "\n",
    "\n",
    "                # Step 3: Use the remaining features to predict missing values using Random Forests\n",
    "                X_train = non_missing_temp.drop(columns=[feature])\n",
    "                y_train = non_missing_temp[[feature]]\n",
    "\n",
    "                model= lgb.LGBMRegressor(**lgb_params)\n",
    "                model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "                # Step 4: Predict missing values for the feature and update all N features\n",
    "                y_pred = model.predict(missing_temp)\n",
    "                df.loc[rows_miss, feature] = y_pred\n",
    "                error_minimize=rmse(y_pred,y_pred_prev)\n",
    "                dictionary[feature].append(error_minimize)  # Append the error_minimize value\n",
    "\n",
    "#         for feature, values in dictionary.items():\n",
    "#             iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n",
    "#             plt.plot(iterations, values, label=feature)  # plot the values\n",
    "#             plt.xlabel('Iterations')\n",
    "#             plt.ylabel('RMSE')\n",
    "#             plt.title('Minimization of RMSE with iterations')\n",
    "#             plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "#         plt.show()\n",
    "        train[features] = np.array(df.iloc[:train.shape[0]][features])\n",
    "        test[features] = np.array(df.iloc[train.shape[0]:][features])\n",
    "\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T15:31:30.305464Z",
     "iopub.status.busy": "2023-11-13T15:31:30.305155Z",
     "iopub.status.idle": "2023-11-13T15:35:29.447692Z",
     "shell.execute_reply": "2023-11-13T15:35:29.446709Z",
     "shell.execute_reply.started": "2023-11-13T15:31:30.305438Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_cols = [f for f in train.columns if pd.api.types.is_numeric_dtype(train[f]) and train[f].nunique() >2]\n",
    "cat_cols = [f for f in train.columns if train[f].nunique()!=2and f not in ['smoking']]\n",
    "\n",
    "sc=MinMaxScaler()\n",
    "\n",
    "global unimportant_features\n",
    "global overall_best_score\n",
    "global overall_best_col\n",
    "unimportant_features=[]\n",
    "overall_best_score=0\n",
    "overall_best_col='none'\n",
    "\n",
    "for col in cont_cols:\n",
    "     train, test=min_max_scaler(train, test, col)\n",
    "\n",
    "def transformer(train, test,cont_cols, target):\n",
    "    '''\n",
    "    Algorithm applies multiples transformations on selected columns and finds the best transformation using a single variable model performance\n",
    "    '''\n",
    "    global unimportant_features\n",
    "    global overall_best_score\n",
    "    global overall_best_col\n",
    "    train_copy = train.copy()\n",
    "    test_copy = test.copy()\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Feature', 'Initial ROC_AUC', 'Transformation', 'Tranformed ROC_AUC']\n",
    "\n",
    "    for col in cont_cols:\n",
    "        \n",
    "        for c in [\"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col, \"log_sqrt\"+col, \"pow_\"+col, \"pow2_\"+col]:\n",
    "            if c in train_copy.columns:\n",
    "                train_copy = train_copy.drop(columns=[c])\n",
    "        \n",
    "        # Log Transformation after MinMax Scaling (keeps data between 0 and 1)\n",
    "        train_copy[\"log_\"+col] = np.log1p(train_copy[col])\n",
    "        test_copy[\"log_\"+col] = np.log1p(test_copy[col])\n",
    "        \n",
    "        # Square Root Transformation\n",
    "        train_copy[\"sqrt_\"+col] = np.sqrt(train_copy[col])\n",
    "        test_copy[\"sqrt_\"+col] = np.sqrt(test_copy[col])\n",
    "        \n",
    "        # Box-Cox transformation\n",
    "        combined_data = pd.concat([train_copy[[col]], test_copy[[col]]], axis=0)\n",
    "        epsilon = 1e-5\n",
    "        transformer = PowerTransformer(method='box-cox')\n",
    "        scaled_data = transformer.fit_transform(combined_data + epsilon)\n",
    "\n",
    "        train_copy[\"bx_cx_\" + col] = scaled_data[:train_copy.shape[0]]\n",
    "        test_copy[\"bx_cx_\" + col] = scaled_data[train_copy.shape[0]:]\n",
    "        # Yeo-Johnson transformation\n",
    "        transformer = PowerTransformer(method='yeo-johnson')\n",
    "        train_copy[\"y_J_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
    "        test_copy[\"y_J_\"+col] = transformer.transform(test_copy[[col]])\n",
    "        \n",
    "        # Power transformation, 0.25\n",
    "        power_transform = lambda x: np.power(x + 1 - np.min(x), 0.25)\n",
    "        transformer = FunctionTransformer(power_transform)\n",
    "        train_copy[\"pow_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
    "        test_copy[\"pow_\"+col] = transformer.transform(test_copy[[col]])\n",
    "        \n",
    "        # Power transformation, 2\n",
    "        power_transform = lambda x: np.power(x + 1 - np.min(x), 2)\n",
    "        transformer = FunctionTransformer(power_transform)\n",
    "        train_copy[\"pow2_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
    "        test_copy[\"pow2_\"+col] = transformer.transform(test_copy[[col]])\n",
    "        \n",
    "        # Log to power transformation\n",
    "        train_copy[\"log_sqrt\"+col] = np.log1p(train_copy[\"sqrt_\"+col])\n",
    "        test_copy[\"log_sqrt\"+col] = np.log1p(test_copy[\"sqrt_\"+col])\n",
    "        \n",
    "        temp_cols = [col, \"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col,  \"pow_\"+col , \"pow2_\"+col,\"log_sqrt\"+col]\n",
    "        \n",
    "        train_copy,test_copy = fill_missing_numerical(train_copy,test_copy,\"defects\",5)\n",
    "#         train_copy[temp_cols] = train_copy[temp_cols].fillna(0)\n",
    "#         test_copy[temp_cols] = test_copy[temp_cols].fillna(0)\n",
    "        \n",
    "        pca = TruncatedSVD(n_components=1)\n",
    "        x_pca_train = pca.fit_transform(train_copy[temp_cols])\n",
    "        x_pca_test = pca.transform(test_copy[temp_cols])\n",
    "        x_pca_train = pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb\"])\n",
    "        x_pca_test = pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb\"])\n",
    "        temp_cols.append(col+\"_pca_comb\")\n",
    "        \n",
    "        test_copy = test_copy.reset_index(drop=True)\n",
    "        \n",
    "        train_copy = pd.concat([train_copy, x_pca_train], axis='columns')\n",
    "        test_copy = pd.concat([test_copy, x_pca_test], axis='columns')\n",
    "        \n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        auc_scores = []\n",
    "        \n",
    "        for f in temp_cols:\n",
    "            X = train_copy[[f]].values\n",
    "            y = train_copy[target].values\n",
    "            \n",
    "            auc = []\n",
    "            for train_idx, val_idx in kf.split(X, y):\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                x_val, y_val = X[val_idx], y[val_idx]\n",
    "#                 model =   SVC(gamma=\"auto\", probability=True, random_state=42)\n",
    "                model =   LogisticRegression() # since it is a large dataset, Logistic Regression would be a good option to save time\n",
    "                model.fit(X_train,y_train)\n",
    "                y_pred = model.predict_proba(x_val)[:,1]\n",
    "                auc.append(roc_auc_score(y_val, y_pred))\n",
    "            auc_scores.append((f, np.mean(auc)))\n",
    "            \n",
    "            if overall_best_score < np.mean(auc):\n",
    "                overall_best_score = np.mean(auc)\n",
    "                overall_best_col = f\n",
    "\n",
    "            if f == col:\n",
    "                orig_auc = np.mean(auc)\n",
    "                \n",
    "        best_col, best_auc = sorted(auc_scores, key=lambda x: x[1], reverse=True)[0]\n",
    "        cols_to_drop = [f for f in temp_cols if f != best_col]\n",
    "        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n",
    "        \n",
    "        if cols_to_drop:\n",
    "            unimportant_features = unimportant_features+cols_to_drop\n",
    "        table.add_row([col,orig_auc,best_col ,best_auc])\n",
    "    print(table)   \n",
    "    print(\"overall best CV ROC AUC score: \",overall_best_score)\n",
    "    return train_copy, test_copy\n",
    "\n",
    "train, test= transformer(train, test,cont_cols, \"smoking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T15:35:29.449659Z",
     "iopub.status.busy": "2023-11-13T15:35:29.449027Z",
     "iopub.status.idle": "2023-11-13T15:35:29.503138Z",
     "shell.execute_reply": "2023-11-13T15:35:29.501864Z",
     "shell.execute_reply.started": "2023-11-13T15:35:29.449623Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_cols=[]\n",
    "for col in cat_cols:\n",
    "    train['cat_'+col]=train[col]\n",
    "    test['cat_'+col]=test[col]\n",
    "#     cat_list=test['cat_'+col].unique()\n",
    "#     train['cat_'+col]=train['cat_'+col].apply(lambda x: x if x in cat_list else np.nan)\n",
    "    selected_cols.append('cat_'+col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T15:35:29.510637Z",
     "iopub.status.busy": "2023-11-13T15:35:29.509761Z",
     "iopub.status.idle": "2023-11-13T16:10:28.629406Z",
     "shell.execute_reply": "2023-11-13T16:10:28.628406Z",
     "shell.execute_reply.started": "2023-11-13T15:35:29.510604Z"
    }
   },
   "outputs": [],
   "source": [
    "def high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n",
    "    '''\n",
    "    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied, \n",
    "    where it takes a maximum of n columns and drops the rest of them treating as rare categories\n",
    "    '''\n",
    "    train_copy=train.copy()\n",
    "    test_copy=test.copy()\n",
    "    ohe_cols=[]\n",
    "    for col in extra_cols:\n",
    "        dict1=train_copy[col].value_counts().to_dict()\n",
    "        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n",
    "        rare_keys=list([*ordered.keys()][n_limit:])\n",
    "#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n",
    "        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n",
    "        \n",
    "        train_copy[col]=train_copy[col].replace(rare_key_map)\n",
    "        test_copy[col]=test_copy[col].replace(rare_key_map)\n",
    "    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n",
    "    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n",
    "    train_copy=train_copy.drop(columns=drop_cols)\n",
    "    test_copy=test_copy.drop(columns=drop_cols)\n",
    "    \n",
    "    return train_copy, test_copy\n",
    "\n",
    "def cat_encoding(train, test,cat_cols, target):\n",
    "    '''Takes in a list of features and applied different categorical encoding techniques including One-hot and return the best one using \n",
    "    a single var model and other encoders if they do not have high correlation'''\n",
    "    global overall_best_score\n",
    "    global overall_best_col\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Feature', 'Encoded Feature', 'ROC AUC Score']\n",
    "    train_copy=train.copy()\n",
    "    test_copy=test.copy()\n",
    "    train_dum = train.copy()\n",
    "    for feature in cat_cols:\n",
    "        cat_labels = train_copy.groupby([feature])[target].mean().sort_values().index\n",
    "        cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n",
    "        train_copy[feature + \"_target\"] = train_copy[feature].map(cat_labels2)\n",
    "        test_copy[feature + \"_target\"] = test_copy[feature].map(cat_labels2)\n",
    "\n",
    "        dic = train_copy[feature].value_counts().to_dict()\n",
    "        train_copy[feature + \"_count\"] =train_copy[feature].map(dic)\n",
    "        test_copy[feature + \"_count\"] = test_copy[feature].map(dic)\n",
    "\n",
    "        dic2=train_copy[feature].value_counts().to_dict()\n",
    "        list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n",
    "        # list1=np.arange(len(dic2.values())) # Higher rank for low count\n",
    "        dic3=dict(zip(list(dic2.keys()),list1))\n",
    "        train_copy[feature+\"_count_label\"]=train_copy[feature].replace(dic3).astype(float)\n",
    "        test_copy[feature+\"_count_label\"]=test_copy[feature].replace(dic3).astype(float)\n",
    "\n",
    "        temp_cols = [feature + \"_count\", feature + \"_count_label\"]\n",
    "        if train_copy[feature].dtype=='O':\n",
    "            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n",
    "            train_copy=train_copy.drop(columns=[feature])\n",
    "            test_copy=test_copy.drop(columns=[feature])\n",
    "        else:\n",
    "            if train_copy[feature].nunique()<=50:\n",
    "                train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n",
    "                test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n",
    "                train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n",
    "                train_copy=train_copy.drop(columns=[feature])\n",
    "                test_copy=test_copy.drop(columns=[feature])\n",
    "#                 temp_cols.append(feature)\n",
    "            else:\n",
    "                train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=10)\n",
    "            \n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        auc_scores = []\n",
    "\n",
    "        for f in temp_cols:\n",
    "            X = train_copy[[f]].values\n",
    "            y = train_copy[target].astype(int).values\n",
    "\n",
    "            auc = []\n",
    "            for train_idx, val_idx in kf.split(X, y):\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                x_val, y_val = X[val_idx], y[val_idx]\n",
    "                model =  HistGradientBoostingClassifier (max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict_proba(x_val)[:,1]\n",
    "                auc.append(roc_auc_score(y_val,  y_pred))\n",
    "            auc_scores.append((f, np.mean(auc)))\n",
    "            if overall_best_score < np.mean(auc):\n",
    "                overall_best_score = np.mean(auc)\n",
    "                overall_best_col = f\n",
    "        best_col, best_auc = sorted(auc_scores, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "        corr = train_copy[temp_cols].corr(method='pearson')\n",
    "        corr_with_best_col = corr[best_col]\n",
    "        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n",
    "        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n",
    "        if cols_to_drop:\n",
    "            train_copy = train_copy.drop(columns=cols_to_drop)\n",
    "            test_copy = test_copy.drop(columns=cols_to_drop)\n",
    "\n",
    "        table.add_row([feature, best_col, best_auc])\n",
    "        print(feature)\n",
    "    print(table)\n",
    "    print(\"overall best CV score: \", overall_best_score)\n",
    "    return train_copy, test_copy\n",
    "\n",
    "train, test= cat_encoding(train, test,selected_cols, \"smoking\")\n",
    "train, test = fill_missing_numerical(train, test,\"smoking\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:10:28.631767Z",
     "iopub.status.busy": "2023-11-13T16:10:28.630996Z",
     "iopub.status.idle": "2023-11-13T16:14:43.971734Z",
     "shell.execute_reply": "2023-11-13T16:14:43.970640Z",
     "shell.execute_reply.started": "2023-11-13T16:10:28.631728Z"
    }
   },
   "outputs": [],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = ['Clustered Feature', 'ROC AUC (CV-TRAIN)']\n",
    "for col in cont_cols:\n",
    "    sub_set=[f for f in unimportant_features if col in f]\n",
    "    temp_train=train[sub_set]\n",
    "    temp_test=test[sub_set]\n",
    "    sc=StandardScaler()\n",
    "    temp_train=sc.fit_transform(temp_train)\n",
    "    temp_test=sc.transform(temp_test)\n",
    "    model = KMeans()\n",
    "\n",
    "    # print(ideal_clusters)\n",
    "    kmeans = KMeans(n_clusters=10)\n",
    "    kmeans.fit(np.array(temp_train))\n",
    "    labels_train = kmeans.labels_\n",
    "\n",
    "    train[col+\"_unimp_cluster_WOE\"] = labels_train\n",
    "    test[col+\"_unimp_cluster_WOE\"] = kmeans.predict(np.array(temp_test))\n",
    "\n",
    "    \n",
    "    kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    X=train[[col+\"_unimp_cluster_WOE\"]].values\n",
    "    y=train[\"smoking\"].astype(int).values\n",
    "\n",
    "    auc=[]\n",
    "    for train_idx, val_idx in kf.split(X,y):\n",
    "        X_train,y_train=X[train_idx],y[train_idx]\n",
    "        x_val,y_val=X[val_idx],y[val_idx]\n",
    "        model = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(x_val)[:,1]\n",
    "        auc.append(roc_auc_score(y_val,y_pred))\n",
    "        \n",
    "    table.add_row([col+\"_unimp_cluster_WOE\",np.mean(auc)])\n",
    "    if overall_best_score<np.mean(auc):\n",
    "        overall_best_score=np.mean(auc)\n",
    "        overall_best_col=col+\"_unimp_cluster_WOE\"\n",
    "\n",
    "print(table)\n",
    "print(\"overall best CV score: \", overall_best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:14:43.973614Z",
     "iopub.status.busy": "2023-11-13T16:14:43.973313Z",
     "iopub.status.idle": "2023-11-13T16:14:43.991355Z",
     "shell.execute_reply": "2023-11-13T16:14:43.990442Z",
     "shell.execute_reply.started": "2023-11-13T16:14:43.973586Z"
    }
   },
   "outputs": [],
   "source": [
    "def better_features(train, test, target, cols, best_score):\n",
    "    new_cols = []\n",
    "    skf = KFold(n_splits=5, shuffle=True, random_state=42)  # Stratified k-fold object\n",
    "    best_list=[]\n",
    "    for i in tqdm(range(len(cols)), desc='Generating Columns'):\n",
    "        col1 = cols[i]\n",
    "        temp_df = pd.DataFrame()  # Temporary dataframe to store the generated columns\n",
    "        temp_df_test = pd.DataFrame()  # Temporary dataframe for test data\n",
    "\n",
    "        for j in range(i+1, len(cols)):\n",
    "            col2 = cols[j]\n",
    "            # Multiply\n",
    "            temp_df[col1 + '*' + col2] = train[col1] * train[col2]\n",
    "            temp_df_test[col1 + '*' + col2] = test[col1] * test[col2]\n",
    "\n",
    "            # Divide (col1 / col2)\n",
    "            temp_df[col1 + '/' + col2] = train[col1] / (train[col2] + 1e-5)\n",
    "            temp_df_test[col1 + '/' + col2] = test[col1] / (test[col2] + 1e-5)\n",
    "\n",
    "            # Divide (col2 / col1)\n",
    "            temp_df[col2 + '/' + col1] = train[col2] / (train[col1] + 1e-5)\n",
    "            temp_df_test[col2 + '/' + col1] = test[col2] / (test[col1] + 1e-5)\n",
    "\n",
    "            # Subtract\n",
    "            temp_df[col1 + '-' + col2] = train[col1] - train[col2]\n",
    "            temp_df_test[col1 + '-' + col2] = test[col1] - test[col2]\n",
    "\n",
    "            # Add\n",
    "            temp_df[col1 + '+' + col2] = train[col1] + train[col2]\n",
    "            temp_df_test[col1 + '+' + col2] = test[col1] + test[col2]\n",
    "\n",
    "        SCORES = []\n",
    "        for column in temp_df.columns:\n",
    "            scores = []\n",
    "            for train_index, val_index in skf.split(train, train[target]):\n",
    "                X_train, X_val = temp_df[column].iloc[train_index].values.reshape(-1, 1), temp_df[column].iloc[val_index].values.reshape(-1, 1)\n",
    "                y_train, y_val = train[target].astype(int).iloc[train_index], train[target].astype(int).iloc[val_index]\n",
    "                model = LogisticRegression()#HistGradientBoostingClassifier(max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict_proba(X_val)[:,1]\n",
    "                score = roc_auc_score( y_val, y_pred)\n",
    "                scores.append(score)\n",
    "            mean_score = np.mean(scores)\n",
    "            SCORES.append((column, mean_score))\n",
    "\n",
    "        if SCORES:\n",
    "            best_col, best_auc = sorted(SCORES, key=lambda x: x[1],reverse=True)[0]\n",
    "            corr_with_other_cols = train.drop([target] + new_cols, axis=1).corrwith(temp_df[best_col])\n",
    "            if (corr_with_other_cols.abs().max() < 0.9 or best_auc > best_score) and corr_with_other_cols.abs().max() !=1 :\n",
    "                train[best_col] = temp_df[best_col]\n",
    "                test[best_col] = temp_df_test[best_col]\n",
    "                new_cols.append(best_col)\n",
    "                print(f\"Added column '{best_col}' with ROC AUC Score: {best_auc:.4f} & Correlation {corr_with_other_cols.abs().max():.4f}\")\n",
    "\n",
    "    return train, test, new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T16:14:43.992726Z",
     "iopub.status.busy": "2023-11-13T16:14:43.992452Z",
     "iopub.status.idle": "2023-11-13T16:14:44.007945Z",
     "shell.execute_reply": "2023-11-13T16:14:44.007114Z",
     "shell.execute_reply.started": "2023-11-13T16:14:43.992701Z"
    }
   },
   "outputs": [],
   "source": [
    "# selected_features=[f for f in train.columns if train[f].nunique()>2 and f not in unimportant_features]\n",
    "# train, test,new_cols=better_features(train, test, 'smoking', selected_features, overall_best_score)\n",
    "# new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:14:44.009343Z",
     "iopub.status.busy": "2023-11-13T16:14:44.009004Z",
     "iopub.status.idle": "2023-11-13T16:14:44.019158Z",
     "shell.execute_reply": "2023-11-13T16:14:44.018225Z",
     "shell.execute_reply.started": "2023-11-13T16:14:44.009312Z"
    }
   },
   "outputs": [],
   "source": [
    "new_cols=['height(cm)*hemoglobin',\n",
    " 'eyesight(left)*Gtp',\n",
    " 'Gtp/systolic',\n",
    " 'Gtp/Cholesterol',\n",
    " 'triglyceride+hemoglobin',\n",
    " 'HDL/Gtp',\n",
    " 'Gtp/LDL',\n",
    " 'hemoglobin+Gtp',\n",
    " 'Gtp/AST',\n",
    " 'ALT*Gtp',\n",
    " 'cat_Gtp/cat_relaxation',\n",
    " 'cat_fasting blood sugar*cat_Gtp',\n",
    " 'cat_triglyceride/cat_Gtp_count',\n",
    " 'cat_HDL*cat_Gtp_count',\n",
    " 'cat_AST*cat_Gtp',\n",
    " 'cat_Gtp*cat_height(cm)_count',\n",
    " 'cat_age_count/cat_Gtp_count',\n",
    " 'cat_Gtp_count/cat_height(cm)_count',\n",
    " 'cat_weight(kg)_count/cat_Gtp_count',\n",
    " 'cat_waist(cm)_count-cat_Gtp_count',\n",
    " 'cat_eyesight(left)_count/cat_Gtp_count',\n",
    " 'cat_Gtp_count/cat_eyesight(right)_count',\n",
    " 'cat_Gtp_count/cat_systolic_count',\n",
    " 'cat_relaxation_count_label/cat_Gtp_count',\n",
    " 'cat_Gtp_count/cat_HDL_count_label',\n",
    " 'cat_hemoglobin_count_label/cat_Gtp_count',\n",
    " 'cat_Urine protein_count/cat_Gtp_count',\n",
    " 'cat_serum creatinine_count/cat_Gtp_count',\n",
    " 'cat_ALT_count*cat_Gtp_count',\n",
    " 'hemoglobin_unimp_cluster_WOE/waist(cm)_unimp_cluster_WOE',\n",
    " 'systolic_unimp_cluster_WOE+Gtp_unimp_cluster_WOE',\n",
    " 'hemoglobin_unimp_cluster_WOE/relaxation_unimp_cluster_WOE',\n",
    " 'fasting blood sugar_unimp_cluster_WOE*hemoglobin_unimp_cluster_WOE',\n",
    " 'Cholesterol_unimp_cluster_WOE/hemoglobin_unimp_cluster_WOE',\n",
    " 'triglyceride_unimp_cluster_WOE+Gtp_unimp_cluster_WOE',\n",
    " 'HDL_unimp_cluster_WOE*hemoglobin_unimp_cluster_WOE',\n",
    " 'LDL_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n",
    " 'hemoglobin_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n",
    " 'AST_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n",
    " 'ALT_unimp_cluster_WOE+Gtp_unimp_cluster_WOE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:14:44.021216Z",
     "iopub.status.busy": "2023-11-13T16:14:44.020488Z",
     "iopub.status.idle": "2023-11-13T16:14:44.131108Z",
     "shell.execute_reply": "2023-11-13T16:14:44.130335Z",
     "shell.execute_reply.started": "2023-11-13T16:14:44.021183Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_arithmetic_operations(train_df, test_df, expressions_list):\n",
    "    '''\n",
    "    We pass the selected arithmetic combinations\n",
    "    '''\n",
    "    for expression in expressions_list:\n",
    "        if expression not in train_df.columns:\n",
    "            # Split the expression based on operators (+, -, *, /)\n",
    "            parts = expression.split('+') if '+' in expression else \\\n",
    "                    expression.split('-') if '-' in expression else \\\n",
    "                    expression.split('*') if '*' in expression else \\\n",
    "                    expression.split('/')\n",
    "\n",
    "            # Get the DataFrame column names involved in the operation\n",
    "            cols = [col for col in parts]\n",
    "\n",
    "            # Perform the corresponding arithmetic operation based on the operator in the expression\n",
    "            if cols[0] in train_df.columns and cols[1] in train_df.columns:\n",
    "                if '+' in expression:\n",
    "                    train_df[expression] = train_df[cols[0]] + train_df[cols[1]]\n",
    "                    test_df[expression] = test_df[cols[0]] + test_df[cols[1]]\n",
    "                elif '-' in expression:\n",
    "                    train_df[expression] = train_df[cols[0]] - train_df[cols[1]]\n",
    "                    test_df[expression] = test_df[cols[0]] - test_df[cols[1]]\n",
    "                elif '*' in expression:\n",
    "                    train_df[expression] = train_df[cols[0]] * train_df[cols[1]]\n",
    "                    test_df[expression] = test_df[cols[0]] * test_df[cols[1]]\n",
    "                elif '/' in expression:\n",
    "                    train_df[expression] = train_df[cols[0]] / (train_df[cols[1]]+1e-5)\n",
    "                    test_df[expression] = test_df[cols[0]] /( test_df[cols[1]]+1e-5)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train, test = apply_arithmetic_operations(train, test, new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:14:44.132447Z",
     "iopub.status.busy": "2023-11-13T16:14:44.132188Z",
     "iopub.status.idle": "2023-11-13T16:14:44.314805Z",
     "shell.execute_reply": "2023-11-13T16:14:44.314013Z",
     "shell.execute_reply.started": "2023-11-13T16:14:44.132424Z"
    }
   },
   "outputs": [],
   "source": [
    "first_drop=[ f for f in unimportant_features if f in train.columns]\n",
    "train=train.drop(columns=first_drop)\n",
    "test=test.drop(columns=first_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:14:44.316318Z",
     "iopub.status.busy": "2023-11-13T16:14:44.316002Z",
     "iopub.status.idle": "2023-11-13T16:16:06.872461Z",
     "shell.execute_reply": "2023-11-13T16:16:06.871547Z",
     "shell.execute_reply.started": "2023-11-13T16:14:44.316292Z"
    }
   },
   "outputs": [],
   "source": [
    "final_drop_list=[]\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Original', 'Final Transformation', 'ROV AUC CV']\n",
    "threshold=0.95\n",
    "# It is possible that multiple parent features share same child features, so store selected features to avoid selecting the same feature again\n",
    "best_cols=[]\n",
    "\n",
    "for col in cont_cols:\n",
    "    sub_set=[f for f in train.columns if (str(col) in str(f)) and (train[f].nunique()>2)]\n",
    "#     print(sub_set)\n",
    "    if len(sub_set)>2:\n",
    "        correlated_features = []\n",
    "\n",
    "        for i, feature in enumerate(sub_set):\n",
    "            # Check correlation with all remaining features\n",
    "            for j in range(i+1, len(sub_set)):\n",
    "                correlation = np.abs(train[feature].corr(train[sub_set[j]]))\n",
    "                # If correlation is greater than threshold, add to list of highly correlated features\n",
    "                if correlation > threshold:\n",
    "                    correlated_features.append(sub_set[j])\n",
    "\n",
    "        # Remove duplicate features from the list\n",
    "        correlated_features = list(set(correlated_features))\n",
    "#         print(correlated_features)\n",
    "        if len(correlated_features)>=2:\n",
    "\n",
    "            temp_train=train[correlated_features]\n",
    "            temp_test=test[correlated_features]\n",
    "            #Scale before applying PCA\n",
    "            sc=StandardScaler()\n",
    "            temp_train=sc.fit_transform(temp_train)\n",
    "            temp_test=sc.transform(temp_test)\n",
    "\n",
    "            # Initiate PCA\n",
    "            pca=TruncatedSVD(n_components=1)\n",
    "            x_pca_train=pca.fit_transform(temp_train)\n",
    "            x_pca_test=pca.transform(temp_test)\n",
    "            x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_final\"])\n",
    "            x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_final\"])\n",
    "            train=pd.concat([train,x_pca_train],axis='columns')\n",
    "            test=pd.concat([test,x_pca_test],axis='columns')\n",
    "\n",
    "            # Clustering\n",
    "            model = KMeans()\n",
    "            kmeans = KMeans(n_clusters=10)\n",
    "            kmeans.fit(np.array(temp_train))\n",
    "            labels_train = kmeans.labels_\n",
    "\n",
    "            train[col+'_final_cluster'] = labels_train\n",
    "            test[col+'_final_cluster'] = kmeans.predict(np.array(temp_test))\n",
    "\n",
    "\n",
    "            correlated_features=correlated_features+[col+\"_pca_comb_final\",col+\"_final_cluster\"]\n",
    "\n",
    "            # See which transformation along with the original is giving you the best univariate fit with target\n",
    "            kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "            scores=[]\n",
    "\n",
    "            for f in correlated_features:\n",
    "                X=train[[f]].values\n",
    "                y=train[\"smoking\"].astype(int).values\n",
    "\n",
    "                auc=[]\n",
    "                for train_idx, val_idx in kf.split(X,y):\n",
    "                    X_train,y_train=X[train_idx],y[train_idx]\n",
    "                    X_val,y_val=X[val_idx],y[val_idx]\n",
    "\n",
    "                    model = HistGradientBoostingClassifier (max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n",
    "                    model.fit(X_train,y_train)\n",
    "                    y_pred = model.predict_proba(X_val)[:,1]\n",
    "                    score = roc_auc_score( y_val, y_pred)\n",
    "                    auc.append(score)\n",
    "                if f not in best_cols:\n",
    "                    scores.append((f,np.mean(auc)))\n",
    "            best_col, best_auc=sorted(scores, key=lambda x:x[1], reverse=True)[0]\n",
    "            best_cols.append(best_col)\n",
    "\n",
    "            cols_to_drop = [f for f in correlated_features if  f not in best_cols]\n",
    "            if cols_to_drop:\n",
    "                final_drop_list=final_drop_list+cols_to_drop\n",
    "            table.add_row([col,best_col ,best_auc])\n",
    "\n",
    "print(table)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:16:06.874360Z",
     "iopub.status.busy": "2023-11-13T16:16:06.873988Z",
     "iopub.status.idle": "2023-11-13T16:16:14.308550Z",
     "shell.execute_reply": "2023-11-13T16:16:14.307684Z",
     "shell.execute_reply.started": "2023-11-13T16:16:06.874326Z"
    }
   },
   "outputs": [],
   "source": [
    "final_features=[f for f in train.columns if f not in ['smoking']]\n",
    "final_features=[*set(final_features)]\n",
    "\n",
    "sc=StandardScaler()\n",
    "\n",
    "train_scaled=train.copy()\n",
    "test_scaled=test.copy()\n",
    "train_scaled[final_features]=sc.fit_transform(train[final_features])\n",
    "test_scaled[final_features]=sc.transform(test[final_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:16:14.309937Z",
     "iopub.status.busy": "2023-11-13T16:16:14.309676Z",
     "iopub.status.idle": "2023-11-13T16:23:24.179178Z",
     "shell.execute_reply": "2023-11-13T16:23:24.178081Z",
     "shell.execute_reply.started": "2023-11-13T16:16:14.309914Z"
    }
   },
   "outputs": [],
   "source": [
    "def post_processor(train, test):\n",
    "    '''\n",
    "    After Scaling, some of the features may be the same and can be eliminated\n",
    "    '''\n",
    "    cols=[f for f in train.columns if \"smoking\" not in f and \"OHE\" not in f]\n",
    "    train_cop=train.copy()\n",
    "    test_cop=test.copy()\n",
    "    drop_cols=[]\n",
    "    for i, feature in enumerate(cols):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            if sum(abs(train_cop[feature]-train_cop[cols[j]]))==0:\n",
    "                if cols[j] not in drop_cols:\n",
    "                    drop_cols.append(cols[j])\n",
    "    print(drop_cols)\n",
    "    train_cop.drop(columns=drop_cols,inplace=True)\n",
    "    test_cop.drop(columns=drop_cols,inplace=True)\n",
    "    \n",
    "    return train_cop, test_cop\n",
    "\n",
    "                    \n",
    "train_cop, test_cop=   post_processor(train_scaled, test_scaled)        \n",
    "\n",
    "train_cop.to_csv('train_processed.csv',index=False)\n",
    "test_cop.to_csv('test_processed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:23:24.181271Z",
     "iopub.status.busy": "2023-11-13T16:23:24.180536Z",
     "iopub.status.idle": "2023-11-13T16:23:24.469526Z",
     "shell.execute_reply": "2023-11-13T16:23:24.468585Z",
     "shell.execute_reply.started": "2023-11-13T16:23:24.181234Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_cop.drop(columns=['smoking'])\n",
    "y_train = train['smoking'].astype(int)\n",
    "\n",
    "X_test = test_cop.copy()\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:23:24.471416Z",
     "iopub.status.busy": "2023-11-13T16:23:24.471126Z",
     "iopub.status.idle": "2023-11-13T16:23:24.487803Z",
     "shell.execute_reply": "2023-11-13T16:23:24.486807Z",
     "shell.execute_reply.started": "2023-11-13T16:23:24.471392Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_most_important_features(X_train, y_train, n,model_input):\n",
    "    xgb_params = {\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'logloss',\n",
    "            'objective': 'binary:logistic',\n",
    "            'tree_method': 'hist',\n",
    "            'verbosity': 0,\n",
    "            'random_state': 42,\n",
    "        }\n",
    "    if device == 'gpu':\n",
    "            xgb_params['tree_method'] = 'gpu_hist'\n",
    "            xgb_params['predictor'] = 'gpu_predictor'\n",
    "    lgb_params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 42,\n",
    "            'device': device,\n",
    "        }\n",
    "    cb_params = {\n",
    "            'grow_policy': 'Depthwise',\n",
    "            'bootstrap_type': 'Bayesian',\n",
    "            'od_type': 'Iter',\n",
    "            'eval_metric': 'AUC',\n",
    "            'loss_function': 'Logloss',\n",
    "            'random_state': 42,\n",
    "            'task_type': device.upper(),\n",
    "        }\n",
    "    if 'xgb' in model_input:\n",
    "        model = xgb.XGBClassifier(**xgb_params)\n",
    "    elif 'cat' in model_input:\n",
    "        model=CatBoostClassifier(**cb_params)\n",
    "    else:\n",
    "        model=lgb.LGBMClassifier(**lgb_params)\n",
    "        \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "    feature_importances_list = []\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "        \n",
    "        y_pred = model.predict_proba(X_val_fold)[:,1]\n",
    "        auc_scores.append(roc_auc_score(y_val_fold, y_pred))\n",
    "        feature_importances = model.feature_importances_\n",
    "        feature_importances_list.append(feature_importances)\n",
    "\n",
    "    avg_auc= np.mean(auc_scores)\n",
    "    avg_feature_importances = np.mean(feature_importances_list, axis=0)\n",
    "\n",
    "    feature_importance_list = [(X_train.columns[i], importance) for i, importance in enumerate(avg_feature_importances)]\n",
    "    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
    "    top_n_features = [feature[0] for feature in sorted_features[:n]]\n",
    "\n",
    "    display_features=top_n_features[:10]\n",
    "    \n",
    "    sns.set_palette(\"Set2\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(range(len(display_features)), [avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features])\n",
    "    plt.yticks(range(len(display_features)), display_features, fontsize=12)\n",
    "    plt.xlabel('Average Feature Importance', fontsize=14)\n",
    "    plt.ylabel('Features', fontsize=10)\n",
    "    plt.title(f'Top {10} of {n} Feature Importances with ROC AUC score {avg_auc}', fontsize=16)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "\n",
    "    # Add data labels on the bars\n",
    "    for index, value in enumerate([avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features]):\n",
    "        plt.text(value + 0.005, index, f'{value:.3f}', fontsize=12, va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return top_n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:23:24.489366Z",
     "iopub.status.busy": "2023-11-13T16:23:24.489047Z",
     "iopub.status.idle": "2023-11-13T16:34:17.718202Z",
     "shell.execute_reply": "2023-11-13T16:34:17.717376Z",
     "shell.execute_reply.started": "2023-11-13T16:23:24.489343Z"
    }
   },
   "outputs": [],
   "source": [
    "n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'cat')\n",
    "n_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'xgb')\n",
    "n_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'lgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:17.720055Z",
     "iopub.status.busy": "2023-11-13T16:34:17.719631Z",
     "iopub.status.idle": "2023-11-13T16:34:17.725547Z",
     "shell.execute_reply": "2023-11-13T16:34:17.724599Z",
     "shell.execute_reply.started": "2023-11-13T16:34:17.720006Z"
    }
   },
   "outputs": [],
   "source": [
    "n_imp_features=[*set(n_imp_features_xgb+n_imp_features_lgbm+n_imp_features_cat)]#\n",
    "print(f\"{len(n_imp_features)} features have been selected from three algorithms for the final model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:17.726931Z",
     "iopub.status.busy": "2023-11-13T16:34:17.726657Z",
     "iopub.status.idle": "2023-11-13T16:34:17.790320Z",
     "shell.execute_reply": "2023-11-13T16:34:17.789232Z",
     "shell.execute_reply.started": "2023-11-13T16:34:17.726908Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train=X_train[n_imp_features]\n",
    "X_test=X_test[n_imp_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:17.792102Z",
     "iopub.status.busy": "2023-11-13T16:34:17.791719Z",
     "iopub.status.idle": "2023-11-13T16:34:17.887631Z",
     "shell.execute_reply": "2023-11-13T16:34:17.886650Z",
     "shell.execute_reply.started": "2023-11-13T16:34:17.792047Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)  \n",
    "class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
    "y_train_numeric = np.array([class_to_index[cls] for cls in y_train])\n",
    "\n",
    "class_counts = np.bincount(y_train_numeric)\n",
    "\n",
    "total_samples = len(y_train_numeric)\n",
    "\n",
    "class_weights = total_samples / (len(classes) * class_counts)\n",
    "\n",
    "class_weights_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n",
    "\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Total samples:\", total_samples)\n",
    "print(\"Class weights:\", class_weights)\n",
    "print(\"Class weights dictionary:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:17.889215Z",
     "iopub.status.busy": "2023-11-13T16:34:17.888834Z",
     "iopub.status.idle": "2023-11-13T16:34:25.655051Z",
     "shell.execute_reply": "2023-11-13T16:34:25.654158Z",
     "shell.execute_reply.started": "2023-11-13T16:34:17.889182Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LeakyReLU, PReLU, ELU\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:25.657031Z",
     "iopub.status.busy": "2023-11-13T16:34:25.656356Z",
     "iopub.status.idle": "2023-11-13T16:34:28.182878Z",
     "shell.execute_reply": "2023-11-13T16:34:28.181916Z",
     "shell.execute_reply.started": "2023-11-13T16:34:25.657002Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd=tensorflow.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True)\n",
    "rms = tensorflow.keras.optimizers.RMSprop()\n",
    "nadam=tensorflow.keras.optimizers.Nadam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "lrelu = lambda x: tensorflow.keras.activations.relu(x, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:28.184538Z",
     "iopub.status.busy": "2023-11-13T16:34:28.184188Z",
     "iopub.status.idle": "2023-11-13T16:34:28.306560Z",
     "shell.execute_reply": "2023-11-13T16:34:28.305647Z",
     "shell.execute_reply.started": "2023-11-13T16:34:28.184506Z"
    }
   },
   "outputs": [],
   "source": [
    "ann = Sequential()\n",
    "ann.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='he_uniform', activation=lrelu))\n",
    "ann.add(Dropout(0.1))\n",
    "ann.add(Dense(16,  kernel_initializer='he_uniform', activation=lrelu))\n",
    "ann.add(Dropout(0.1))\n",
    "ann.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))\n",
    "ann.add(Dropout(0.1))\n",
    "\n",
    "ann.add(Dense(1,  kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "ann.compile(loss=\"binary_crossentropy\", optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:28.308290Z",
     "iopub.status.busy": "2023-11-13T16:34:28.307927Z",
     "iopub.status.idle": "2023-11-13T16:34:28.333091Z",
     "shell.execute_reply": "2023-11-13T16:34:28.332211Z",
     "shell.execute_reply.started": "2023-11-13T16:34:28.308258Z"
    }
   },
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n",
    "        self.test_size = test_size\n",
    "        self.kfold = kfold\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split_data(self, X, y, random_state_list):\n",
    "        if self.kfold:\n",
    "            for random_state in random_state_list:\n",
    "                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n",
    "                for train_index, val_index in kf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                    yield X_train, X_val, y_train, y_val\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.device = device\n",
    "        self.random_state = random_state\n",
    "        self.models = self._define_model()\n",
    "        self.len_models = len(self.models)\n",
    "        \n",
    "    def _define_model(self):\n",
    "        xgb_params = {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': 0.1,\n",
    "            'max_depth': 4,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.1,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'logloss',\n",
    "            'objective': 'binary:logistic',\n",
    "            'tree_method': 'hist',\n",
    "            'verbosity': 0,\n",
    "            'random_state': self.random_state,\n",
    "#             'class_weight':class_weights_dict,\n",
    "        }\n",
    "        if self.device == 'gpu':\n",
    "            xgb_params['tree_method'] = 'gpu_hist'\n",
    "            xgb_params['predictor'] = 'gpu_predictor'\n",
    "            \n",
    "        xgb_params2=xgb_params.copy() \n",
    "        xgb_params2['subsample']= 0.3\n",
    "        xgb_params2['max_depth']=8\n",
    "        xgb_params2['learning_rate']=0.005\n",
    "        xgb_params2['colsample_bytree']=0.9\n",
    "\n",
    "        xgb_params3=xgb_params.copy() \n",
    "        xgb_params3['subsample']= 0.6\n",
    "        xgb_params3['max_depth']=6\n",
    "        xgb_params3['learning_rate']=0.02\n",
    "        xgb_params3['colsample_bytree']=0.7      \n",
    "        \n",
    "        lgb_params = {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.02,\n",
    "            'subsample': 0.20,\n",
    "            'colsample_bytree': 0.56,\n",
    "            'reg_alpha': 0.25,\n",
    "            'reg_lambda': 5e-08,\n",
    "            'objective': 'binary',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'device': self.device,\n",
    "            'random_state': self.random_state,\n",
    "#             'class_weight':class_weights_dict,\n",
    "        }\n",
    "        lgb_params2 = {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.20,\n",
    "            'colsample_bytree': 0.56,\n",
    "            'reg_alpha': 0.25,\n",
    "            'reg_lambda': 5e-08,\n",
    "            'objective': 'binary',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'device': self.device,\n",
    "            'random_state': self.random_state,\n",
    "        }\n",
    "        lgb_params3=lgb_params.copy()  \n",
    "        lgb_params3['subsample']=0.9\n",
    "        lgb_params3['reg_lambda']=0.3461495211744402\n",
    "        lgb_params3['reg_alpha']=0.3095626288582237\n",
    "        lgb_params3['max_depth']=8\n",
    "        lgb_params3['learning_rate']=0.007\n",
    "        lgb_params3['colsample_bytree']=0.5\n",
    "\n",
    "        lgb_params4=lgb_params2.copy()  \n",
    "        lgb_params4['subsample']=0.7\n",
    "        lgb_params4['reg_lambda']=0.1\n",
    "        lgb_params4['reg_alpha']=0.2\n",
    "        lgb_params4['max_depth']=10\n",
    "        lgb_params4['learning_rate']=0.007\n",
    "        lgb_params4['colsample_bytree']=0.5\n",
    "        cb_params = {\n",
    "            'iterations': self.n_estimators,\n",
    "            'depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'l2_leaf_reg': 0.7,\n",
    "            'random_strength': 0.2,\n",
    "            'max_bin': 200,\n",
    "            'od_wait': 65,\n",
    "            'one_hot_max_size': 120,\n",
    "            'grow_policy': 'Depthwise',\n",
    "            'bootstrap_type': 'Bayesian',\n",
    "            'od_type': 'Iter',\n",
    "            'eval_metric': 'AUC',\n",
    "            'loss_function': 'Logloss',\n",
    "            'task_type': self.device.upper(),\n",
    "            'random_state': self.random_state,\n",
    "        }\n",
    "        cb_sym_params = cb_params.copy()\n",
    "        cb_sym_params['grow_policy'] = 'SymmetricTree'\n",
    "        cb_loss_params = cb_params.copy()\n",
    "        cb_loss_params['grow_policy'] = 'Lossguide'\n",
    "        \n",
    "        cb_params2=  cb_params.copy()\n",
    "        cb_params2['learning_rate']=0.01\n",
    "        cb_params2['depth']=8\n",
    "        \n",
    "        cb_params3={\n",
    "            'iterations': self.n_estimators,\n",
    "            'random_strength': 0.1, \n",
    "            'one_hot_max_size': 70, \n",
    "            'max_bin': 100, \n",
    "            'learning_rate': 0.008, \n",
    "            'l2_leaf_reg': 0.3, \n",
    "            'grow_policy': 'Depthwise', \n",
    "            'depth': 10, \n",
    "            'max_bin': 200,\n",
    "            'od_wait': 65,\n",
    "            'bootstrap_type': 'Bayesian',\n",
    "            'od_type': 'Iter',\n",
    "            'eval_metric': 'AUC',\n",
    "            'loss_function': 'Logloss',\n",
    "            'task_type': self.device.upper(),\n",
    "            'random_state': self.random_state,\n",
    "        }\n",
    "        cb_params4=  cb_params.copy()\n",
    "        cb_params4['learning_rate']=0.01\n",
    "        cb_params4['depth']=12\n",
    "        dt_params= {'min_samples_split': 30, 'min_samples_leaf': 10, 'max_depth': 8, 'criterion': 'gini'}\n",
    "        \n",
    "        models = {\n",
    "            'xgb': xgb.XGBClassifier(**xgb_params),\n",
    "#             'xgb2': xgb.XGBClassifier(**xgb_params2),\n",
    "            'xgb3': xgb.XGBClassifier(**xgb_params3),\n",
    "            'lgb': lgb.LGBMClassifier(**lgb_params),\n",
    "            'lgb2': lgb.LGBMClassifier(**lgb_params2),\n",
    "#             'lgb3': lgb.LGBMClassifier(**lgb_params3),\n",
    "#             'lgb4': lgb.LGBMClassifier(**lgb_params4),\n",
    "            'cat': CatBoostClassifier(**cb_params),\n",
    "#             'cat2': CatBoostClassifier(**cb_params2),\n",
    "#             'cat3': CatBoostClassifier(**cb_params2),\n",
    "#             'cat4': CatBoostClassifier(**cb_params2),\n",
    "#             \"cat_sym\": CatBoostClassifier(**cb_sym_params),\n",
    "#             \"cat_loss\": CatBoostClassifier(**cb_loss_params),\n",
    "#             'hist_gbm' : HistGradientBoostingClassifier (max_iter=300, learning_rate=0.001,  max_leaf_nodes=80,\n",
    "#                                                          max_depth=6,random_state=self.random_state),#class_weight=class_weights_dict, \n",
    "#             'gbdt': GradientBoostingClassifier(max_depth=6,  n_estimators=1000,random_state=self.random_state),\n",
    "            'lr': LogisticRegression(),\n",
    "#             'rf': RandomForestClassifier(max_depth= 9,max_features= 'auto',min_samples_split= 10,\n",
    "#                                                           min_samples_leaf= 4,  n_estimators=500,random_state=self.random_state),\n",
    "# #             'svc': SVC(gamma=\"auto\", probability=True),\n",
    "# #             'knn': KNeighborsClassifier(n_neighbors=5),\n",
    "#             'mlp': MLPClassifier(random_state=self.random_state, max_iter=1000),\n",
    "#              'etr':ExtraTreesClassifier(min_samples_split=55, min_samples_leaf= 15, max_depth=10,\n",
    "#                                        n_estimators=200,random_state=self.random_state),\n",
    "            'dt' :DecisionTreeClassifier(**dt_params,random_state=self.random_state),\n",
    "#             'ada': AdaBoostClassifier(random_state=self.random_state),\n",
    "            'ann':ann,\n",
    "                                       \n",
    "        }\n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:28.334613Z",
     "iopub.status.busy": "2023-11-13T16:34:28.334247Z",
     "iopub.status.idle": "2023-11-13T16:34:28.347363Z",
     "shell.execute_reply": "2023-11-13T16:34:28.346563Z",
     "shell.execute_reply.started": "2023-11-13T16:34:28.334582Z"
    }
   },
   "outputs": [],
   "source": [
    "class OptunaWeights:\n",
    "    def __init__(self, random_state, n_trials=3000):\n",
    "        self.study = None\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "        # Define the weights for the predictions from each model\n",
    "        weights = [trial.suggest_float(f\"weight{n}\", -1, 2) for n in range(len(y_preds))]\n",
    "\n",
    "        # Calculate the weighted prediction\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
    "\n",
    "        auc_score = roc_auc_score(y_true, weighted_pred)\n",
    "        log_loss_score=log_loss(y_true, weighted_pred)\n",
    "        return auc_score#/log_loss_score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
    "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T16:34:28.353188Z",
     "iopub.status.busy": "2023-11-13T16:34:28.352905Z",
     "iopub.status.idle": "2023-11-13T18:15:21.507395Z",
     "shell.execute_reply": "2023-11-13T18:15:21.506465Z",
     "shell.execute_reply.started": "2023-11-13T16:34:28.353164Z"
    }
   },
   "outputs": [],
   "source": [
    "kfold = True\n",
    "n_splits = 1 if not kfold else 5\n",
    "random_state = 2023\n",
    "random_state_list = [42] # used by split_data [71]\n",
    "n_estimators = 9999 # 9999\n",
    "early_stopping_rounds = 300\n",
    "verbose = False\n",
    "\n",
    "splitter = Splitter(kfold=kfold, n_splits=n_splits)\n",
    "\n",
    "# Initialize an array for storing test predictions\n",
    "test_predss = np.zeros(X_test.shape[0])\n",
    "ensemble_score = []\n",
    "weights = []\n",
    "trained_models = {'xgb':[], 'lgb':[]}\n",
    "\n",
    "    \n",
    "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "            \n",
    "    # Get a set of Regressor models\n",
    "    classifier = Classifier(n_estimators, device, random_state)\n",
    "    models = classifier.models\n",
    "    \n",
    "    # Initialize lists to store oof and test predictions for each base model\n",
    "    oof_preds = []\n",
    "    test_preds = []\n",
    "    \n",
    "    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
    "    for name, model in models.items():\n",
    "        if ('cat' in name) or (\"lgb\" in name) or (\"xgb\" in name):\n",
    "            if 'lgb' == name: #categorical_feature=cat_features\n",
    "                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#,categorical_feature=cat_features,\n",
    "                          early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "            elif 'cat' ==name:\n",
    "                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#cat_features=cat_features,\n",
    "                          early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "            else:\n",
    "                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "        elif name in 'ann':\n",
    "            model.fit(X_train_, y_train_, validation_data=(X_val, y_val),batch_size=4, epochs=5,verbose=verbose)\n",
    "        else:\n",
    "            model.fit(X_train_, y_train_)\n",
    "        \n",
    "        if name in 'ann':\n",
    "            test_pred = np.array(model.predict(X_test))[:, 0]\n",
    "            y_val_pred = np.array(model.predict(X_val))[:, 0]\n",
    "        else:\n",
    "            test_pred = model.predict_proba(X_test)[:, 1]\n",
    "            y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        score = roc_auc_score(y_val, y_val_pred)\n",
    "#         score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n",
    "\n",
    "        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] ROC AUC score: {score:.5f}')\n",
    "        \n",
    "        oof_preds.append(y_val_pred)\n",
    "        test_preds.append(test_pred)\n",
    "        \n",
    "        if name in trained_models.keys():\n",
    "            trained_models[f'{name}'].append(deepcopy(model))\n",
    "    # Use Optuna to find the best ensemble weights\n",
    "    optweights = OptunaWeights(random_state=random_state)\n",
    "    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
    "    \n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "#     score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n",
    "    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] ------------------>  ROC AUC score {score:.5f}')\n",
    "    ensemble_score.append(score)\n",
    "    weights.append(optweights.weights)\n",
    "    \n",
    "    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T18:15:21.509605Z",
     "iopub.status.busy": "2023-11-13T18:15:21.508883Z",
     "iopub.status.idle": "2023-11-13T18:15:21.517104Z",
     "shell.execute_reply": "2023-11-13T18:15:21.516012Z",
     "shell.execute_reply.started": "2023-11-13T18:15:21.509570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the mean ROC AUC  score of the ensemble\n",
    "mean_score = np.mean(ensemble_score)\n",
    "std_score = np.std(ensemble_score)\n",
    "print(f'Ensemble ROC AUC score {mean_score:.5f}  {std_score:.5f}')\n",
    "\n",
    "# Print the mean and standard deviation of the ensemble weights for each model\n",
    "print('--- Model Weights ---')\n",
    "mean_weights = np.mean(weights, axis=0)\n",
    "std_weights = np.std(weights, axis=0)\n",
    "for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
    "    print(f'{name}: {mean_weight:.5f}  {std_weight:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T18:15:21.518654Z",
     "iopub.status.busy": "2023-11-13T18:15:21.518346Z",
     "iopub.status.idle": "2023-11-13T18:15:22.375746Z",
     "shell.execute_reply": "2023-11-13T18:15:22.374791Z",
     "shell.execute_reply.started": "2023-11-13T18:15:21.518628Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_importance(models, feature_cols, title, head=15):\n",
    "    importances = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df[\"importance\"] = model.feature_importances_\n",
    "        _df[\"feature\"] = pd.Series(feature_cols)\n",
    "        _df[\"fold\"] = i\n",
    "        _df = _df.sort_values('importance', ascending=False)\n",
    "        _df = _df.head(head)\n",
    "        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n",
    "        \n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    # display(feature_importance.groupby([\"feature\"]).mean().reset_index().drop('fold', axis=1))\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance, color= (0.4, 0.76, 0.65), errorbar='sd')\n",
    "    plt.xlabel('Importance', fontsize=14)\n",
    "    plt.ylabel('Feature', fontsize=14)\n",
    "    plt.title(f'{title} Feature Importance', fontsize=18)\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.show()\n",
    "    \n",
    "for name, models in trained_models.items():\n",
    "    visualize_importance(models, list(X_train.columns), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T18:15:22.377446Z",
     "iopub.status.busy": "2023-11-13T18:15:22.377107Z",
     "iopub.status.idle": "2023-11-13T18:15:22.805333Z",
     "shell.execute_reply": "2023-11-13T18:15:22.804384Z",
     "shell.execute_reply.started": "2023-11-13T18:15:22.377415Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/playground-series-s3e24/sample_submission.csv')\n",
    "sub['smoking'] =  test_predss\n",
    "sub.to_csv('submission_pure.csv',index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T18:43:21.588998Z",
     "iopub.status.busy": "2023-11-13T18:43:21.588097Z",
     "iopub.status.idle": "2023-11-13T18:43:22.175099Z",
     "shell.execute_reply": "2023-11-13T18:43:22.174302Z",
     "shell.execute_reply.started": "2023-11-13T18:43:21.588962Z"
    }
   },
   "outputs": [],
   "source": [
    "sub1=pd.read_csv('/kaggle/input/lb-0-88048-simple-smoking-ensemble/combo_50_35_15.csv')\n",
    "sub2=pd.read_csv(\"/kaggle/input/pg-s3e24-brute-force-and-ignorance/submission.csv\")\n",
    "sub3=pd.read_csv(\"/kaggle/input/ps-s3e24-smoker-status-predictions/submission.csv\")\n",
    "sub4=pd.read_csv(\"/kaggle/input/ps-s3-ep24-eda-modeling-submission/Voting_Stacker_full_cv_baseline_submission.csv\")\n",
    "sub5=pd.read_csv(\"/kaggle/input/efficient-prediction-of-smoker-status/xgb_pseudo_opt_submission.csv\")\n",
    "sub6=pd.read_csv(\"/kaggle/input/pg-s3-e24-eda-modeling-ensemle-nn/nn_submission.csv\")\n",
    "\n",
    "\n",
    "def scale(df):\n",
    "    df['smoking']=(df['smoking']-df['smoking'].min())/(df['smoking'].max()-df['smoking'].min())\n",
    "    return df\n",
    "\n",
    "sub_combined=sub1.copy() \n",
    "\n",
    "sub1=scale(sub1)\n",
    "sub2=scale(sub2)\n",
    "sub3=scale(sub3)\n",
    "sub4=scale(sub4)\n",
    "sub5=scale(sub5)\n",
    "sub6=scale(sub6)\n",
    "\n",
    "sub=scale(sub)\n",
    "\n",
    "sub_combined['smoking']=(5*sub1['smoking'] + 4*sub2['smoking'] + 5*sub[\"smoking\"]+1/4*sub3[\"smoking\"]\n",
    "                         +1/16*sub4[\"smoking\"]+1/64*sub5[\"smoking\"]+1/256*sub6[\"smoking\"]) \n",
    "\n",
    "sub_combined=scale(sub_combined)\n",
    "sub_combined.to_csv('submission5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6622892,
     "sourceId": 60891,
     "sourceType": "competition"
    },
    {
     "datasetId": 2367101,
     "sourceId": 3989074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 149116262,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 149393795,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 149552339,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 150293926,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 150329906,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 150339002,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 150418632,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
