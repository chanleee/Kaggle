{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:23.772241Z",
     "start_time": "2023-11-24T12:15:23.395426Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014473,
     "end_time": "2023-11-09T03:15:46.533694",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.519221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --target=./chan-venv/lib/python3.10/site-packages --upgrade lightgbm==3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:23.772405Z",
     "start_time": "2023-11-24T12:15:23.395747Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --target=./chan-venv/lib/python3.10/site-packages --upgrade fastparquet pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --target=./chan-venv/lib/python3.10/site-packages cmaes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013174,
     "end_time": "2023-11-09T03:15:46.560482",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.547308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 사용 설명서\n",
    "\n",
    "## 0. requirements\n",
    "#### 0.1. 로컬 환경에서 package 설치법\n",
    "- ```!pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages <package_name>```\n",
    "\n",
    "#### 0.2. kaggle api 설치법\n",
    "- https://www.kaggle.com/docs/api#getting-started-installation-&-authentication 참고\n",
    "- ```!pip install --target=/home/<user_name>/<venv_name>/lib/python3.10/site-packages kaggle```\n",
    "- kaggle api token 다운로드 후 upload (kaggle.json)\n",
    "- ```!mkdir ~/.kaggle``` \n",
    "- ```!mv ~/kaggle.json ~/.kaggle/``` (kaggle.json 파일을 ~/.kaggle/ 로 이동)\n",
    "- ```!chmod 600 ~/.kaggle/kaggle.json``` (권한 설정)\n",
    "\n",
    "## 1. config 설정\n",
    "\n",
    "#### 1.1. init config\n",
    "- MODE: train, inference 중 선택 (train : 로컬 환경, inference : 캐글 환경)\n",
    "- KAGGLE_DATASET_NAME: 캐글 환경에서 inference 시 사용할 데이터셋 이름 \n",
    "  - 이 이름으로 캐글 데이터셋이 생성됩니다. (중복 불가)\n",
    "\n",
    "#### 1.2. train / inference config\n",
    "- model_directory: 모델 저장 경로\n",
    "- data_directory: 데이터 경로\n",
    "- train_mode: train 모드 여부\n",
    "- infer_mode: inference 모드 여부\n",
    "\n",
    "#### 1.3. model config\n",
    "- model_name: 사용할 모델 이름\n",
    "    - 실제 아래 models_config에 있는 모델 이름과 동일해야 합니다 (아래중에서 선택하는것임).(:list)\n",
    "- target: target column 이름\n",
    "- split_method: 데이터 분리 방식\n",
    "  - time_series: 시계열 데이터 분리\n",
    "  - rolling: 롤링 윈도우 방식 데이터 분리\n",
    "  - blocking: 블록 방식 데이터 분리\n",
    "  - holdout: holdout 방식 데이터 분리\n",
    "- n_splits: 데이터 분리 개수 (1 ~)\n",
    "- correct: 데이터 분리 시 날짜 boundary를 맞출지 여부 (True / False)\n",
    "- initial_fold_size_ratio: 초기 fold size 비율 (0 ~ 1)\n",
    "- train_test_ratio: train, test 비율 (0 ~ 1)\n",
    "- ~train_start: 학습 데이터 기간 시작~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~train_end: 학습 데이터 기간 끝~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~valid_start: 검증 데이터 기간 시작~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- ~valid_end: 검증 데이터 기간 끝~ (holdout 방식에서만 사용 -> split_method가 holdout이면 직접 설정)\n",
    "- optuna_random_state: optuna random state\n",
    "                                - \n",
    "#### 1.4. model heyperparameter config\n",
    "- models_config: 모델 하이퍼파라미터 설정\n",
    "    - model: 모델 클래스\n",
    "    - params: 모델 하이퍼파라미터들\n",
    "        - ... : 모델 하이퍼파라미터\n",
    "\n",
    "## 2. Global Method\n",
    "- reduce_mem_usage: 메모리 사용량 줄이는 함수\n",
    "- compute_triplet_imbalance: triplet imbalance 계산 함수\n",
    "- calculate_triplet_imbalance_numba: triplet imbalance 계산 함수\n",
    "- print_log: 함수 실행 전후에 원하는 코드를 실행해주는 decorator 함수입니다.\n",
    "- zero_sum: zero sum 함수\n",
    "\n",
    "## 3. Pre Code\n",
    "- DataPreprocessor: 데이터 전처리 클래스\n",
    "- FeatureEngineer: 피쳐 엔지니어링 클래스\n",
    "- Splitter: 데이터 분리 클래스\n",
    "- Model: 모델 클래스\n",
    "- Trainer: 학습 클래스\n",
    "\n",
    "## 4. Main Code\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013176,
     "end_time": "2023-11-09T03:15:46.586959",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.573783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013293,
     "end_time": "2023-11-09T03:15:46.613651",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.600358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0. requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:23.772532Z",
     "start_time": "2023-11-24T12:15:23.575382Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021933,
     "end_time": "2023-11-09T03:15:46.649226",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.627293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --target=/home/chan/chan-venv/lib/python3.10/site-packages <package_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014012,
     "end_time": "2023-11-09T03:15:46.677485",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.663473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 0.2. kaggle api 설치법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:23.772693Z",
     "start_time": "2023-11-24T12:15:23.575530Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021246,
     "end_time": "2023-11-09T03:15:46.712389",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.691143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --target=/home/chan/chan-venv/lib/python3.10/site-packages kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:23.772763Z",
     "start_time": "2023-11-24T12:15:23.575762Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020857,
     "end_time": "2023-11-09T03:15:46.747089",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.726232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:23.772830Z",
     "start_time": "2023-11-24T12:15:23.576730Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02085,
     "end_time": "2023-11-09T03:15:46.782033",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.761183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!mv ~/kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:23.772891Z",
     "start_time": "2023-11-24T12:15:23.576779Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021282,
     "end_time": "2023-11-09T03:15:46.817302",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.796020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013582,
     "end_time": "2023-11-09T03:15:46.844948",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.831366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. config 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013741,
     "end_time": "2023-11-09T03:15:46.924523",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.910782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.1. init config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.114313Z",
     "start_time": "2023-11-24T12:15:23.576840Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026122,
     "end_time": "2023-11-09T03:15:46.964537",
     "exception": false,
     "start_time": "2023-11-09T03:15:46.938415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE = \"train\"  # train, inference both\n",
    "KAGGLE_DATASET_NAME = \"model-version-chan-100\"\n",
    "CHAN_GROUP_DATASET_NAME = 'df-group-version-100'\n",
    "CHAN_REFER_DATASET_NAME = 'df-refer-version-100'\n",
    "DF_GROUP_VERSION = \"100\"\n",
    "DF_REFER_VERSION = \"100\"\n",
    "chan_DIR1 = f'./baseline/df_group/{DF_GROUP_VERSION}'\n",
    "chan_DIR2 = f'./baseline/df_refer/{DF_REFER_VERSION}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.114601Z",
     "start_time": "2023-11-24T12:15:24.113734Z"
    },
    "papermill": {
     "duration": 5.644978,
     "end_time": "2023-11-09T03:16:25.965358",
     "exception": false,
     "start_time": "2023-11-09T03:16:20.320380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/chan-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "import functools\n",
    "import time\n",
    "from numba import njit, prange\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer,minmax_scale\n",
    "from sklearn.decomposition import PCA,TruncatedSVD,LatentDirichletAllocation\n",
    "from sklearn.neighbors import KNeighborsClassifier,NearestNeighbors\n",
    "from sklearn.impute import KNNImputer\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', font_scale=1.4)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"train\":\n",
    "    if not os.path.exists(chan_DIR1):\n",
    "        os.makedirs(chan_DIR1)\n",
    "    if not os.path.exists(chan_DIR2):\n",
    "        os.makedirs(chan_DIR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014983,
     "end_time": "2023-11-09T03:16:25.995747",
     "exception": false,
     "start_time": "2023-11-09T03:16:25.980764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.2. train / inference config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.114904Z",
     "start_time": "2023-11-24T12:15:24.114225Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.3.2', '2.0.1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.__version__, xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.117487Z",
     "start_time": "2023-11-24T12:15:24.114483Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "EPS = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.253184Z",
     "start_time": "2023-11-24T12:15:24.114571Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025998,
     "end_time": "2023-11-09T03:16:26.037324",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.011326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in train mode\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    print(\"You are in train mode\")\n",
    "    model_directory = \"./models/\" + time.strftime(\"%Y%m%d_%H:%M:%S\", time.localtime(time.time() + 9 * 60 * 60))\n",
    "    data_directory = \"./data\"\n",
    "    train_mode = True\n",
    "    infer_mode = False\n",
    "elif MODE == \"inference\":\n",
    "    print(\"You are in inference mode\")\n",
    "    model_directory = f'/kaggle/input/{KAGGLE_DATASET_NAME}'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = False\n",
    "    infer_mode = True\n",
    "elif MODE == \"both\":\n",
    "    print(\"You are in both mode\")\n",
    "    model_directory = f'/kaggle/working/'\n",
    "    data_directory = \"/kaggle/input/optiver-trading-at-the-close\"\n",
    "    train_mode = True\n",
    "    infer_mode = True\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01504,
     "end_time": "2023-11-09T03:16:26.067759",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.052719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.3. model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.256507Z",
     "start_time": "2023-11-24T12:15:24.252809Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# stacing mode가 True면 일단 어떤 경우에도 무조건 스테킹\n",
    "# 예를들어 single model이면 split 기준으로 만들어진 모델들을 같은 validation으로 stacking\n",
    "# stacking mode가 False면 single model이면 모델 \n",
    "\n",
    "# 싱글모델일때 여러가지를 할수있다, 싱글모델 싱글 폴드일때 n_estimators를 1.2배로 늘려서 사용할 수 있다. \n",
    "# 싱글모델 다중폴드일때, 스테킹으로 할 수 도 있고 나중에 그 모델들로 각각예측한거 n빵할수도있다.\n",
    "# 다중모델일때 싱글폴드면 한 폴드에서 바로 스테킹할 수 있다. 또는 여기서 n빵으로도 가능 + 각각을 \n",
    "# 다중모델일때 다중폴드면 각 폴드마다 스테킹을 하고 나중에 그 모델들로 각각예측한거 n빵\n",
    "\n",
    "# n 빵하는건 그냥 각 모델 저장만 하면됨 / 스테킹은 각 모델 저장 + 스테킹 모델 저장\n",
    "\n",
    "# 기본적으로 N빵하는데 stacking mode 가 True인경우에 stacking 모델 돌리고 저장까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.258954Z",
     "start_time": "2023-11-24T12:15:24.253359Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    ### default config\n",
    "    \"data_dir\": data_directory,\n",
    "    \"model_dir\": model_directory,\n",
    "    \"train_mode\": train_mode,  # True : train, False : not train\n",
    "    \"infer_mode\": infer_mode,  # True : inference, False : not inference\n",
    "\n",
    "    ### model config\n",
    "    \"model_name\": [\"lgb\"],  # model name\n",
    "    \"stacking_mode\": False,  # stacking mode or not (single model도 split되면 그걸로 stacking)\n",
    "    \"stacking_algorithm\": None,  # \"optuna\",  # or None\n",
    "\n",
    "    \"target\": \"target\",\n",
    "\n",
    "    ### model hyperparameter\n",
    "    \"optuna_random_state\": 42,\n",
    "\n",
    "    ### cv hyperparameter\n",
    "    \"split_method\": \"rolling\",  # time_series, rolling, blocking, holdout\n",
    "    \"n_splits\": 3,  # number of splits\n",
    "    \"correct\": True,  # correct boundary\n",
    "    \"gap\": 0.05,  # gap between train and test (0.05 = 5% of train size)\n",
    "    \"initial_fold_size_ratio\": 0.8,  # initial fold size ratio\n",
    "    \"train_test_ratio\": 0.9,  # train, test ratio\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.433181Z",
     "start_time": "2023-11-24T12:15:24.293925Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if config[\"stacking_mode\"] == True and config[\"n_splits\"] == 1:\n",
    "    raise ValueError(\"stacking mode is True but n_splits is 1, cannot stacking\")\n",
    "if config[\"stacking_mode\"] == False and config[\"stacking_algorithm\"] is not None:\n",
    "    raise ValueError(\"stacking mode is False but stacking_algorithm is not None, impossible\")\n",
    "if config[\"stacking_mode\"] == True and config[\"stacking_algorithm\"] is None:\n",
    "    raise ValueError(\"stacking mode is True but stacking_algorithm is None, impossible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.436579Z",
     "start_time": "2023-11-24T12:15:24.433040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026838,
     "end_time": "2023-11-09T03:16:26.109734",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.082896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"data_dir\": data_directory,\n",
    "#     \"model_dir\": model_directory,\n",
    "# \n",
    "#     \"train_mode\": train_mode,  # True : train, False : not train\n",
    "#     \"infer_mode\": infer_mode,  # True : inference, False : not inference\n",
    "#     \"model_name\": [\"lgb\"],  # model name\n",
    "#     \"final_mode\": True,  # True : using final model, False : not using final model\n",
    "#     \"best_iterate_ratio\": 1.2,  # best iteration ratio\n",
    "#     'target': 'target',\n",
    "# \n",
    "#     'split_method': 'holdout',  # time_series, rolling, blocking, holdout\n",
    "#     'n_splits': 1,  # number of splits\n",
    "#     'correct': True,  # correct boundary\n",
    "#     'gap': 0,  # gap between train and test (0.05 = 5% of train size)\n",
    "# \n",
    "#     'initial_fold_size_ratio': 0.8,  # initial fold size ratio\n",
    "#     'train_test_ratio': 0.9,  # train, test ratio\n",
    "# \n",
    "#     'optuna_random_state': 42,\n",
    "# }\n",
    "# config[\"model_mode\"] = \"single\" if len(config[\"model_name\"]) == 1 else \"stacking\"  # 모델 수에 따라서 single / stacking 판단\n",
    "# config[\"mae_mode\"] = True if config[\"model_mode\"] == \"single\" and not config[\n",
    "#     \"final_mode\"] else False  # single 모델이면서 final_mode가 아닌경우 폴드가 여러개일때 모델 평가기준이 없어서 mae로 평가\n",
    "# config[\"inference_n_splits\"] = len(config['model_name']) if config[\"final_mode\"] or config[\"mae_mode\"] else config[\n",
    "#     \"n_splits\"]  # final_mode가 아닌경우 n_splits만큼 inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015075,
     "end_time": "2023-11-09T03:16:26.139996",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.124921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1.4. model heyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:24.436730Z",
     "start_time": "2023-11-24T12:15:24.433779Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027209,
     "end_time": "2023-11-09T03:16:26.182510",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.155301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    \"lgb\": {\n",
    "        \"model\": lgb.LGBMRegressor,\n",
    "        \"params\": {\n",
    "            \"objective\": \"mae\",\n",
    "            \"n_estimators\": 9999,  # 2040\n",
    "            \"num_leaves\": 126,\n",
    "            \"subsample\": 0.7628752081565437,\n",
    "            \"colsample_bytree\": 0.6380919043232433,\n",
    "            \"learning_rate\": 0.01795041572109495,\n",
    "            \"n_jobs\": 4,\n",
    "            \"device\": \"gpu\",\n",
    "            \"verbosity\": -1,\n",
    "            \"importance_type\": \"gain\",\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"xgb\": {\n",
    "        \"model\": xgb.XGBRegressor,\n",
    "        \"params\": {\n",
    "            \"objective\": \"reg:linear\",\n",
    "            \"n_estimators\": 2400,\n",
    "            \"max_depth\": 14,\n",
    "            \"eta\": 0.0073356282482453065,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.30000000000000004,\n",
    "            \"colsample_bylevel\": 0.9,\n",
    "            \"min_child_weight\": 0.4824060812428942,\n",
    "            \"reg_lambda\": 182.50819193990537,\n",
    "            \"reg_alpha\": 0.03171419713574529,\n",
    "            \"gamma\": 0.9162634503670075,\n",
    "            \"tree_method\": \"gpu_hist\",\n",
    "            \"n_jobs\": 4,\n",
    "            \"verbosity\": 0,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:35.388266Z",
     "start_time": "2023-11-24T12:15:24.433929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading optiver-trading-at-the-close.zip to ./data\n",
      " 99%|███████████████████████████████████████▍| 198M/201M [00:09<00:00, 26.8MB/s]\n",
      "100%|████████████████████████████████████████| 201M/201M [00:09<00:00, 22.7MB/s]\n",
      "Archive:  ./data/optiver-trading-at-the-close.zip\n",
      "  inflating: ./data/example_test_files/revealed_targets.csv  \n",
      "  inflating: ./data/example_test_files/sample_submission.csv  \n",
      "  inflating: ./data/example_test_files/test.csv  \n",
      "  inflating: ./data/optiver2023/__init__.py  \n",
      "  inflating: ./data/optiver2023/competition.cpython-310-x86_64-linux-gnu.so  \n",
      "  inflating: ./data/public_timeseries_testing_util.py  \n",
      "  inflating: ./data/train.csv        \n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    if not os.path.exists(config[\"model_dir\"]):\n",
    "        os.makedirs(config[\"model_dir\"])\n",
    "    if not os.path.exists(config[\"data_dir\"]):\n",
    "        os.makedirs(config[\"data_dir\"])\n",
    "    !kaggle competitions download optiver-trading-at-the-close -p {config[\"data_dir\"]} --force\n",
    "    !unzip -o {config[\"data_dir\"]}/optiver-trading-at-the-close.zip -d {config[\"data_dir\"]}\n",
    "    !rm {config[\"data_dir\"]}/optiver-trading-at-the-close.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015155,
     "end_time": "2023-11-09T03:16:26.253174",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.238019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Global Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:35.413551Z",
     "start_time": "2023-11-24T12:15:35.390527Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030413,
     "end_time": "2023-11-09T03:16:26.299001",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.268588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:35.553531Z",
     "start_time": "2023-11-24T12:15:35.400067Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.117482,
     "end_time": "2023-11-09T03:16:26.432299",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.314817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param df_values: \n",
    "    :param comb_indices: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val + EPS)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    \"\"\"\n",
    "    Calculate the triplet imbalance for each row in the DataFrame.\n",
    "    :param price: \n",
    "    :param df: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:35.553723Z",
     "start_time": "2023-11-24T12:15:35.544505Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.035964,
     "end_time": "2023-11-09T03:16:26.484162",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.448198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(message_format):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # self 확인: 첫 번째 인자가 클래스 인스턴스인지 확인합니다.\n",
    "            if args and hasattr(args[0], 'infer'):\n",
    "                self = args[0]\n",
    "\n",
    "                # self.infer가 False이면 아무 것도 출력하지 않고 함수를 바로 반환합니다.\n",
    "                if self.infer:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            if result is not None:\n",
    "                data_shape = getattr(result, 'shape', 'No shape attribute')\n",
    "                shape_message = f\", shape({data_shape})\"\n",
    "            else:\n",
    "                shape_message = \"\"\n",
    "\n",
    "            print(f\"\\n{'-' * 100}\")\n",
    "            print(message_format.format(func_name=func.__name__, elapsed_time=elapsed_time) + shape_message)\n",
    "            print(f\"{'-' * 100}\\n\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:35.553811Z",
     "start_time": "2023-11-24T12:15:35.544798Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0316,
     "end_time": "2023-11-09T03:16:26.537507",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.505907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:35.694457Z",
     "start_time": "2023-11-24T12:15:35.545166Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def custom_pct_change(series, window=1, epsilon=1e-10):\n",
    "    return (series.diff(window) / (series.shift(window) + epsilon)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}sec')\n",
    "    \n",
    "def print_trace(name: str = ''):\n",
    "    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017946,
     "end_time": "2023-11-09T03:16:26.571351",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.553405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### \n",
    "각 클래스의 method는 각자 필요에 따라 추가 해서 사용하면 됩니다. 이때 class의 주석에 method를 추가하고, method의 주석에는 method의 역할을 간단하게 적어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019169,
     "end_time": "2023-11-09T03:16:26.609856",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.590687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Pre Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019027,
     "end_time": "2023-11-09T03:16:26.648726",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.629699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'train':\n",
    "    average_columns = pd.read_csv('average_columns.csv')\n",
    "if MODE == 'inference':\n",
    "    average_columns = pd.read_csv(f'/kaggle/input/{KAGGLE_DATASET_NAME}/average_columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_imb_size = average_columns['imbalance_size']\n",
    "avg_imb_flag = average_columns['imbalance_buy_sell_flag']\n",
    "avg_ref_price = average_columns['reference_price']\n",
    "avg_match_size = average_columns['matched_size']\n",
    "avg_far_price = average_columns['far_price']\n",
    "avg_near_price = average_columns['near_price']\n",
    "avg_bid_price = average_columns['bid_price']\n",
    "avg_bid_size = average_columns['bid_size']\n",
    "avg_ask_price = average_columns['ask_price']\n",
    "avg_ask_size = average_columns['ask_size']\n",
    "avg_wap = average_columns['wap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:35.694808Z",
     "start_time": "2023-11-24T12:15:35.694288Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.032447,
     "end_time": "2023-11-09T03:16:26.699576",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.667129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    데이터 전처리 클래스\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        전처리할 데이터\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    handle_missing_data()\n",
    "        결측치 처리\n",
    "    handle_outliers()\n",
    "        이상치 처리\n",
    "    normalize()\n",
    "        정규화\n",
    "    custom_preprocessing()\n",
    "        사용자 정의 전처리\n",
    "    transform()\n",
    "        전처리 수행\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, infer=False):\n",
    "        self.data = data  # reduce_mem_usage(data) # reduce_mem_usage 정밀도 훼손함 \n",
    "        self.infer = infer\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def handle_missing_data(self):\n",
    "        # 결측치 처리 코드\n",
    "        self.data = self.data.dropna(subset=[\"target\"]) if self.infer == False else self.data\n",
    "\n",
    "        #어떤 column이 test api에서는 비어있을지 모른다. -> 모든 column에 대해서 평균값을 저장해둔 파일이 있어야한다. average_columns.csv\n",
    "        #train에서는 'imbalance_size', 'reference_price', 'matched_size', 'bid_price', 'ask_price', 'wap' 얘네가 없는 애들은 drop하자.\n",
    "        #test에서는 이런 애들이 나와도 drop할 수 없으니 평균값으로 impute\n",
    "        #condition1, 2에 따라서 far price, near_price도 다르게 impute을 해줘야할 것으로 예상됨. -> condition1은 0으로, condition2는 mean으로\n",
    "        \n",
    "        columns_drop_train = ['imbalance_size', 'reference_price', 'matched_size', 'bid_price', 'ask_price', 'wap']\n",
    "        self.data = self.data.dropna(subset=columns_drop_train) if self.infer == False else self.data\n",
    "        \n",
    "        self.data['imbalance_size'] = self.data['imbalance_size'].fillna(self.data['stock_id'].map(avg_imb_size))\n",
    "        self.data['imbalance_buy_sell_flag'] = self.data['imbalance_buy_sell_flag'].fillna(self.data['stock_id'].map(avg_imb_flag))\n",
    "        self.data['reference_price'] = self.data['reference_price'].fillna(self.data['stock_id'].map(avg_ref_price))\n",
    "        self.data['matched_size'] = self.data['matched_size'].fillna(self.data['stock_id'].map(avg_match_size))\n",
    "        self.data['bid_price'] = self.data['bid_price'].fillna(self.data['stock_id'].map(avg_bid_price))\n",
    "        self.data['bid_size'] = self.data['bid_size'].fillna(self.data['stock_id'].map(avg_bid_size))\n",
    "        self.data['ask_price'] = self.data['ask_price'].fillna(self.data['stock_id'].map(avg_ask_price))\n",
    "        self.data['ask_size'] = self.data['ask_size'].fillna(self.data['stock_id'].map(avg_ask_size))\n",
    "        self.data['wap'] = self.data['wap'].fillna(self.data['stock_id'].map(avg_wap))\n",
    "        \n",
    "        condition = self.data['seconds_in_bucket'] < 300\n",
    "        self.data.loc[condition, 'near_price'] = self.data.loc[condition, 'near_price'].fillna(0)\n",
    "        self.data.loc[condition, 'far_price'] = self.data.loc[condition, 'far_price'].fillna(0)\n",
    "        \n",
    "        condition = 300 <= self.data['seconds_in_bucket']\n",
    "        self.data.loc[condition, 'near_price'] = self.data.loc[condition, 'near_price'].fillna(self.data.loc[condition, 'stock_id'].map(avg_near_price))\n",
    "        self.data.loc[condition, 'far_price'] = self.data.loc[condition, 'far_price'].fillna(self.data.loc[condition, 'stock_id'].map(avg_far_price))\n",
    "        \n",
    "        self.data = self.data.reset_index(drop=True) if self.infer == False else self.data\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def handle_outliers(self):\n",
    "        # 이상치 처리 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def normalize(self):\n",
    "        # 정규화 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def custom_preprocessing(self):\n",
    "        # 사용자 정의 전처리 코드\n",
    "        return self.data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def transform(self):\n",
    "        # 전처리 수행 코드 (위의 메소드 활용 가능)\n",
    "        self.handle_missing_data()\n",
    "        #print(self.data.isna().sum())\n",
    "        # self.handle_outliers()\n",
    "        # self.normalize()\n",
    "        # self.custom_preprocessing()\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018425,
     "end_time": "2023-11-09T03:16:26.733619",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.715194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:35.701986Z",
     "start_time": "2023-11-24T12:15:35.695250Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "global_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'train':\n",
    "    average_values = pd.read_csv('average_values.csv')\n",
    "if MODE == 'inference':\n",
    "    average_values = pd.read_csv(f'/kaggle/input/{KAGGLE_DATASET_NAME}/average_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'inference':\n",
    "    # 이건 global 변수로 설정되어있어야 loop 돌때마다 실행되는 불상사를 막을 수 있음.\n",
    "    \n",
    "    df_groups = {}\n",
    "    df_refers = {}\n",
    "    \n",
    "    for i in range(200):\n",
    "        \n",
    "        key_name = f\"stock{i}\"\n",
    "        df_groups[key_name] = pd.read_csv(f'/kaggle/input/{CHAN_GROUP_DATASET_NAME}/{i}.csv')\n",
    "        pre_refer = pd.read_csv(f'/kaggle/input/{CHAN_REFER_DATASET_NAME}/{i}.csv')\n",
    "\n",
    "        \n",
    "        for MetricKind in ['time_vol_l1', 'stock_vol_l1']: #이거를 classifier를 뭐를 쓰냐에 따라 선택할 수 있도록 해야함.\n",
    "            key_name_= f\"stock{i}_{MetricKind}\"\n",
    "            metric_columns = [col for col in pre_refer.columns if MetricKind in col]\n",
    "            metric_columns.append('time_id')\n",
    "            metric_columns.append('stock_id')\n",
    "            df_refers[key_name_] = pre_refer[metric_columns]\n",
    "            \n",
    "        \"\"\"\n",
    "        위치에는 수정이 필요할 수 있음.\n",
    "        \"\"\"\n",
    "    \n",
    "    df_models = {}\n",
    "    \n",
    "    for i in range(200):\n",
    "        for MetricKind in ['time_vol_l1', 'stock_vol_l1']: #이거를 classifier를 뭐를 쓰냐에 따라 선택할 수 있도록 해야함.\n",
    "            key_name_ = f\"stock{i}_{MetricKind}\"\n",
    "            df_models[key_name_] = joblib.load(f'/kaggle/input/{KAGGLE_DATASET_NAME}/{i}_{MetricKind}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX_for_Neighbors= 80\n",
    "USE_PRICE_NN_FEATURES = False\n",
    "USE_VOL_NN_FEATURES = True\n",
    "USE_SIZE_NN_FEATURES = False\n",
    "\n",
    "USE_TIME_ID_NN = True\n",
    "USE_STOCK_ID_NN = True\n",
    "\n",
    "USE_INFERENCE_AGG = False\n",
    "\n",
    "if USE_INFERENCE_AGG == False:\n",
    "    N_NEIGHBORS_MAX=1\n",
    "if USE_INFERENCE_AGG == True:\n",
    "    N_NEIGHBORS_MAX=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: Optional[Dict] = None, \n",
    "                 exclude_self: bool = False,\n",
    "                 algorithm: str = 'auto'):\n",
    "        \n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "        nn = NearestNeighbors(\n",
    "            n_neighbors=N_NEIGHBORS_MAX_for_Neighbors, \n",
    "            p=p, \n",
    "            metric=metric, \n",
    "            metric_params=metric_params,\n",
    "            algorithm = algorithm\n",
    "        )\n",
    "\n",
    "        nn.fit(pivot)\n",
    "        _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "        #여기서 nn을 저장하는 식으로 하면 나중에 수정을 해서 다시 코드를 run해야하는 경우에 시간적인 이득을 좀 벌 수 있을 것 같기도 한데..\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "        \n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n,:,:], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "        # 여기서 pivot은 feature_values를 이용하는데\n",
    "        # train에서는 당연히 문제가 없을꺼고 test에서도 문제가 없는지 확인해야할 것 같다.\n",
    "        # 왜냐면 저 n만큼의 데이터가 없을수도 있을것 같다는 생각이 들기 때문\n",
    "        # n에 해당하는 것이 neighbors의 수다. 상관없을듯!\n",
    "\n",
    "        dst = pivot_aggs.unstack().reset_index()\n",
    "        dst.columns = ['stock_id', 'time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n",
    "        return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        feature_pivot = df.pivot(index = 'time_id', columns='stock_id', values=feature_col)\n",
    "        # feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "        # 이건.. mean값으로 impute하는건데 impute이 필요해서 이게 있는건가 싶기는 한데 일단 나는 preprocess에서 이미 필요한 Impute을 다 했으니깐\n",
    "        # 이건 나중에 필요에 따라 여기서 따로 처리를 해주는걸로\n",
    "        \n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX_for_Neighbors, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX_for_Neighbors):\n",
    "            feature_values[i, :, :] += feature_pivot.values[self.neighbors[:, i], :]\n",
    "        \n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n",
    "        \n",
    "class StockIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        feature_pivot = df.pivot(index = 'time_id', columns='stock_id', values=feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX_for_Neighbors, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX_for_Neighbors):\n",
    "            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborsClassifier:\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 data: pd.DataFrame,\n",
    "                 p: float,\n",
    "                 weights: str = 'uniform',\n",
    "                 metric: str = 'minkowski',\n",
    "                 metric_params: Optional[Dict] = None,\n",
    "                 algorithm: str = 'auto'\n",
    "                ):\n",
    "        \n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "        if MODE == 'train':\n",
    "            self.x_data = data.drop('time_id', axis=1)\n",
    "            self.y_data = data['time_id']\n",
    "            \n",
    "            self.model = KNeighborsClassifier(\n",
    "                            n_neighbors=N_NEIGHBORS_MAX,\n",
    "                            p=p, \n",
    "                            metric=metric, \n",
    "                            metric_params=metric_params,\n",
    "                            algorithm = algorithm)\n",
    "            \n",
    "            self.model.fit(self.x_data,self.y_data)\n",
    "            \n",
    "            joblib.dump(self.model, f\"{config['model_dir']}/{self.name}.pkl\")\n",
    "            \"\"\"\n",
    "            여기 모델 저장 장소 위치를 베이스라인으로 바꿀때 model dir로 바꿔야할듯\n",
    "            \"\"\"\n",
    "            # print(f\"Successfully saved {self.name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:36.063280Z",
     "start_time": "2023-11-24T12:15:35.695476Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    이 클래스는 데이터 세트에 대한 피처 엔지니어링을 수행합니다.\n",
    "    클래스의 주요 목적은 데이터 세트에 대한 다양한 변환 및 가공을 통해 머신 러닝 모델에 적합한 형태의 피처를 생성하는 것입니다.\n",
    "\n",
    "    클래스에는 다음과 같은 메서드들이 포함됩니다:\n",
    "    1. feature_version_n: 피처 엔지니어링의 다양한 버전을 구현합니다. \n",
    "       이 메서드들은 데이터에 대한 고유한 변환을 적용하며, 다른 피처 엔지니어링 버전의 결과를 결합할 수도 있습니다.\n",
    "    2. transform: 모든 피처 엔지니어링 버전의 결과를 결합하여 최종적으로 통합된 데이터 세트를 생성하고 반환합니다.\n",
    "\n",
    "    feature_version_n 메서드의 'args' 매개변수에 대한 설명:\n",
    "    - 'args'는 가변 인자로, 다른 피처 엔지니어링 버전의 결과를 전달하는 데 사용됩니다.\n",
    "    - 예를 들어, feature_version_2 메서드가 feature_version_0의 결과를 필요로 하는 경우, \n",
    "      feature_version_2(feature_version_0()) 형태로 호출할 수 있습니다.\n",
    "    - 이런 방식으로 'args'를 사용하면, 하나의 피처 엔지니어링 버전이 다른 버전의 결과를 참조하고 활용할 수 있습니다.\n",
    "\n",
    "    주의: 이 클래스는 원본 데이터를 직접 수정하지 않습니다. 모든 변환은 새로운 데이터 프레임에 적용되며, \n",
    "    transform 메서드는 최종적으로 통합된 데이터 세트를 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, infer=False, feature_versions=None, dependencies=None,\n",
    "                 base_directory=\"./data/fe_versions\"):\n",
    "        self.data = data\n",
    "        self.infer = infer\n",
    "        self.feature_versions = feature_versions or []\n",
    "        self.dependencies = dependencies or {}  # 피처 버전 간 의존성을 정의하는 딕셔너리\n",
    "        self.base_directory = base_directory\n",
    "        if not os.path.exists(self.base_directory):\n",
    "            os.makedirs(self.base_directory)\n",
    "\n",
    "    def _save_to_parquet(self, df, version_name):\n",
    "        file_path = os.path.join(self.base_directory, f\"{version_name}.parquet\")\n",
    "        df.to_parquet(file_path)\n",
    "        print(f\"Saved {version_name} to {file_path}\")\n",
    "\n",
    "    def _load_from_parquet(self, version_name):\n",
    "        file_path = os.path.join(self.base_directory, f\"{version_name}.parquet\")\n",
    "        if os.path.exists(file_path):\n",
    "            return pq.read_table(file_path).to_pandas()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {file_path} not found.\")\n",
    "\n",
    "    \"\"\"\n",
    "    def feature_version_n(self, *args):\n",
    "        \"\"\"\"\"\"\n",
    "        이 메서드는 피처 엔지니어링을 위한 예제 버전으로, 새로운 피처들을 데이터 프레임에 추가합니다.\n",
    "    \n",
    "        과정은 다음과 같습니다:\n",
    "        1. 빈 데이터 프레임 생성: 원본 데이터(self.data)의 인덱스를 기반으로 빈 데이터 프레임(df)을 생성합니다.\n",
    "        2. 새 피처 추가: 원본 데이터의 'feature' 컬럼에 1을 더하여 새 피처를 'new_feature'라는 이름으로 df에 추가합니다.\n",
    "        3. 다른 피처 의존성 추가: args[0]에서 'feature' 값을 가져와 원본 데이터의 'feature_2'에 더하고, 이를 'new_feature_2'라는 이름으로 df에 추가합니다.\n",
    "           이 단계에서 args[0]는 이 메서드의 첫 번째 인자로, 다른 피처 엔지니어링 버전의 결과를 나타냅니다.\n",
    "        4. 결측값 처리: df의 모든 결측값을 -999로 채웁니다.\n",
    "    \n",
    "        이 메서드는 데이터 변환 및 가공 과정에서 다른 피처 엔지니어링 메서드와 함께 사용될 수 있으며, \n",
    "        머신 러닝 모델의 입력으로 사용될 수 있는 가공된 피처 세트를 생성합니다.\n",
    "    \n",
    "        반환 값: 가공된 피처를 포함하는 데이터 프레임(df)\n",
    "        \"\"\"\"\"\"\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        # add new feature to df\n",
    "        df[\"new_feature\"] = self.data[\"feature\"] + 1\n",
    "        # add new feature that depends on other feature\n",
    "        df[\"new_feature_2\"] = self.data[\"feature_2\"] + args[0][\"feature\"]\n",
    "        # fill nan values with -999\n",
    "        df = df.fillna(-999)\n",
    "        return df\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def generate_global_features(data):\n",
    "        global_features[\"version_0\"] = {\n",
    "            \"median_size\": data.groupby(\"stock_id\")[\"bid_size\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_size\"].median(),\n",
    "            \"std_size\": data.groupby(\"stock_id\")[\"bid_size\"].std() + data.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "            \"ptp_size\": data.groupby(\"stock_id\")[\"bid_size\"].max() - data.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "            \"median_price\": data.groupby(\"stock_id\")[\"bid_price\"].median() + data.groupby(\"stock_id\")[\n",
    "                \"ask_price\"].median(),\n",
    "            \"std_price\": data.groupby(\"stock_id\")[\"bid_price\"].std() + data.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "            \"ptp_price\": data.groupby(\"stock_id\")[\"bid_price\"].max() - data.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        }\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_selection(self, data, exclude_columns):\n",
    "        # 제외할 컬럼을 뺀 나머지로 구성된 새로운 DataFrame을 생성합니다.\n",
    "        selected_columns = [c for c in data.columns if c not in exclude_columns]\n",
    "        data = data[selected_columns]\n",
    "        return data\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_0(self, *args, version_name=\"feature_version_alvin_0\"):\n",
    "        # feature engineering version 0\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df[\"dow\"] = self.data[\"date_id\"] % 5\n",
    "        df[\"seconds\"] = self.data[\"seconds_in_bucket\"] % 60\n",
    "        df[\"minute\"] = self.data[\"seconds_in_bucket\"] // 60\n",
    "\n",
    "        for key, value in global_features[\"version_0\"].items():\n",
    "            df[f\"global_{key}\"] = self.data[\"stock_id\"].map(value.to_dict())\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_1(self, *args, version_name=\"feature_version_alvin_1\"):\n",
    "        # feature engineering version 1\n",
    "        # create empty dataframe\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "        df[\"volume\"] = self.data.eval(\"ask_size + bid_size\")\n",
    "        df[\"mid_price\"] = self.data.eval(\"(ask_price + bid_price) / 2\")\n",
    "        df[\"liquidity_imbalance\"] = self.data.eval(f\"(bid_size-ask_size)/(bid_size+ask_size+{EPS})\")\n",
    "        df[\"matched_imbalance\"] = self.data.eval(f\"(imbalance_size-matched_size)/(matched_size+imbalance_size+{EPS})\")\n",
    "        df[\"size_imbalance\"] = self.data.eval(f\"bid_size / ask_size+{EPS}\")\n",
    "\n",
    "        for c in combinations(prices, 2):\n",
    "            df[f\"{c[0]}_{c[1]}_imb\"] = self.data.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]} + {EPS})\")\n",
    "\n",
    "        for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "            triplet_feature = calculate_triplet_imbalance_numba(c, self.data)\n",
    "            df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_2_0(self, *args, version_name=\"feature_version_alvin_2_0\"):\n",
    "        # feature engineering version 2_0\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        df[\"imbalance_momentum\"] = self.data.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / (self.data[\n",
    "                                                                                                            'matched_size'] + EPS)\n",
    "        df[\"price_spread\"] = self.data[\"ask_price\"] - self.data[\"bid_price\"]\n",
    "        # temporary concat for groupby(stock_id)\n",
    "        temp_df = pd.concat([self.data, df], axis=1)\n",
    "        df[\"spread_intensity\"] = temp_df.groupby(['stock_id'])['price_spread'].diff()\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_2_1(self, *args, version_name=\"feature_version_alvin_2_1\"):\n",
    "        # feature engineering version 2_1\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "        df['price_pressure'] = self.data['imbalance_size'] * (self.data['ask_price'] - self.data['bid_price'])\n",
    "        df['market_urgency'] = args[1]['price_spread'] * args[0]['liquidity_imbalance']\n",
    "        df['depth_pressure'] = (self.data['ask_size'] - self.data['bid_size']) * (\n",
    "                self.data['far_price'] - self.data['near_price'])\n",
    "        for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "            df[f\"all_prices_{func}\"] = self.data[prices].agg(func, axis=1)\n",
    "            df[f\"all_sizes_{func}\"] = self.data[sizes].agg(func, axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_3(self, *args, version_name=\"feature_version_alvin_3\"):\n",
    "        # feature engineering version 3\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "            for window in [1, 2, 3, 10]:\n",
    "                df[f\"{col}_shift_{window}\"] = self.data.groupby('stock_id')[col].shift(window)\n",
    "                # df[f\"{col}_ret_{window}\"] = self.data.groupby('stock_id')[col].pct_change(window)\n",
    "                df[f\"{col}_ret_{window}\"] = self.data.groupby('stock_id')[col].transform(lambda x: custom_pct_change(x, window))\n",
    "\n",
    "        for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n",
    "            for window in [1, 2, 3, 10]:\n",
    "                df[f\"{col}_diff_{window}\"] = self.data.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_alvin_4_0(self, *args, version_name=\"feature_version_alvin_4_0\"):\n",
    "        # feature engineering version 4\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "            for window in [1, 2, 3, 10]:\n",
    "                # SMA 1, 2, 3, 10 - 이동 평균\n",
    "                df[f\"{col}_sma_{window}\"] = self.data.groupby('stock_id')[col].rolling(window=window).mean().values\n",
    "                # EMA 1, 2, 3, 10 - 지수 이동 평균\n",
    "                df[f\"{col}_ema_{window}\"] = self.data.groupby('stock_id')[col].ewm(span=window).mean().values\n",
    "                # WMA 1, 2, 3, 10 - 가중 이동 평균\n",
    "                weights = np.arange(1, window + 1)\n",
    "                df[f\"{col}_wma_{window}\"] = self.data.groupby('stock_id')[col].rolling(window=window).apply(\n",
    "                    lambda x: np.dot(x, weights) / weights.sum(), raw=True).values\n",
    "                # Volatility 1, 2, 3, 10 - 변동성\n",
    "                df[f\"{col}_volatility_{window}\"] = self.data.groupby('stock_id')[col].rolling(\n",
    "                    window=window).std().reset_index(\n",
    "                    level=0, drop=True)\n",
    "                # Price Range 1, 2, 3, 10 - 가격 범위\n",
    "                rolling_max = self.data.groupby('stock_id')[col].rolling(window=window).max().reset_index(level=0,\n",
    "                                                                                                          drop=True)\n",
    "                rolling_min = self.data.groupby('stock_id')[col].rolling(window=window).min().reset_index(level=0,\n",
    "                                                                                                          drop=True)\n",
    "                df[f\"{col}_range_{window}\"] = rolling_max - rolling_min\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_ta_indicators_1(self, *args, version_name=\"feature_version_ta_indicators_1\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        # Price Relative to Matched Size\n",
    "        df['price_relative_to_matched_size'] = self.data['reference_price'] / self.data['matched_size']\n",
    "        # Price Volatility\n",
    "        df['price_volatility'] = args[1]['price_spread'] / args[0]['mid_price']\n",
    "        # Price Change per Second\n",
    "        df['price_change_per_second'] = self.data.groupby(\"stock_id\")['reference_price'].diff() / \\\n",
    "                                        self.data.groupby(\"stock_id\")['seconds_in_bucket'].diff()\n",
    "        # WAP to Reference Price Ratio\n",
    "        df['wap_to_reference_price_ratio'] = self.data['wap'] / self.data['reference_price']\n",
    "        # Matched Size as % of Total Volume\n",
    "        df['matched_size_percent_of_total_volume'] = self.data['matched_size'] / (\n",
    "                self.data['bid_size'] + self.data['ask_size']) * 100\n",
    "\n",
    "        return df\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_ta_indicators_2(self, *args, version_name=\"feature_version_ta_indicators_2\"):\n",
    "        df = pd.DataFrame(index=self.data.index)\n",
    "        # Weighted Price Spread\n",
    "        df['weighted_price_spread'] = args[0]['price_spread'] * (self.data['bid_size'] + self.data['ask_size']) / (\n",
    "                self.data['bid_size'] * self.data['ask_size'])\n",
    "        # Imbalance Ratio\n",
    "        df['imbalance_ratio'] = self.data['imbalance_size'] / self.data['matched_size']\n",
    "        # Price Movement Indicator\n",
    "        df['price_movement_indicator'] = self.data['reference_price'] - self.data['wap']\n",
    "        # Bid-Ask Volume Ratio\n",
    "        df['bid_ask_volume_ratio'] = self.data['bid_size'] / self.data['ask_size']\n",
    "        # Relative Size to WAP\n",
    "        df['relative_size_to_wap'] = self.data['matched_size'] / self.data['wap']\n",
    "\n",
    "        return df\n",
    "        \n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def feature_version_chan_1(self, *args, version_name=\"feature_version_chan_1\"):\n",
    "\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        columns_to_drop = list(df.columns)\n",
    "        \n",
    "        if MODE == 'train':\n",
    "            columns_to_drop.remove('target')\n",
    "        \n",
    "        if MODE == 'inference':\n",
    "            columns_to_drop.remove('time_id')\n",
    "            \n",
    "        def add_imbalance_features(df):\n",
    "            epsilon = 1e-10\n",
    "            # features = ['seconds_in_bucket', 'imbalance_buy_sell_flag',\n",
    "            #            'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n",
    "            #             'reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap',\n",
    "            #             'imb_s1', 'imb_s2'\n",
    "            #            ]\n",
    "            # features list는 id로 끝나는 feature들 다 빼고 'imb_s1','imb_s2'를 추가한 것.\n",
    "            \n",
    "            df['imb_s1'] = df.eval('(bid_size-ask_size)/(bid_size+ask_size)').astype(np.float32);\n",
    "            df['imb_s2'] = df.eval('(imbalance_size-matched_size)/(matched_size+imbalance_size)').astype(np.float32)\n",
    "            \n",
    "            return df\n",
    "\n",
    "        def flatten_name(prefix, src_names):\n",
    "            # 이 함수는 나중에 PreProcessing / FE해서 새롭게 column이름 만들거나 하면 그걸 반영해주는 애인건가 싶기는 한데\n",
    "            # 안 넣을 필요는 없어서 그냥 일단 넣어봄.\n",
    "            # prefix는 book이나 그런거 들어가는 거였던 것 같고, 이번 대회에서는 데이터에 맞지 않네.\n",
    "            # 이거 나중에 필요해지므로 넣어둘게\n",
    "            ret = []\n",
    "            for c in src_names:\n",
    "                if c[0] in ['date_id','stock_id','time_id']:\n",
    "                    ret.append(c[0])\n",
    "                else:\n",
    "                    #ret.append('.'.join([prefix] + list(c)))\n",
    "                    ret.append('.'.join(list(c)))\n",
    "            return ret\n",
    "\n",
    "        def log_return(series: np.ndarray):\n",
    "            return np.log(series).diff()\n",
    "\n",
    "        def realized_volatility(series):\n",
    "            return np.sqrt(np.sum(series**2))\n",
    "\n",
    "        # def advance_fe(df):\n",
    "        #     # 처리하고 싶은 Feature들 몇개 골라서 features list 만들고\n",
    "        #     # 그 뒤에 time id group by해서 agg 만들고\n",
    "        #     # tick을 만드는게 의미있는지 확인해본 다음에\n",
    "        #     # 의미가 있으면 (아마 뒤에도 써야해서 있을 것 같긴 한데..) tick도 만들어\n",
    "        #     # 그 뒤에는 이제 NN feature들을 만들 수 있을거야..\n",
    "        #     # 이게 맞는걸까..? 모르겠지만.. 현재로써는 이게 그나마 해보고 싶은 방법이니 해보자.\n",
    "        #     df['total_volume'] = df['ask_size']+df['bid_size']\n",
    "            \n",
    "        #     df['log_return'] = df.groupby('time_id')['wap'].apply(log_return).reset_index(level=0, drop=True)\n",
    "        #     df['log_return_ask'] = df.groupby('time_id')['ask_price'].apply(log_return).reset_index(level=0, drop=True)\n",
    "        #     df['log_return_bid'] = df.groupby('time_id')['bid_price'].apply(log_return).reset_index(level=0, drop=True)\n",
    "\n",
    "        #     # 'average_values' 데이터프레임에서 평균값을 시리즈로 추출합니다.\n",
    "        #     avg_log_return = average_values['log_return']\n",
    "        #     avg_log_return_ask = average_values['log_return_ask']\n",
    "        #     avg_log_return_bid = average_values['log_return_bid']\n",
    "            \n",
    "        #     # 각 컬럼에 대해 'map'을 사용하여 'stock_id'에 해당하는 평균값을 매핑하고, 'fillna'로 결측치를 채웁니다.\n",
    "        #     df['log_return'] = df['log_return'].fillna(df['stock_id'].map(avg_log_return))\n",
    "        #     df['log_return_ask'] = df['log_return_ask'].fillna(df['stock_id'].map(avg_log_return_ask))\n",
    "        #     df['log_return_bid'] = df['log_return_bid'].fillna(df['stock_id'].map(avg_log_return_bid))\n",
    "            \n",
    "        #     return df\n",
    "        \n",
    "        def advance_fe(df):\n",
    "            # 처리하고 싶은 Feature들 몇개 골라서 features list 만들고\n",
    "            # 그 뒤에 time id group by해서 agg 만들고\n",
    "            # tick을 만드는게 의미있는지 확인해본 다음에\n",
    "            # 의미가 있으면 (아마 뒤에도 써야해서 있을 것 같긴 한데..) tick도 만들어\n",
    "            # 그 뒤에는 이제 NN feature들을 만들 수 있을거야..\n",
    "            # 이게 맞는걸까..? 모르겠지만.. 현재로써는 이게 그나마 해보고 싶은 방법이니 해보자.\n",
    "            df['total_volume'] = df['ask_size']+df['bid_size']\n",
    "            \n",
    "            first_indices = df.groupby('time_id').head(1).index\n",
    "            last_indices = df.groupby('time_id').tail(1).index\n",
    "            \n",
    "            df['log_return'] = np.log(df['wap']).diff()\n",
    "            last_waps = df.loc[last_indices, 'wap'].values\n",
    "            next_waps = df.loc[first_indices, 'wap'].values\n",
    "            first_log_returns = np.log(next_waps / last_waps)\n",
    "            df.loc[first_indices, 'log_return'] = first_log_returns\n",
    "\n",
    "            df['log_return_ask'] = np.log(df['ask_price']).diff()\n",
    "            last_asks = df.loc[last_indices, 'ask_price'].values\n",
    "            next_asks = df.loc[first_indices, 'ask_price'].values\n",
    "            first_log_return_asks = np.log(next_asks / last_asks)\n",
    "            df.loc[first_indices, 'log_return_ask'] = first_log_return_asks\n",
    "\n",
    "            df['log_return_bid'] = np.log(df['bid_price']).diff()\n",
    "            last_bids = df.loc[last_indices, 'bid_price'].values\n",
    "            next_bids = df.loc[first_indices, 'bid_price'].values\n",
    "            first_log_return_bids = np.log(next_bids / last_bids)\n",
    "            df.loc[first_indices, 'log_return_bid'] = first_log_return_bids\n",
    "            \n",
    "            # 'average_values' 데이터프레임에서 평균값을 시리즈로 추출합니다.\n",
    "            avg_log_return = average_values['log_return']\n",
    "            avg_log_return_ask = average_values['log_return_ask']\n",
    "            avg_log_return_bid = average_values['log_return_bid']\n",
    "            \n",
    "            # 각 컬럼에 대해 'map'을 사용하여 'stock_id'에 해당하는 평균값을 매핑하고, 'fillna'로 결측치를 채웁니다.\n",
    "            df['log_return'] = df['log_return'].fillna(df['stock_id'].map(avg_log_return))\n",
    "            df['log_return_ask'] = df['log_return_ask'].fillna(df['stock_id'].map(avg_log_return_ask))\n",
    "            df['log_return_bid'] = df['log_return_bid'].fillna(df['stock_id'].map(avg_log_return_bid))\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        def advance_fe_v2_1(df):\n",
    "            \n",
    "            features = {\n",
    "                'wap': [np.sum, np.mean, np.std],\n",
    "                'total_volume': [np.sum, np.mean, np.std],\n",
    "                'imb_s1': [np.sum, np.mean, np.std],\n",
    "                'imb_s2': [np.sum, np.mean, np.std],\n",
    "                #'seconds_in_bucket': ['count'],\n",
    "                'log_return': [np.sum, realized_volatility, np.mean, np.std],\n",
    "                'log_return_ask': [np.sum, realized_volatility, np.mean, np.std],\n",
    "                'log_return_bid': [np.sum, realized_volatility, np.mean, np.std]\n",
    "            }\n",
    "            \n",
    "            agg = df.groupby(['date_id', 'stock_id'],as_index=False).agg(features).reset_index(level=0, drop=True)\n",
    "            agg.columns = flatten_name('tid', agg.columns) \n",
    "            \n",
    "            return agg\n",
    "            \n",
    "        def advance_fe_v2_2(df):\n",
    "            \n",
    "            features = {\n",
    "                'wap': [np.sum, np.mean, np.std],\n",
    "                'total_volume': [np.sum, np.mean, np.std],\n",
    "                'imb_s1': [np.sum, np.mean, np.std],\n",
    "                'imb_s2': [np.sum, np.mean, np.std],\n",
    "                #'seconds_in_bucket': ['count'],\n",
    "                'log_return': [np.sum, realized_volatility, np.mean, np.std],\n",
    "                'log_return_ask': [np.sum, realized_volatility, np.mean, np.std],\n",
    "                'log_return_bid': [np.sum, realized_volatility, np.mean, np.std]\n",
    "            }\n",
    "            \n",
    "            agg = df.groupby('stock_id').agg(features).reset_index(level=0, drop=False)\n",
    "            agg.columns = flatten_name('sid', agg.columns) \n",
    "            \n",
    "            return agg\n",
    "\n",
    "        def advance_fe_v2_3(df):\n",
    "            \n",
    "            features = {\n",
    "                'wap': [np.sum, np.mean, np.std],\n",
    "                'total_volume': [np.sum, np.mean, np.std],\n",
    "                'imb_s1': [np.sum, np.mean, np.std],\n",
    "                'imb_s2': [np.sum, np.mean, np.std],\n",
    "                #'seconds_in_bucket': ['count'],\n",
    "                'log_return': [np.sum, realized_volatility, np.mean, np.std],\n",
    "                'log_return_ask': [np.sum, realized_volatility, np.mean, np.std],\n",
    "                'log_return_bid': [np.sum, realized_volatility, np.mean, np.std]\n",
    "            }\n",
    "            \n",
    "            agg = df.groupby(['date_id']).agg(features).reset_index(level=0, drop=False)\n",
    "            agg.columns = flatten_name('did', agg.columns) \n",
    "            \n",
    "            return agg\n",
    "\n",
    "        def advance_fe_v3(df):\n",
    "            # Set 'time_id' and 'stock_id' as a multi-index\n",
    "            prices = df.set_index(['time_id', 'stock_id'])[['bid_price', 'ask_price']]\n",
    "                \n",
    "            # Initialize the ticks dictionary\n",
    "            ticks = {}\n",
    "            \n",
    "            # Iterate over each unique pair of time_id and stock_id\n",
    "            for tid, sid in prices.index.unique():\n",
    "                try:\n",
    "                    # Select prices for the current time_id and stock_id\n",
    "                    price_list = prices.loc[(tid, sid)].values.flatten()\n",
    "                    # Calculate the smallest non-zero price difference\n",
    "                    price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "                    ticks[(tid, sid)] = price_diff[0] if price_diff else np.nan\n",
    "                except Exception as e:\n",
    "                    print_trace(f'tid={tid}, sid={sid}')\n",
    "                    ticks[(tid, sid)] = np.nan\n",
    "            \n",
    "            # Create a DataFrame with unique time_ids and stock_ids\n",
    "            unique_pairs = df[['time_id', 'stock_id']].drop_duplicates()\n",
    "                \n",
    "            dst = pd.DataFrame(unique_pairs)\n",
    "            \n",
    "            # Map each time_id and stock_id pair to the corresponding tick size\n",
    "            dst['tick_size'] = dst.set_index(['time_id', 'stock_id']).index.map(ticks.get)\n",
    "            \n",
    "            return dst.reset_index(drop=True)\n",
    "\n",
    "        def fe_final(df):\n",
    "            df = add_imbalance_features(df)\n",
    "            df = advance_fe(df)\n",
    "            # print(df.columns)\n",
    "            # train에서는 전반부 agg feature를 만들어서 nn feature를 만들거기 때문에 추가적인 fe 절차가 들어감.\n",
    "            # inference에서는 전반부 agg feature 자체를 만들지 않음.\n",
    "            \n",
    "            if MODE == 'train':\n",
    "                #df_1 = pd.merge(df, advance_fe_v2_1(df),on=['stock_id','date_id'],how='left')\n",
    "                df_4 = pd.merge(df,advance_fe_v3(df),on=['stock_id', 'time_id'],how='left')\n",
    "                df_4.drop('target', axis=1, inplace=True)\n",
    "                \n",
    "            if MODE == 'inference':\n",
    "                df_4 = pd.merge(df,advance_fe_v3(df),on=['stock_id', 'time_id'],how='left')\n",
    "                df_4.drop('time_id', axis=1, inplace=True)\n",
    "                \n",
    "            #이건 예전에 쓰던거\n",
    "            #df_2 = pd.merge(df, advance_fe_v2_2(df),on=['stock_id'],how='left')\n",
    "            #df_3 = pd.merge(df_2, advance_fe_v2_3(df),on=['date_id'],how='left')\n",
    "            return df_4\n",
    "\n",
    "        df = fe_final(df)\n",
    "        df['real_price'] = 0.01 / df['tick_size']\n",
    "        print(df.isna().sum())\n",
    "        \n",
    "        # the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\n",
    "        # df['tau'] = np.sqrt(1 / df['seconds_in_bucket.count']) seconds in bucket이 하나니깐 이것도 필요없지.\n",
    "\n",
    "        # nan_counts = df.isna().sum()\n",
    "        # print(nan_counts)\n",
    "\n",
    "        # unique_counts = df.nunique()\n",
    "        # print(unique_counts)\n",
    "        \n",
    "        \"\"\"\n",
    "        if MODE == 'train':\n",
    "            \n",
    "            time_id_neighbors: List[Neighbors] = []\n",
    "            stock_id_neighbors: List[Neighbors] = []\n",
    "            \n",
    "            df_pv = df[['time_id', 'stock_id']]\n",
    "    \n",
    "            df_pv['real_price'] = df['real_price']\n",
    "            df_pv['log_return.realized_volatility'] = df['log_return.realized_volatility']\n",
    "            df_pv['total_volume.sum'] = df['total_volume.sum']\n",
    "    \n",
    "            #df_pv = df_pv.groupby(['time_id', 'stock_id'], as_index=False).agg({'log_return.realized_volatility': 'mean','real_price': 'mean','total_volume.sum':'mean'})\n",
    "\n",
    "            if USE_PRICE_NN_FEATURES:\n",
    "                pivot = df_pv.pivot(index='time_id', columns='stock_id', values='real_price')\n",
    "                pivot = pivot.fillna(pivot.mean()) #아마 na값이 없기도 하고 이걸 하면 전처리 과정이 너무 복잡해서 일단 생략\n",
    "                pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "                \n",
    "                time_id_neighbors.append(\n",
    "                            TimeIdNeighbors(\n",
    "                                'time_price_c', \n",
    "                                pivot, \n",
    "                                p=2, \n",
    "                                metric='canberra', \n",
    "                                exclude_self=True\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "                print('time_price_c complete')\n",
    "                \n",
    "                time_id_neighbors.append(\n",
    "                            TimeIdNeighbors(\n",
    "                                'time_price_m', \n",
    "                                pivot, \n",
    "                                p=2, \n",
    "                                metric='mahalanobis',\n",
    "                                metric_params = {'VI': np.cov(pivot.values.T)}\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "                print('time_price_m complete')\n",
    "    \n",
    "                stock_id_neighbors.append(\n",
    "                            StockIdNeighbors(\n",
    "                                'stock_price_l1', \n",
    "                                minmax_scale(pivot.transpose()), \n",
    "                                p=1, \n",
    "                                exclude_self=True)\n",
    "                        )\n",
    "            \n",
    "                print('stock_price_l1 complete')\n",
    "\n",
    "            if USE_VOL_NN_FEATURES:\n",
    "                pivot = df_pv.pivot(index='time_id', columns='stock_id', values='log_return.realized_volatility')\n",
    "                pivot = pivot.fillna(pivot.mean())\n",
    "                pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "            \n",
    "                time_id_neighbors.append(\n",
    "                            TimeIdNeighbors('time_vol_l1', pivot, p=1)\n",
    "                        )\n",
    "            \n",
    "                print('time_vol_l1 complete')\n",
    "            \n",
    "                stock_id_neighbors.append(\n",
    "                            StockIdNeighbors(\n",
    "                                'stock_vol_l1', \n",
    "                                minmax_scale(pivot.transpose()), \n",
    "                                p=1, \n",
    "                                exclude_self=True\n",
    "                            )\n",
    "                        )\n",
    "                \n",
    "                print('stock_vol_l1 complete')\n",
    "    \n",
    "\n",
    "            if USE_SIZE_NN_FEATURES:\n",
    "                pivot = df_pv.pivot(index='time_id', columns='stock_id', values='total_volume.sum')\n",
    "                pivot = pivot.fillna(pivot.mean())\n",
    "                pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "                \n",
    "                time_id_neighbors.append(\n",
    "                            TimeIdNeighbors(\n",
    "                                'time_size_m', \n",
    "                                pivot, \n",
    "                                p=2, \n",
    "                                metric='mahalanobis', \n",
    "                                metric_params={'VI':np.cov(pivot.values.T)}\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "                print('time_size_m complete')\n",
    "            \n",
    "                time_id_neighbors.append(\n",
    "                            TimeIdNeighbors(\n",
    "                                'time_size_c', \n",
    "                                pivot, \n",
    "                                p=2, \n",
    "                                metric='canberra'\n",
    "                            )\n",
    "                        )\n",
    "                print('time_size_c complete')\n",
    "                \n",
    "            if not USE_TIME_ID_NN:\n",
    "                time_id_neighbors = []\n",
    "            \n",
    "            if not USE_STOCK_ID_NN:\n",
    "                stock_id_neighbors = []\n",
    "\n",
    "            for time_id_neighbor in time_id_neighbors:\n",
    "                joblib.dump(time_id_neighbor, f\"{config['model_dir']}/{time_id_neighbor.name}.pkl\")\n",
    "\n",
    "            print('success save time_id_neighbor')\n",
    "            \n",
    "            for stock_id_neighbor in stock_id_neighbors:\n",
    "                joblib.dump(stock_id_neighbor, f\"{config['model_dir']}/{stock_id_neighbor.name}.pkl\")\n",
    "\n",
    "            print('success save stock_id_neighbor')\n",
    "\n",
    "            \n",
    "            def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "                df2 = df.copy()\n",
    "            \n",
    "                feature_cols_stock = {\n",
    "                    'log_return.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "                    'total_volume.sum': [np.mean]\n",
    "                }\n",
    "                \n",
    "                feature_cols = {\n",
    "                    'log_return.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "                    'real_price': [np.max, np.mean, np.min],\n",
    "                    'total_volume.sum': [np.mean],\n",
    "                    'tau_nn20_stock_vol_l1_mean': [np.mean],\n",
    "                    'size.sum_nn20_stock_vol_l1_mean': [np.mean]\n",
    "                }\n",
    "            \n",
    "                time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "                time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "                stock_id_neighbor_sizes = [10, 20, 40]\n",
    "            \n",
    "                ndf: Optional[pd.DataFrame] = None\n",
    "            \n",
    "                def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "                    if ndf is None:\n",
    "                        return dst\n",
    "                    else:\n",
    "                        ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "                        return ndf\n",
    "                        \n",
    "                # neighbor stock_id\n",
    "                for feature_col in feature_cols_stock.keys():\n",
    "                    try:\n",
    "                        if feature_col not in df2.columns:\n",
    "                            print(f\"column {feature_col} is skipped\")\n",
    "                            continue\n",
    "                        if not stock_id_neighbors:\n",
    "                            continue\n",
    "                        for nn in stock_id_neighbors:\n",
    "                            nn.rearrange_feature_values(df2, feature_col)\n",
    "                        for agg in feature_cols_stock[feature_col]:\n",
    "                            for n in stock_id_neighbor_sizes:\n",
    "                                try:\n",
    "                                    for nn in stock_id_neighbors:\n",
    "                                        dst = nn.make_nn_feature(n, agg)\n",
    "                                        ndf = _add_ndf(ndf, dst)\n",
    "                                except Exception:\n",
    "                                    print_trace('stock-id nn')\n",
    "                                    pass\n",
    "                    except Exception:\n",
    "                        print_trace('stock-id nn')\n",
    "                        pass\n",
    "                if ndf is not None:\n",
    "                    df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "\n",
    "    \n",
    "                ndf = None\n",
    "            \n",
    "            \n",
    "            \n",
    "                # neighbor time_id\n",
    "                for feature_col in feature_cols.keys():\n",
    "                    try:\n",
    "                        if not USE_PRICE_NN_FEATURES and feature_col == 'real_price':\n",
    "                            continue\n",
    "                        if feature_col not in df2.columns:\n",
    "                            print(f\"column {feature_col} is skipped\")\n",
    "                            continue\n",
    "            \n",
    "                        for nn in time_id_neighbors:\n",
    "                            nn.rearrange_feature_values(df2, feature_col)\n",
    "                            \n",
    "                        if 'volatility' in feature_col:\n",
    "                            time_id_ns = time_id_neigbor_sizes_vol\n",
    "                        else:\n",
    "                            time_id_ns = time_id_neigbor_sizes\n",
    "            \n",
    "                        for agg in feature_cols[feature_col]:\n",
    "                            for n in time_id_ns:\n",
    "                                try:\n",
    "                                    for nn in time_id_neighbors:\n",
    "                                        dst = nn.make_nn_feature(n, agg)\n",
    "                                        ndf = _add_ndf(ndf, dst)\n",
    "                                except Exception:\n",
    "                                    print_trace('time-id nn')\n",
    "                                    pass\n",
    "                    except Exception:\n",
    "                        print_trace('time-id nn')\n",
    "                if ndf is not None:\n",
    "                    df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "                # features further derived from nearest neighbor features\n",
    "                try:\n",
    "                    if USE_PRICE_NN_FEATURES:\n",
    "                        for sz in time_id_neigbor_sizes:\n",
    "                            denominator = f\"real_price_nn{sz}_time_price_c\"\n",
    "                            df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
    "                            df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
    "                            df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
    "                        for sz in time_id_neigbor_sizes_vol:\n",
    "                                denominator = f\"log_return1.realized_volatility_nn{sz}_time_price_c\"\n",
    "                                df2[f'vol_rankmin_{sz}'] = \\\n",
    "                                    df2['log_return.realized_volatility'] / df2[f\"{denominator}_amin\"]\n",
    "                                df2[f'vol_rankmax_{sz}'] = \\\n",
    "                                    df2['log_return.realized_volatility'] / df2[f\"{denominator}_amax\"]\n",
    "                            \n",
    "                        # price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n",
    "                        # for c in price_cols:\n",
    "                        #     del df2[c] 원래 탭이 한번 덜 되어있었는데 뭔가 이상한 것 같아서 이 부분을 수정함.\n",
    "                    \n",
    "                    if USE_PRICE_NN_FEATURES:\n",
    "                        for sz in time_id_neigbor_sizes_vol:\n",
    "                            tgt = f'log_return.realized_volatility_nn{sz}_time_price_m_mean'\n",
    "                            df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
    "                except Exception:\n",
    "                    print_trace('nn features')\n",
    "                    \n",
    "                return df2\n",
    "                \n",
    "            gc.collect()\n",
    "            \n",
    "            df = make_nearest_neighbor_feature(df)\n",
    "        \n",
    "            # print(df.columns)\n",
    "            # print('real_price' in df.columns)\n",
    "            # df.drop(columns='Unnamed: 0', inplace=True) 이거를 해야하나 모르겠어서..\n",
    "            \n",
    "            # selected_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
    "            #                    'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
    "            #                    'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
    "            #                    'ask_size', 'wap', 'time_id', 'imb_s1', 'imb_s2','total_volume', 'log_return',\n",
    "            #                    'log_return_ask', 'log_return_bid', 'wap.sum',\n",
    "            #                    'wap.mean', 'wap.std', 'total_volume.sum', 'total_volume.mean',\n",
    "            #                    'total_volume.std', 'imb_s1.sum', 'imb_s1.mean', 'imb_s1.std',\n",
    "            #                    'imb_s2.sum', 'imb_s2.mean', 'imb_s2.std', 'log_return.sum',\n",
    "            #                    'log_return.realized_volatility', 'log_return.mean', 'log_return.std',\n",
    "            #                    'log_return_ask.sum', 'log_return_ask.realized_volatility',\n",
    "            #                    'log_return_ask.mean', 'log_return_ask.std', 'log_return_bid.sum',\n",
    "            #                    'log_return_bid.realized_volatility', 'log_return_bid.mean',\n",
    "            #                    'log_return_bid.std', 'tick_size','real_price'] tid groupby agg 사용시\n",
    "            \n",
    "            selected_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
    "                               'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
    "                               'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
    "                               'ask_size', 'wap', 'time_id', 'imb_s1', 'imb_s2','total_volume', 'log_return',\n",
    "                               'log_return_ask', 'log_return_bid','tick_size','real_price']\n",
    "            \n",
    "            unused_columns = ['wap.sum', 'wap.mean', 'wap.std', 'total_volume.sum',\n",
    "                               'total_volume.mean', 'total_volume.std', 'imb_s1.sum', 'imb_s1.mean', \n",
    "                               'imb_s1.std', 'imb_s2.sum', 'imb_s2.mean', 'imb_s2.std', \n",
    "                               'log_return.sum', 'log_return.realized_volatility', 'log_return.mean', \n",
    "                               'log_return.std', 'log_return_ask.sum', \n",
    "                               'log_return_ask.realized_volatility', 'log_return_ask.mean', \n",
    "                               'log_return_ask.std', 'log_return_bid.sum', \n",
    "                               'log_return_bid.realized_volatility', 'log_return_bid.mean', \n",
    "                               'log_return_bid.std']\n",
    "        \n",
    "            refered_columns = list(df.columns)\n",
    "            for col in selected_columns:\n",
    "                refered_columns.remove(col)\n",
    "            for col in unused_columns:\n",
    "                refered_columns.remove(col)\n",
    "            if 'row_id' in refered_columns:\n",
    "                refered_columns.remove('row_id')\n",
    "            if 'target' in refered_columns:\n",
    "                refered_columns.remove('target')\n",
    "            refered_columns.append('time_id')\n",
    "            refered_columns.append('stock_id')\n",
    "\n",
    "            df_grouped = df.groupby('stock_id')\n",
    "\n",
    "            for stock_id, df_group in df_grouped:\n",
    "                df_select = df_group[selected_columns]\n",
    "                # df_select.fillna(df_select.mean(), inplace=True)\n",
    "                df_select.to_csv(f'./baseline/df_group/{DF_GROUP_VERSION}/{stock_id}.csv')\n",
    "                df_refer = df_group[refered_columns]\n",
    "                df_refer.to_csv(f'./baseline/df_refer/{DF_REFER_VERSION}/{stock_id}.csv')\n",
    "            \n",
    "                for time_id_neighbor in time_id_neighbors:\n",
    "                    if time_id_neighbor.name == 'time_price_c' or time_id_neighbor.name == 'time_vol_l1' or time_id_neighbor.name == 'time_size_c':\n",
    "                        classifier_name = time_id_neighbor.name\n",
    "                        classifier_p = time_id_neighbor.p\n",
    "                        classifier_algorithm = time_id_neighbor.algorithm\n",
    "                        classifier_metric = time_id_neighbor.metric\n",
    "                        #classifier_metric_params = time_id_neighbor.metric_params\n",
    "            \n",
    "                        NeighborsClassifier(name=f'{stock_id}_{classifier_name}',\n",
    "                                            data=df_select,\n",
    "                                            p = classifier_p,\n",
    "                                            metric = classifier_metric,\n",
    "                                            #metric_params = classifier_metric_params,\n",
    "                                            algorithm = classifier_algorithm)\n",
    "                        \n",
    "                    if time_id_neighbor.name == 'time_price_m' or time_id_neighbor.name == 'time_size_m':\n",
    "                        classifier_name = time_id_neighbor.name\n",
    "                        classifier_p = time_id_neighbor.p\n",
    "                        classifier_algorithm = time_id_neighbor.algorithm\n",
    "                        classifier_metric = time_id_neighbor.metric\n",
    "                        classifier_metric_params = time_id_neighbor.metric_params\n",
    "            \n",
    "                        NeighborsClassifier(name=f'{stock_id}_{classifier_name}',\n",
    "                                            data=df_select,\n",
    "                                            p = classifier_p,\n",
    "                                            metric = classifier_metric,\n",
    "                                            metric_params = classifier_metric_params,\n",
    "                                            algorithm = classifier_algorithm)\n",
    "                    \n",
    "                \n",
    "                for stock_id_neighbor in stock_id_neighbors:\n",
    "                    \n",
    "                    classifier_name = stock_id_neighbor.name\n",
    "                    classifier_p = stock_id_neighbor.p\n",
    "                    classifier_algorithm = stock_id_neighbor.algorithm\n",
    "                    classifier_metric = stock_id_neighbor.metric\n",
    "                    #classifier_metric_params = stock_id_neighbor.metric_params\n",
    "        \n",
    "                    NeighborsClassifier(name=f'{stock_id}_{classifier_name}',\n",
    "                                        data=df_select,\n",
    "                                        p = classifier_p,\n",
    "                                        metric = classifier_metric,\n",
    "                                        #metric_params = classifier_metric_params,\n",
    "                                        algorithm = classifier_algorithm)\n",
    "                    \n",
    "            print('success save df_select, df_refer')    \n",
    "            print('success save classifier')\n",
    "\n",
    "            df = df.drop(unused_columns,axis=1)\n",
    "                \n",
    "        if MODE == 'inference':\n",
    "            #df2 = df.tail(200) api에서 cache를 쓰면 이런 식으로 바꿔줘야하나..?\n",
    "            df_filtered = df.dropna(subset=['stock_id'])\n",
    "            df_grouped = df_filtered.groupby('stock_id')\n",
    "            \n",
    "            result = pd.DataFrame()\n",
    "            \n",
    "            for MetricKind in ['time_vol_l1', 'stock_vol_l1']:\n",
    "\n",
    "                result_ing = pd.DataFrame()\n",
    "                    \n",
    "                for stock_id, test_x in df_grouped:\n",
    "                    test_x.drop(['row_id','currently_scored'],axis=1,inplace=True)\n",
    "                    stock_id_ = test_x['stock_id'].iloc[0]\n",
    "                    key_name_ = f\"stock{stock_id_}_{MetricKind}\"\n",
    "                    df_refer = df_refers[key_name_]\n",
    "                    df_model = df_models[key_name_]\n",
    "\n",
    "                    refer_time_id = df_model.predict(test_x)\n",
    "                    refer_time_id_row = df_refer[df_refer['time_id'] == refer_time_id[0]]\n",
    "                    refer_time_id_row.drop(columns='time_id',axis=1,inplace=True)\n",
    "\n",
    "                    result_ing = pd.concat([result_ing,refer_time_id_row],axis=0)\n",
    "                if 'stock_id' in result.columns:    \n",
    "                    result = pd.merge(result,result_ing,on=['stock_id'],how='left')\n",
    "                else:\n",
    "                    result = pd.concat([result,result_ing],axis=1)\n",
    "                \n",
    "            df = pd.merge(df,result,on=['stock_id'],how='left')\n",
    "        \n",
    "        \n",
    "        ENABLE_RANK_NORMALIZATION = False #일단은 false로 해둠.\n",
    "        \n",
    "        if ENABLE_RANK_NORMALIZATION:\n",
    "            df['tid.total_volume.sum']  = df.groupby('time_id')['tid.total_volume.sum'].rank()\n",
    "            df['tid.total_volume.mean'] = df.groupby('time_id')['tid.total_volume.mean'].rank()\n",
    "            df['did.total_volume.sum']  = df.groupby('date_id')['did.total_volume.sum'].rank()\n",
    "            df['did.total_volume.mean'] = df.groupby('date_id')['did.total_volume.mean'].rank()\n",
    "            df['sid.total_volume.sum']  = df.groupby('stock_id')['sid.total_volume.sum'].rank()\n",
    "            df['sid.total_volume.mean'] = df.groupby('stock_id')['sid.total_volume.mean'].rank()\n",
    "        \n",
    "            #df['tau'] = df.groupby('time_id')['tau'].rank() tau는 일단 사용하지 않음.\n",
    "        \"\"\"\n",
    "\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        \n",
    "        #print('what we made')\n",
    "        #print(df.columns)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    \n",
    "    # you can add more feature engineering version like above\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def execute_feature_versions(self, save=False, load=False):\n",
    "        results = {}\n",
    "\n",
    "        for version in self.feature_versions:\n",
    "            if load:\n",
    "                df = self._load_from_parquet(version)\n",
    "            else:\n",
    "                method = getattr(self, version, None)\n",
    "                if callable(method):\n",
    "                    args = []\n",
    "                    for dep in self.dependencies.get(version, []):\n",
    "                        dep_result = results.get(dep)\n",
    "                        if isinstance(dep_result, pd.DataFrame):\n",
    "                            args.append(dep_result)\n",
    "                        elif dep_result is None and hasattr(self, dep):\n",
    "                            dep_method = getattr(self, dep)\n",
    "                            dep_result = dep_method()\n",
    "                            results[dep] = dep_result\n",
    "                            args.append(dep_result)\n",
    "                        else:\n",
    "                            args.append(None)\n",
    "                    df = method(*args)\n",
    "                    if save:\n",
    "                        self._save_to_parquet(df, version)\n",
    "            results[version] = df\n",
    "\n",
    "        # return that was in self.feature_versions\n",
    "        return {k: v for k, v in results.items() if k in self.feature_versions}\n",
    "\n",
    "    @print_log(\"Executed {func_name}, Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    def transform(self, save=False, load=False):\n",
    "        feature_versions_results = self.execute_feature_versions(save=save, load=load)\n",
    "        if not self.infer:\n",
    "            self.data[\"date_id_copy\"] = self.data[\"date_id\"]\n",
    "        concat_df = pd.concat([self.data] + list(feature_versions_results.values()), axis=1)\n",
    "\n",
    "        exclude_columns = [\"row_id\", \"time_id\", \"date_id\"]\n",
    "        final_data = self.feature_selection(concat_df, exclude_columns)\n",
    "        return final_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015785,
     "end_time": "2023-11-09T03:16:26.835377",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.819592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:36.595717Z",
     "start_time": "2023-11-24T12:15:36.057278Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.060516,
     "end_time": "2023-11-09T03:16:26.914353",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.853837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    \"\"\"\n",
    "    데이터 분리 클래스\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    method : str\n",
    "        데이터 분리 방식\n",
    "    n_splits : int\n",
    "        데이터 분리 개수\n",
    "    correct : bool\n",
    "        데이터 분리 시 boundary를 맞출지 여부\n",
    "    initial_fold_size_ratio : float\n",
    "        초기 fold size 비율\n",
    "    train_test_ratio : float\n",
    "        train, test 비율\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    split()\n",
    "        데이터 분리 수행\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, n_splits, correct, initial_fold_size_ratio=0.6, train_test_ratio=0.8, gap=0,\n",
    "                 overlap=True, train_start=0,\n",
    "                 train_end=390, valid_start=391, valid_end=480):\n",
    "        self.method = method\n",
    "        self.n_splits = n_splits\n",
    "        self.correct = correct\n",
    "        self.initial_fold_size_ratio = initial_fold_size_ratio\n",
    "        self.train_test_ratio = train_test_ratio\n",
    "\n",
    "        self.gap = gap\n",
    "        self.overlap = overlap\n",
    "\n",
    "        # only for holdout method\n",
    "        self.train_start = train_start\n",
    "        self.train_end = train_end\n",
    "        self.valid_start = valid_start\n",
    "        self.valid_end = valid_end\n",
    "\n",
    "        self.target = config[\"target\"]\n",
    "\n",
    "        self.boundaries = []\n",
    "\n",
    "    def split(self, data):\n",
    "        self.data = reduce_mem_usage(data)\n",
    "        self.all_dates = self.data['date_id_copy'].unique()\n",
    "        if self.method == \"time_series\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Time series split method only works with n_splits > 1\")\n",
    "            return self._time_series_split(data)\n",
    "        elif self.method == \"rolling\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Rolling split method only works with n_splits > 1\")\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"blocking\":\n",
    "            if self.n_splits <= 1:\n",
    "                raise ValueError(\"Blocking split method only works with n_splits > 1\")\n",
    "            self.initial_fold_size_ratio = 1.0 / self.n_splits\n",
    "            return self._rolling_split(data)\n",
    "        elif self.method == \"holdout\":\n",
    "            if self.n_splits != 1:\n",
    "                raise ValueError(\"Holdout method only works with n_splits=1\")\n",
    "            return self._holdout_split(data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method\")\n",
    "\n",
    "    def _correct_boundary(self, data, idx, direction=\"forward\"):\n",
    "        # Correct the boundary based on date_id_copy\n",
    "        original_idx = idx\n",
    "        if idx == 0 or idx == len(data) - 1:\n",
    "            return idx\n",
    "        if direction == \"forward\":\n",
    "            while idx < len(data) and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx += 1\n",
    "        elif direction == \"backward\":\n",
    "            while idx > 0 and data.iloc[idx]['date_id_copy'] == data.iloc[original_idx]['date_id_copy']:\n",
    "                idx -= 1\n",
    "            idx += 1  # adjust to include the boundary\n",
    "        return idx\n",
    "\n",
    "    def _time_series_split(self, data):\n",
    "        n = len(data)\n",
    "        initial_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        initial_test_size = int(initial_fold_size * (1 - self.train_test_ratio))\n",
    "        increment = (1.0 - self.initial_fold_size_ratio) / (self.n_splits - 1)\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            fold_size = int(n * (self.initial_fold_size_ratio + i * increment))\n",
    "            train_size = fold_size - initial_test_size\n",
    "\n",
    "            if self.correct:\n",
    "                train_size = self._correct_boundary(data, train_size, \"forward\")\n",
    "                end_of_test = self._correct_boundary(data, train_size + initial_test_size, \"forward\")\n",
    "            else:\n",
    "                end_of_test = train_size + initial_test_size\n",
    "\n",
    "            train_slice = data.iloc[:train_size]\n",
    "            test_slice = data.iloc[train_size:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "\n",
    "    def _rolling_split(self, data):\n",
    "        n = len(data)\n",
    "        total_fold_size = int(n * self.initial_fold_size_ratio)\n",
    "        test_size = int(total_fold_size * (1 - self.train_test_ratio))\n",
    "        gap_size = int(total_fold_size * self.gap)\n",
    "        train_size = total_fold_size - test_size\n",
    "        rolling_increment = (n - total_fold_size) // (self.n_splits - 1)\n",
    "\n",
    "        end_of_test = n - 1\n",
    "        start_of_test = end_of_test - test_size\n",
    "        end_of_train = start_of_test - gap_size\n",
    "        start_of_train = end_of_train - train_size\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            if self.correct:\n",
    "                start_of_train = self._correct_boundary(data, start_of_train, direction=\"forward\")\n",
    "                end_of_train = self._correct_boundary(data, end_of_train, direction=\"backward\")\n",
    "                start_of_test = self._correct_boundary(data, start_of_test, direction=\"forward\")\n",
    "                end_of_test = self._correct_boundary(data, end_of_test, direction=\"forward\")\n",
    "\n",
    "            train_slice = data[start_of_train:end_of_train]\n",
    "            test_slice = data[start_of_test:end_of_test]\n",
    "            if test_slice.shape[0] == 0:\n",
    "                raise ValueError(\"Try setting correct=False or Try reducing the train_test_ratio\")\n",
    "\n",
    "            X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_train = train_slice[self.target]\n",
    "            X_test = test_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "            y_test = test_slice[self.target]\n",
    "\n",
    "            self.boundaries.append((\n",
    "                train_slice['date_id_copy'].iloc[0],\n",
    "                train_slice['date_id_copy'].iloc[-1],\n",
    "                test_slice['date_id_copy'].iloc[0],\n",
    "                test_slice['date_id_copy'].iloc[-1]\n",
    "            ))\n",
    "            yield X_train, y_train, X_test, y_test\n",
    "            start_of_train = max(start_of_train - rolling_increment, 0)\n",
    "            end_of_train -= rolling_increment\n",
    "            start_of_test -= rolling_increment\n",
    "            end_of_test -= rolling_increment\n",
    "\n",
    "    def _holdout_split(self, data):\n",
    "        # train_start ~ train_end : 학습 데이터 기간\n",
    "        # valid_start ~ valid_end : 검증 데이터 기간\n",
    "        # 학습 및 검증 데이터 분리\n",
    "        train_mask = (data['date_id_copy'] >= self.train_start) & (data['date_id_copy'] <= self.train_end)\n",
    "        valid_mask = (data['date_id_copy'] >= self.valid_start) & (data['date_id_copy'] <= self.valid_end)\n",
    "\n",
    "        train_slice = data[train_mask]\n",
    "        valid_slice = data[valid_mask]\n",
    "\n",
    "        X_train = train_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_train = train_slice[self.target]\n",
    "        X_valid = valid_slice.drop(columns=[self.target, 'date_id_copy'])\n",
    "        y_valid = valid_slice[self.target]\n",
    "\n",
    "        self.boundaries.append((\n",
    "            train_slice['date_id_copy'].iloc[0],\n",
    "            train_slice['date_id_copy'].iloc[-1],\n",
    "            valid_slice['date_id_copy'].iloc[0],\n",
    "            valid_slice['date_id_copy'].iloc[-1]\n",
    "        ))\n",
    "        yield X_train, y_train, X_valid, y_valid\n",
    "\n",
    "    def visualize_splits(self):\n",
    "        print(\"Visualizing Train/Test Split Boundaries\")\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        for idx, (train_start, train_end, test_start, test_end) in enumerate(self.boundaries):\n",
    "            train_width = train_end - train_start + 1\n",
    "            plt.barh(y=idx, width=train_width, left=train_start, color='blue', edgecolor='black')\n",
    "            plt.text(train_start + train_width / 2, idx - 0.15, f'{train_start}-{train_end}', ha='center', va='center',\n",
    "                     color='black', fontsize=8)\n",
    "\n",
    "            test_width = test_end - test_start + 1\n",
    "            plt.barh(y=idx, width=test_width, left=test_start, color='red', edgecolor='black')\n",
    "            if test_width > 0:\n",
    "                plt.text(test_start + test_width / 2, idx + 0.15, f'{test_start}-{test_end}', ha='center', va='center',\n",
    "                         color='black', fontsize=8)\n",
    "\n",
    "        plt.yticks(range(len(self.boundaries)), [f\"split {i + 1}\" for i in range(len(self.boundaries))])\n",
    "        plt.xticks(self.all_dates[::int(len(self.all_dates) / 10)])\n",
    "        plt.xlabel(\"date_id_copy\")\n",
    "        plt.title(\"Train/Test Split Boundaries\")\n",
    "        plt.grid(axis='x')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017633,
     "end_time": "2023-11-09T03:16:26.985144",
     "exception": false,
     "start_time": "2023-11-09T03:16:26.967511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:36.750264Z",
     "start_time": "2023-11-24T12:15:36.595480Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.04399,
     "end_time": "2023-11-09T03:16:27.048011",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.004021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "class OptunaWeights:\n",
    "    def __init__(self, random_state, n_trials=5000):\n",
    "        self.study = None\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "        # Define the weights for the predictions from each model\n",
    "        #         weights = [trial.suggest_float(f\"weight{n}\", -2, 3) for n in range(len(y_preds))]\n",
    "        weights = [max(0, trial.suggest_float(f\"weight{n}\", -2, 3)) for n in range(len(y_preds))]\n",
    "        # Calculate the weighted prediction\n",
    "        if sum(weights) == 0:\n",
    "            num_models = len(y_preds)\n",
    "            weights = [1 / num_models] * num_models\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
    "        auc_score = mean_absolute_error(y_true, weighted_pred)\n",
    "        #         log_loss_score=log_loss(y_true, weighted_pred)\n",
    "        return auc_score  #/log_loss_score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\",\n",
    "                                         direction='maximize')\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
    "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "\n",
    "    def weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:36.935022Z",
     "start_time": "2023-11-24T12:15:36.747314Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.052318,
     "end_time": "2023-11-09T03:16:27.118407",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.066089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # V2 ModlePipeline\n",
    "# class _ModelPipeline:\n",
    "#     def __init__(self):\n",
    "#         self.model_mode = config[\"model_mode\"]  # \"single\" or \"stacking\"\n",
    "#         self.final_mode = config[\"final_mode\"]  # False\n",
    "#         self.train_mode = config[\"train_mode\"]  # True\n",
    "#         self.mae_mode = config[\"mae_mode\"]  # False\n",
    "#         self.model_names = config[\"model_name\"]  # [\"lgb\", \"xgb\", \"pytorch_cnn\"]\n",
    "#         self.n_splits = config[\"n_splits\"]  # 5\n",
    "#         self.model_dir = config[\"model_dir\"]  # \"./model\"\n",
    "#         self.optuna_random_state = config[\"optuna_random_state\"]  # 42\n",
    "# \n",
    "#         self.best_iterate_ratio = config[\"best_iterate_ratio\"]  # 1.2\n",
    "# \n",
    "#         self.models = []  # [] fold 에서 모델 저장\n",
    "#         self.models_list = []  # [[],] 전체 fold 에서 모델 저장\n",
    "#         self.single_final_model = None\n",
    "# \n",
    "#         self.predictions = []  # [] fold 에서 모델 예측값 저장\n",
    "#         self.single_model_mae = []  # [] fold 에서 mae 값 저장\n",
    "# \n",
    "#         self.optuna_weights = []  # [] fold 에서 optuna weights 저장\n",
    "# \n",
    "#         self.inference_models = {}\n",
    "#         self.inference_prediction = None\n",
    "# \n",
    "#     def train(self, idx, X_train, y_train, X_valid=None, y_valid=None, use_early_stopping=True, best_iteration=None):\n",
    "#         self.models = []\n",
    "#         for model_name in self.model_names:\n",
    "#             model_cls = models_config[model_name][\"model\"]\n",
    "#             params = models_config[model_name][\"params\"]\n",
    "#             if best_iteration is not None:\n",
    "#                 params[\"n_estimators\"] = best_iteration[model_name]\n",
    "#             model = model_cls(**params)\n",
    "#             print(f\"\\n\\n================== Training {model_name} ({idx}/{config['n_splits']})==================\")\n",
    "#             if \"lgb\" in model_name:\n",
    "#                 fit_params = {\n",
    "#                     \"callbacks\": [lgb.callback.log_evaluation(period=100)]\n",
    "#                 }\n",
    "#                 if use_early_stopping:\n",
    "#                     fit_params[\"callbacks\"].append(lgb.callback.early_stopping(stopping_rounds=100))\n",
    "#                 if X_valid is not None and y_valid is not None:\n",
    "#                     fit_params[\"eval_set\"] = [(X_valid, y_valid)]\n",
    "# \n",
    "#                 model.fit(X_train, y_train, **fit_params)\n",
    "# \n",
    "#             elif \"xgb\" in model_name:\n",
    "#                 fit_params = {\n",
    "#                     \"eval_set\": [(X_valid, y_valid)],\n",
    "#                     \"eval_metric\": \"mae\",\n",
    "#                     \"verbose\": 100\n",
    "#                 }\n",
    "#                 if use_early_stopping:\n",
    "#                     fit_params[\"early_stopping_rounds\"] = 100\n",
    "# \n",
    "#                 model.fit(X_train, y_train, **fit_params)\n",
    "# \n",
    "#             elif model_name == \"pytorch_cnn\":\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 raise ValueError(\"Invalid model name\")\n",
    "#             print(f\"Successfully trained {model_name} ({idx}/{config['n_splits']})\")\n",
    "#             self.models.append(model)\n",
    "#             # joblib.dump(model, f\"{self.model_dir}/{idx}_{model_name}.pkl\")\n",
    "#             # print(f\"Successfully saved model ({self.model_dir}/{idx}_{model_name}.pkl)\")\n",
    "#         self.models_list.append(self.models)\n",
    "# \n",
    "#     def predict(self, idx, X_test,\n",
    "#                 infer=False):  # stacking이 true든 아니든간에 일단 각 모델 prediction은 해야함 근데 stacking이 false이면 n빵해야함\n",
    "#         self.predictions = []\n",
    "#         if infer:\n",
    "#             if not self.mae_mode:\n",
    "#                 for model_name in self.model_names:\n",
    "#                     # print(\n",
    "#                     #     f\"\\n\\n================== Inference each model {model_name} ({idx}/{config['n_splits']})==================\")\n",
    "#                     model = self.inference_models[f\"{self.model_dir}/{idx}_{model_name}.pkl\"] if MODE != \"both\" else \\\n",
    "#                         self.models_list[idx][self.model_names.index(model_name)]\n",
    "#                     self.predictions.append(model.predict(X_test))\n",
    "#                     self.inference_prediction = self.predictions[0]  # for sigle model (non stakcing)\n",
    "#                     # print(f\"Successfully inference {model_name} ({idx}/{config['n_splits']})\")\n",
    "#             else:\n",
    "#                 # single model mae\n",
    "#                 model = self.single_final_model[0]\n",
    "#                 self.predictions.append(model.predict(X_test))\n",
    "#                 self.inference_prediction = self.predictions[0]  # for sigle model (mae)\n",
    "# \n",
    "#         else:\n",
    "#             for model_name, model in zip(self.model_names, self.models):\n",
    "#                 print(\n",
    "#                     f\"\\n\\n================== Predict each model {model_name} ({idx}/{config['n_splits']})==================\")\n",
    "#                 self.predictions.append(model.predict(X_test))\n",
    "#                 print(f\"Successfully predicted {model_name} ({idx}/{config['n_splits']})\")\n",
    "#                 if self.mae_mode:\n",
    "#                     score = mean_absolute_error(y_test, self.predictions[0])\n",
    "#                     print(f\"Score for single model {model_name} ({idx}/{config['n_splits']}): {score}\")\n",
    "#                     self.single_model_mae.append(score)\n",
    "# \n",
    "#     def stacking(self, idx, y_test=None, infer=False):\n",
    "#         # stacking 코드\n",
    "#         optuna = OptunaWeights(random_state=self.optuna_random_state)\n",
    "#         if infer:\n",
    "#             self.inference_prediction = None\n",
    "#             # print(f\"\\n\\n================== Inference stacking ({idx}/{config['n_splits']})==================\")\n",
    "#             optuna.weights = self.optuna_weights[idx]\n",
    "#             self.inference_prediction = optuna.predict(self.predictions)\n",
    "#             print(f\"Successfully inference stacking ({idx}/{config['n_splits']})\")\n",
    "#         else:\n",
    "#             print(f\"\\n\\n================== Stacking ({idx}/{config['n_splits']})==================\")\n",
    "#             y_test_pred = optuna.fit_predict(y_test.values, self.predictions)\n",
    "#             score = mean_absolute_error(y_test, y_test_pred)\n",
    "#             print(f\"Score for stacking ({idx}/{config['n_splits']}): {score}\")\n",
    "#             self.optuna_weights.append(optuna.weights)\n",
    "#             print(f\"Successfully stacking ({idx}/{config['n_splits']})\")\n",
    "# \n",
    "#     def final_train(self, data):\n",
    "#         best_iterations = {name: [] for name in self.model_names}\n",
    "# \n",
    "#         for n in range(self.n_splits):\n",
    "#             for model_name in self.model_names:\n",
    "#                 model = self.models_list[n][self.model_names.index(model_name)]\n",
    "#                 if \"lgb\" in model_name:\n",
    "#                     best_iterations[model_name].append(model.best_iteration_)\n",
    "#                 elif \"xgb\" in model_name:\n",
    "#                     best_iterations[model_name].append(model.get_booster().best_iteration)\n",
    "# \n",
    "#         average_best_iterations = {name: int(int(np.mean(iterations)) * self.best_iterate_ratio) for name, iterations in\n",
    "#                                    best_iterations.items()}\n",
    "#         for model_name, average in average_best_iterations.items():\n",
    "#             print(f\"Average best iteration for {model_name.upper()} models: {average}\")\n",
    "# \n",
    "#         # final model 로 학습할거라 이전 모델 초기화\n",
    "#         self.models_list = []\n",
    "#         self.models = []\n",
    "#         self.predictions = []\n",
    "#         if self.model_mode == \"stacking\":\n",
    "#             splitter = Splitter(method=\"holdout\", n_splits=1, correct=True)\n",
    "#             for idx, (X_final_train, y_final_train, X_final_test, y_final_test) in enumerate(splitter.split(data)):\n",
    "#                 print(X_final_train.shape, y_final_train.shape, X_final_test.shape, y_final_test.shape)\n",
    "#                 self.train(idx, X_final_train, y_final_train, X_final_test, y_final_test,\n",
    "#                            use_early_stopping=False,\n",
    "#                            best_iteration=average_best_iterations)\n",
    "#                 self.predict(idx, X_final_test)\n",
    "#                 self.stacking(idx, y_final_test)\n",
    "#         else:\n",
    "#             X_final_train = data.drop(columns=[config[\"target\"], 'date_id_copy'])\n",
    "#             y_final_train = data[config[\"target\"]]\n",
    "#             print(X_final_train.shape, y_final_train.shape)\n",
    "#             self.train(0, X_final_train, y_final_train,\n",
    "#                        use_early_stopping=False,\n",
    "#                        best_iteration=average_best_iterations)\n",
    "# \n",
    "#     def save_models(self):\n",
    "#         # 모델 저장\n",
    "#         if MODE == \"train\":\n",
    "#             for idx in range(config[\"inference_n_splits\"]):\n",
    "#                 for n_model, model_name in enumerate(self.model_names):\n",
    "#                     model = self.models_list[idx][n_model]\n",
    "#                     joblib.dump(model, f\"{self.model_dir}/{idx}_{model_name}.pkl\")\n",
    "#                     print(f\"Successfully saved model ({self.model_dir}/{idx}_{model_name}.pkl)\")\n",
    "#             if self.mae_mode:\n",
    "#                 # select best model\n",
    "#                 max_idx = np.argmin(self.single_model_mae)\n",
    "#                 self.single_final_model = self.models_list[max_idx]\n",
    "#                 print(f\"The best model is {self.model_names[0]} ({max_idx}/{config['n_splits']})\")\n",
    "#                 joblib.dump(self.single_final_model, f\"{self.model_dir}/single_final_model.pkl\")\n",
    "#                 print(f\"Successfully saved single model mae ({self.model_dir}/single_final_model.pkl)\")\n",
    "# \n",
    "#     def save_optuna_weights(self):\n",
    "#         # optuna weights 저장\n",
    "#         if MODE == \"train\":\n",
    "#             if self.model_mode == \"stacking\":\n",
    "#                 joblib.dump(self.optuna_weights, f\"{self.model_dir}/optuna_weights.pkl\")\n",
    "#                 print(f\"Successfully saved optuna weights ({self.model_dir}/optuna_weights.pkl)\")\n",
    "# \n",
    "#     def load_models(self):\n",
    "#         # 모델 불러오기\n",
    "#         if MODE == \"inference\":\n",
    "#             for idx in range(config[\"inference_n_splits\"]):\n",
    "#                 for model_name in self.model_names:\n",
    "#                     model = joblib.load(f\"{self.model_dir}/{idx}_{model_name}.pkl\")\n",
    "#                     print(f\"Successfully loaded model ({self.model_dir}/{idx}_{model_name}.pkl)\")\n",
    "#                     self.inference_models[f\"{self.model_dir}/{idx}_{model_name}.pkl\"] = model\n",
    "#             if self.mae_mode:\n",
    "#                 self.single_final_model = joblib.load(f\"{self.model_dir}/single_final_model.pkl\")\n",
    "#                 print(f\"Successfully loaded single model mae ({self.model_dir}/single_final_model.pkl)\")\n",
    "# \n",
    "#     def load_optuna_weights(self):\n",
    "#         # optuna weights 불러오기\n",
    "#         if MODE == \"inference\":\n",
    "#             if self.model_mode == \"stacking\":\n",
    "#                 self.optuna_weights = joblib.load(f\"{self.model_dir}/optuna_weights.pkl\")\n",
    "#                 print(f\"Successfully loaded optuna weights ({self.model_dir}/optuna_weights.pkl)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:37.116005Z",
     "start_time": "2023-11-24T12:15:36.927876Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# V3 ModlePipeline\n",
    "# model_mode = \"single\" or \"stacking\"\n",
    "class ModelPipeline:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.models_list = []\n",
    "\n",
    "        self.predictions = []\n",
    "        self.predictions_list = []  # for single stacking\n",
    "        \n",
    "        self.inference_prediction = None\n",
    "\n",
    "        self.optuna_weights = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _lgb_train(model, X_train, y_train, X_valid, y_valid):\n",
    "        fit_params = {\n",
    "            \"callbacks\": [lgb.callback.log_evaluation(period=100), lgb.callback.early_stopping(stopping_rounds=100)],\n",
    "            # you can use callback manually\n",
    "            \"eval_set\": [(X_valid, y_valid)],\n",
    "        }\n",
    "        model.fit(X_train, y_train, **fit_params)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _xgb_train(model, X_train, y_train, X_valid, y_valid):\n",
    "        fit_params = {\n",
    "            \"eval_set\": [(X_valid, y_valid)],\n",
    "            \"eval_metric\": \"mae\",\n",
    "            \"verbose\": 100,\n",
    "            \"early_stopping_rounds\": 100\n",
    "        }\n",
    "        model.fit(X_train, y_train, **fit_params)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _cnn_train(model, X_train, y_train, X_valid, y_valid):\n",
    "        pass\n",
    "\n",
    "    def train(self, idx, X_train, y_train, X_valid, y_valid):\n",
    "        self.models = []  # each fold 에서 모델 저장\n",
    "        for model_name in config[\"model_name\"]:\n",
    "            model_cls = models_config[model_name][\"model\"]\n",
    "            params = models_config[model_name][\"params\"]\n",
    "            model = model_cls(**params)\n",
    "            print(f\"\\n\\n================== Training {model_name} ({idx}/{config['n_splits']})==================\")\n",
    "            if \"lgb\" in model_name:\n",
    "                trained_model = self._lgb_train(model, X_train, y_train, X_valid, y_valid)\n",
    "            elif \"xgb\" in model_name:\n",
    "                trained_model = self._xgb_train(model, X_train, y_train, X_valid, y_valid)\n",
    "            elif model_name == \"pytorch_cnn\":\n",
    "                trained_model = self._cnn_train(model, X_train, y_train, X_valid, y_valid)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model name\")\n",
    "            self.models.append(trained_model)\n",
    "            print(f\"Successfully trained {model_name} ({idx}/{config['n_splits']})\")\n",
    "        self.models_list.append(self.models)  # 전체 fold 에서 모델 저장\n",
    "\n",
    "    def predict(self, idx, X_test):\n",
    "        self.predictions = []  # each fold 에서 모델 예측값 저장\n",
    "        self.inference_prediction = None\n",
    "        self.models = self.models_list[idx]  # 각 fold 에서 모델 불러 오기\n",
    "        for model_name, model in zip(config[\"model_name\"], self.models):\n",
    "            print(\n",
    "                f\"\\n\\n================== Predict each model {model_name} ({idx}/{config['n_splits']})==================\") if MODE != \"inference\" else None\n",
    "            prediction = model.predict(X_test)  # 각 모델 예측\n",
    "            self.predictions.append(prediction)\n",
    "        self.inference_prediction = np.sum(self.predictions, axis=0) / len(self.predictions) # non stacking\n",
    "        self.predictions_list.append(self.predictions[0]) if config[\"stacking_mode\"] and len(config[\"model_name\"]) == 1 else None # for single stacking\n",
    "        print(f\"Successfully predicted {model_name} ({idx}/{config['n_splits']})\") if MODE != \"inference\" else None\n",
    "\n",
    "    def _optuna_stacking(self, idx, y_test, X_test, infer):\n",
    "        optuna = OptunaWeights(random_state=config[\"optuna_random_state\"])\n",
    "        if infer:\n",
    "            self.inference_prediction = None\n",
    "            optuna.weights = self.optuna_weights[idx]\n",
    "            if idx == -1:\n",
    "                self.inference_prediction = optuna.predict(self.predictions_list)\n",
    "                self.predictions_list = []\n",
    "            else:\n",
    "                self.inference_prediction = optuna.predict(self.predictions)\n",
    "        else:\n",
    "            if idx == -1:  # single model stacking\n",
    "                #  predict each model and add to self.prediction with y_test]\n",
    "                self.predictions = []\n",
    "                for idx, models in enumerate(self.models_list):\n",
    "                    for model_name, model in zip(config[\"model_name\"], models):\n",
    "                        print(\n",
    "                            f\"\\n\\n================== Predict each model for stacking {model_name} ({idx}/{config['n_splits']})==================\")\n",
    "                        self.predictions.append(model.predict(X_test))\n",
    "                        print(f\"Successfully predicted for stacking {model_name} ({idx}/{config['n_splits']})\")\n",
    "                idx = -1\n",
    "            print(f\"\\n\\n================== Stacking ({idx}/{config['n_splits']})==================\")\n",
    "            y_test_pred = optuna.fit_predict(y_test.values, self.predictions)\n",
    "            score = mean_absolute_error(y_test, y_test_pred)\n",
    "            print(f\"Score for stacking ({idx}/{config['n_splits']}): {score}\")\n",
    "            self.optuna_weights.append(optuna.weights)\n",
    "            print(f\"Successfully stacking ({idx}/{config['n_splits']})\")\n",
    "\n",
    "    def stacking(self, idx, y_test=None, X_test=None, infer=False):\n",
    "        if config[\"stacking_algorithm\"] == \"optuna\":\n",
    "            self._optuna_stacking(idx, y_test, X_test, infer=infer)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid stacking algorithm\")\n",
    "\n",
    "    def save_models(self):\n",
    "        if MODE == \"train\":\n",
    "            for idx in range(config[\"n_splits\"]):\n",
    "                for n_model, model_name in enumerate(config[\"model_name\"]):\n",
    "                    model = self.models_list[idx][n_model]\n",
    "                    joblib.dump(model, f\"{config['model_dir']}/{idx}_{model_name}.pkl\")\n",
    "                    print(f\"Successfully saved model ({config['model_dir']}/{idx}_{model_name}.pkl)\")\n",
    "\n",
    "    def save_optuna_weights(self):\n",
    "        if MODE == \"train\":\n",
    "            if config[\"stacking_mode\"]:\n",
    "                joblib.dump(self.optuna_weights, f\"{config['model_dir']}/optuna_weights.pkl\")\n",
    "                print(f\"Successfully saved optuna weights ({config['model_dir']}/optuna_weights.pkl)\")\n",
    "\n",
    "    def load_models(self):  # both 이면 안해도됨\n",
    "        if MODE == \"inference\":\n",
    "            for idx in range(config[\"n_splits\"]):\n",
    "                self.models = []\n",
    "                for model_name in config[\"model_name\"]:\n",
    "                    model = joblib.load(f\"{config['model_dir']}/{idx}_{model_name}.pkl\")\n",
    "                    self.models.append(model)\n",
    "                    print(f\"Successfully loaded model ({config['model_dir']}/{idx}_{model_name}.pkl)\")\n",
    "                self.models_list.append(self.models)\n",
    "\n",
    "    def load_optuna_weights(self):\n",
    "        if MODE == \"inference\":\n",
    "            if config[\"stacking_mode\"]:\n",
    "                self.optuna_weights = joblib.load(f\"{config['model_dir']}/optuna_weights.pkl\")\n",
    "                print(f\"Successfully loaded optuna weights ({config['model_dir']}/optuna_weights.pkl)\")\n",
    "                \n",
    "#     def load_chan_models(self):\n",
    "#         if MODE == \"inference\":\n",
    "#             for i in range(200):\n",
    "#                 joblib.load(f\"{config['model_dir']}/{i}_time_vol_l1.pkl\")\n",
    "#                 print(f\"Successfully loaded chan models ({config['model_dir']}/{i}_time_vol_l1.pkl)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016047,
     "end_time": "2023-11-09T03:16:27.179270",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.163223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ## Main\n",
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:37.116272Z",
     "start_time": "2023-11-24T12:15:37.115793Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dependencies = {\n",
    "#     \"feature_version_ta_indicators_1\": [\"feature_version_alvin_1\", \"feature_version_alvin_2_0\"],\n",
    "#     \"feature_version_ta_indicators_2\": [\"feature_version_alvin_2_0\"],\n",
    "#     \"feature_version_alvin_2_1\": [\"feature_version_alvin_1\", \"feature_version_alvin_2_0\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:37.122555Z",
     "start_time": "2023-11-24T12:15:37.116122Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # FOR VISUALIZE\n",
    "# df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "# \n",
    "# # 데이터 전처리\n",
    "# data_processor = DataPreprocessor(df)\n",
    "# df = data_processor.transform()\n",
    "# # Save할 피쳐 엔지니어링 함수 선택\n",
    "# feature_engineer = FeatureEngineer(df, feature_versions=['feature_version_alvin_0', 'feature_version_alvin_1',\n",
    "#                                            'feature_version_alvin_2_0',\n",
    "#                                            'feature_version_alvin_2_1', 'feature_version_alvin_3',\n",
    "#                                            'feature_version_alvin_4_0'\n",
    "#                                            ],\n",
    "#                      dependencies=dependencies)\n",
    "# feature_engineer.generate_global_features(df)\n",
    "# df = feature_engineer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:15:37.122718Z",
     "start_time": "2023-11-24T12:15:37.116496Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022352,
     "end_time": "2023-11-09T03:16:27.254351",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.231999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# splitter = Splitter(method=config[\"split_method\"], n_splits=config[\"n_splits\"], correct=config[\"correct\"],\n",
    "#                     initial_fold_size_ratio=0.8,\n",
    "#                     train_test_ratio=0.9, gap=0.05)\n",
    "# for idx, (X_train, y_train, X_test, y_test) in enumerate(splitter.split(df)):\n",
    "#     print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "# splitter.visualize_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T13:14:29.007800Z",
     "start_time": "2023-11-24T12:15:37.479188Z"
    },
    "papermill": {
     "duration": 0.026853,
     "end_time": "2023-11-09T03:16:27.296794",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.269941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed handle_missing_data, Elapsed time: 1.54 seconds, shape((5237760, 17))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 1.54 seconds, shape((5237760, 17))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed generate_global_features, Elapsed time: 0.63 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stock_id                   0\n",
      "date_id                    0\n",
      "seconds_in_bucket          0\n",
      "imbalance_size             0\n",
      "imbalance_buy_sell_flag    0\n",
      "reference_price            0\n",
      "matched_size               0\n",
      "far_price                  0\n",
      "near_price                 0\n",
      "bid_price                  0\n",
      "bid_size                   0\n",
      "ask_price                  0\n",
      "ask_size                   0\n",
      "wap                        0\n",
      "time_id                    0\n",
      "row_id                     0\n",
      "imb_s1                     0\n",
      "imb_s2                     0\n",
      "total_volume               0\n",
      "log_return                 0\n",
      "log_return_ask             0\n",
      "log_return_bid             0\n",
      "tick_size                  0\n",
      "real_price                 0\n",
      "dtype: int64\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_version_chan_1, Elapsed time: 181.96 seconds, shape((5237760, 8))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Saved feature_version_chan_1 to ./data/fe_versions/feature_version_chan_1.parquet\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed execute_feature_versions, Elapsed time: 182.99 seconds, shape(No shape attribute)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed feature_selection, Elapsed time: 0.22 seconds, shape((5237760, 23))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 183.69 seconds, shape((5237760, 23))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "(3759635, 21) (3759635,) (417999, 21) (417999,)\n",
      "\n",
      "\n",
      "================== Training lgb (0/3)==================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.82259\n",
      "[200]\tvalid_0's l1: 5.81173\n",
      "[300]\tvalid_0's l1: 5.80806\n",
      "[400]\tvalid_0's l1: 5.80652\n",
      "[500]\tvalid_0's l1: 5.80487\n",
      "[600]\tvalid_0's l1: 5.80286\n",
      "[700]\tvalid_0's l1: 5.80175\n",
      "[800]\tvalid_0's l1: 5.80109\n",
      "[900]\tvalid_0's l1: 5.80081\n",
      "[1000]\tvalid_0's l1: 5.8005\n",
      "[1100]\tvalid_0's l1: 5.80057\n",
      "Early stopping, best iteration is:\n",
      "[1003]\tvalid_0's l1: 5.80046\n",
      "Successfully trained lgb (0/3)\n",
      "\n",
      "\n",
      "================== Predict each model lgb (0/3)==================\n",
      "Successfully predicted lgb (0/3)\n",
      "(3755290, 21) (3755290,) (418000, 21) (418000,)\n",
      "\n",
      "\n",
      "================== Training lgb (1/3)==================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.94509\n",
      "[200]\tvalid_0's l1: 5.93409\n",
      "[300]\tvalid_0's l1: 5.93114\n",
      "[400]\tvalid_0's l1: 5.92976\n",
      "[500]\tvalid_0's l1: 5.92836\n",
      "[600]\tvalid_0's l1: 5.92628\n",
      "[700]\tvalid_0's l1: 5.92567\n",
      "[800]\tvalid_0's l1: 5.92495\n",
      "[900]\tvalid_0's l1: 5.92462\n",
      "Early stopping, best iteration is:\n",
      "[890]\tvalid_0's l1: 5.92458\n",
      "Successfully trained lgb (1/3)\n",
      "\n",
      "\n",
      "================== Predict each model lgb (1/3)==================\n",
      "Successfully predicted lgb (1/3)\n",
      "(3543925, 21) (3543925,) (418000, 21) (418000,)\n",
      "\n",
      "\n",
      "================== Training lgb (2/3)==================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.2368\n",
      "[200]\tvalid_0's l1: 6.22663\n",
      "[300]\tvalid_0's l1: 6.22297\n",
      "[400]\tvalid_0's l1: 6.22081\n",
      "[500]\tvalid_0's l1: 6.21995\n",
      "[600]\tvalid_0's l1: 6.21742\n",
      "[700]\tvalid_0's l1: 6.2165\n",
      "[800]\tvalid_0's l1: 6.21561\n",
      "[900]\tvalid_0's l1: 6.21538\n",
      "[1000]\tvalid_0's l1: 6.21492\n",
      "[1100]\tvalid_0's l1: 6.21464\n",
      "[1200]\tvalid_0's l1: 6.2146\n",
      "[1300]\tvalid_0's l1: 6.21482\n",
      "Early stopping, best iteration is:\n",
      "[1207]\tvalid_0's l1: 6.21454\n",
      "Successfully trained lgb (2/3)\n",
      "\n",
      "\n",
      "================== Predict each model lgb (2/3)==================\n",
      "Successfully predicted lgb (2/3)\n",
      "Successfully saved model (./models/20231214_02:48:06/0_lgb.pkl)\n",
      "Successfully saved model (./models/20231214_02:48:06/1_lgb.pkl)\n",
      "Successfully saved model (./models/20231214_02:48:06/2_lgb.pkl)\n",
      "Visualizing Train/Test Split Boundaries\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_pipeline = ModelPipeline()\n",
    "if config[\"train_mode\"]:\n",
    "    # 데이터 불러오기\n",
    "    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "\n",
    "    # 데이터 전처리\n",
    "    data_processor = DataPreprocessor(data=df)\n",
    "    df = data_processor.transform()\n",
    "\n",
    "    # 사용할 피쳐 엔지니어링 함수 선택\n",
    "    feature_engineer = FeatureEngineer(data=df, feature_versions=[\"feature_version_chan_1\"])\n",
    "    feature_engineer.generate_global_features(data=df)\n",
    "    df = feature_engineer.transform(save=True) # 맨 처음에는 save=True 돌렸으면, 다음부턴 transform(load=True)로 바꾸면 됨\n",
    "    \n",
    "    splitter = Splitter(method=config[\"split_method\"], n_splits=config[\"n_splits\"], correct=config[\"correct\"],\n",
    "                        initial_fold_size_ratio=config[\"initial_fold_size_ratio\"],\n",
    "                        train_test_ratio=config[\"train_test_ratio\"], gap=config[\"gap\"])\n",
    "    for idx, (X_train, y_train, X_test, y_test) in enumerate(splitter.split(data=df)):\n",
    "        print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "        model_pipeline.train(idx=idx, X_train=X_train, y_train=y_train, X_valid=X_test, y_valid=y_test)\n",
    "        model_pipeline.predict(idx=idx, X_test=X_test)\n",
    "        if config[\"stacking_mode\"] and len(config[\"model_name\"]) > 1:  # 각 폴드마다 stacking\n",
    "            model_pipeline.stacking(idx=idx, y_test=y_test)\n",
    "    if config[\"stacking_mode\"] and len(config[\"model_name\"]) == 1:  # single model 에 대한 stacking\n",
    "        model_pipeline.stacking(idx=-1, y_test=y_test,\n",
    "                                X_test=X_test)  # stacking with last fold. if you want you can stacking with all folds\n",
    "    model_pipeline.save_models()\n",
    "    model_pipeline.save_optuna_weights()\n",
    "    splitter.visualize_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014762,
     "end_time": "2023-11-09T03:16:27.326796",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.312034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### upload kaggle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset init\n",
    "! /home/username/.local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "#### dataset create \n",
    "! /home/username/.local/bin/kaggle datasets create -p {config['model_dir']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T17:36:29.621298Z",
     "iopub.status.busy": "2023-12-13T17:36:29.621023Z",
     "iopub.status.idle": "2023-12-13T17:36:29.630868Z",
     "shell.execute_reply": "2023-12-13T17:36:29.630057Z",
     "shell.execute_reply.started": "2023-12-13T17:36:29.621274Z"
    }
   },
   "outputs": [],
   "source": [
    "#lgb.plot_importance(model_pipeline.models[0], max_num_features=90, figsize=(10,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T13:14:29.050753Z",
     "start_time": "2023-11-24T13:14:29.007343Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-13T17:36:29.633924Z",
     "iopub.status.busy": "2023-12-13T17:36:29.631927Z",
     "iopub.status.idle": "2023-12-13T17:36:29.639823Z",
     "shell.execute_reply": "2023-12-13T17:36:29.638976Z",
     "shell.execute_reply.started": "2023-12-13T17:36:29.633891Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# KAGGLE_DATASET_NAME = \"model-version-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T13:15:31.234390Z",
     "start_time": "2023-11-24T13:14:29.050625Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02825,
     "end_time": "2023-11-09T03:16:27.370494",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.342244",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: ./models/20231214_02:48:06/dataset-metadata.json\n",
      "Starting upload for file 1_lgb.pkl\n",
      "100%|██████████████████████████████████████| 10.2M/10.2M [00:03<00:00, 3.01MB/s]\n",
      "Upload successful: 1_lgb.pkl (10MB)\n",
      "Starting upload for file 2_lgb.pkl\n",
      "100%|██████████████████████████████████████| 13.6M/13.6M [00:02<00:00, 4.79MB/s]\n",
      "Upload successful: 2_lgb.pkl (14MB)\n",
      "Starting upload for file 0_lgb.pkl\n",
      "100%|██████████████████████████████████████| 11.4M/11.4M [00:03<00:00, 3.94MB/s]\n",
      "Upload successful: 0_lgb.pkl (11MB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/chanlee1012/model-version-chan-100\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"train\":\n",
    "    ! /usr/local/bin/kaggle datasets init -p {config['model_dir']}\n",
    "    import json\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data[\"title\"] = data[\"title\"].replace(\"INSERT_TITLE_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "    data[\"id\"] = data[\"id\"].replace(\"INSERT_SLUG_HERE\", f\"{KAGGLE_DATASET_NAME}\")\n",
    "\n",
    "    with open(f\"{config['model_dir']}/dataset-metadata.json\", \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "    ! /usr/local/bin/kaggle datasets create -p {config['model_dir']}\n",
    "\n",
    "    # !/usr/local/bin/kaggle datasets version -p {config['model_dir']} -m 'Updated data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T17:36:29.652854Z",
     "iopub.status.busy": "2023-12-13T17:36:29.652343Z",
     "iopub.status.idle": "2023-12-13T17:36:29.663210Z",
     "shell.execute_reply": "2023-12-13T17:36:29.662330Z",
     "shell.execute_reply.started": "2023-12-13T17:36:29.652830Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if MODE == \"train\":\n",
    "#     ! /usr/local/bin/kaggle datasets init -p {chan_DIR1}\n",
    "#     import json\n",
    "    \n",
    "#     with open(f\"{chan_DIR1}/dataset-metadata.json\", \"r\") as file:\n",
    "#         data = json.load(file)\n",
    "\n",
    "#     data['title'] = data['title'].replace(\"INSERT_TITLE_HERE\", CHAN_GROUP_DATASET_NAME)\n",
    "#     data['id'] = data['id'].replace(\"INSERT_SLUG_HERE\", CHAN_GROUP_DATASET_NAME)\n",
    "\n",
    "#     with open(f\"{chan_DIR1}/dataset-metadata.json\", \"w\") as file:\n",
    "#         json.dump(data, file, indent=2)\n",
    "\n",
    "#     ! /usr/local/bin/kaggle datasets create -p {chan_DIR1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T17:36:29.664642Z",
     "iopub.status.busy": "2023-12-13T17:36:29.664302Z",
     "iopub.status.idle": "2023-12-13T17:36:29.674495Z",
     "shell.execute_reply": "2023-12-13T17:36:29.673615Z",
     "shell.execute_reply.started": "2023-12-13T17:36:29.664601Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if MODE == \"train\":\n",
    "#     ! /usr/local/bin/kaggle datasets init -p {chan_DIR2}\n",
    "#     import json\n",
    "    \n",
    "#     with open(f\"{chan_DIR2}/dataset-metadata.json\", \"r\") as file:\n",
    "#         data = json.load(file)\n",
    "\n",
    "#     data['title'] = data['title'].replace(\"INSERT_TITLE_HERE\", CHAN_REFER_DATASET_NAME)\n",
    "#     data['id'] = data['id'].replace(\"INSERT_SLUG_HERE\", CHAN_REFER_DATASET_NAME)\n",
    "\n",
    "#     with open(f\"{chan_DIR2}/dataset-metadata.json\", \"w\") as file:\n",
    "#         json.dump(data, file, indent=2)\n",
    "\n",
    "#     ! /usr/local/bin/kaggle datasets create -p {chan_DIR2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T13:15:31.269548Z",
     "start_time": "2023-11-24T13:15:31.234190Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-13T17:36:29.675791Z",
     "iopub.status.busy": "2023-12-13T17:36:29.675455Z",
     "iopub.status.idle": "2023-12-13T17:36:29.682276Z",
     "shell.execute_reply": "2023-12-13T17:36:29.681484Z",
     "shell.execute_reply.started": "2023-12-13T17:36:29.675762Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dependencies = {\n",
    "#     \"feature_version_ta_indicators_1\": [\"feature_version_alvin_1\", \"feature_version_alvin_2_0\"],\n",
    "#     \"feature_version_ta_indicators_2\": [\"feature_version_alvin_2_0\"],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T13:15:31.417133Z",
     "start_time": "2023-11-24T13:15:31.271095Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-13T17:44:34.735143Z",
     "iopub.status.busy": "2023-12-13T17:44:34.734405Z",
     "iopub.status.idle": "2023-12-13T17:44:52.463109Z",
     "shell.execute_reply": "2023-12-13T17:44:52.461857Z",
     "shell.execute_reply.started": "2023-12-13T17:44:34.735110Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 53.62895,
     "end_time": "2023-11-09T03:17:21.014741",
     "exception": false,
     "start_time": "2023-11-09T03:16:27.385791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model (/kaggle/input/model-version-chan-75/0_lgb.pkl)\n",
      "Successfully loaded model (/kaggle/input/model-version-chan-75/1_lgb.pkl)\n",
      "Successfully loaded model (/kaggle/input/model-version-chan-75/2_lgb.pkl)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed handle_missing_data, Elapsed time: 3.33 seconds, shape((5237760, 17))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed transform, Elapsed time: 3.33 seconds, shape((5237760, 17))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Executed generate_global_features, Elapsed time: 1.41 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features_ is 65 and input n_features is 21",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# prediction\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_splits\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mmodel_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacking_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     61\u001b[0m         model_pipeline\u001b[38;5;241m.\u001b[39mstacking(idx\u001b[38;5;241m=\u001b[39mi, infer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[34], line 66\u001b[0m, in \u001b[0;36mModelPipeline.predict\u001b[0;34m(self, idx, X_test)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================== Predict each model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_splits\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)==================\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m MODE \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 각 모델 예측\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions) \u001b[38;5;66;03m# non stacking\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:800\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m!=\u001b[39m n_features:\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features of the model must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    801\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch the input. Model n_features_ is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput n_features is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mpredict(X, raw_score\u001b[38;5;241m=\u001b[39mraw_score, start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration, num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[1;32m    804\u001b[0m                              pred_leaf\u001b[38;5;241m=\u001b[39mpred_leaf, pred_contrib\u001b[38;5;241m=\u001b[39mpred_contrib, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features_ is 65 and input n_features is 21"
     ]
    }
   ],
   "source": [
    "if config[\"infer_mode\"]:\n",
    "    import optiver2023\n",
    "    optiver2023.make_env.func_dict['__called__'] = False\n",
    "    \n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    y_min, y_max = -64, 64\n",
    "    qps = []\n",
    "    counter = 0\n",
    "    # cache = pd.DataFrame()\n",
    "\n",
    "    model_pipeline.load_models()\n",
    "    model_pipeline.load_optuna_weights()\n",
    "\n",
    "    # This is for the generate_global_features (only need to run once)\n",
    "    df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "    data_processor = DataPreprocessor(data=df)\n",
    "    df = data_processor.transform()\n",
    "    feature_engineer = FeatureEngineer(data=df)\n",
    "    feature_engineer.generate_global_features(data=df)\n",
    "\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        test['time_id'] = (test['seconds_in_bucket'] / 10) + (test['date_id'] * 55)\n",
    "        #print(test.columns)\n",
    "        \n",
    "        now_time = time.time()\n",
    "        # cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        # if counter > 0:\n",
    "        #     cache = cache.groupby(['stock_id']).tail(21).sort_values(\n",
    "        #         by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "        # preprocessing\n",
    "        data_processor = DataPreprocessor(data=test, infer=True)\n",
    "        test_df = data_processor.transform()\n",
    "\n",
    "        # feature engineering\n",
    "        feature_engineer = FeatureEngineer(data=test_df, infer=True,\n",
    "                                           feature_versions=[\"feature_version_chan_1\"])\n",
    "        test_df = feature_engineer.transform()\n",
    "        \n",
    "        feat = test_df[-len(test):]\n",
    "        \n",
    "#         if not test.currently_scored.iloc[0]:\n",
    "#             sample_prediction['target'] = 0\n",
    "#             env.predict(sample_prediction)\n",
    "#             counter += 1\n",
    "#             qps.append(time.time() - now_time)\n",
    "#             if counter % 10 == 0:\n",
    "#                 print(counter, 'qps:', np.mean(qps))\n",
    "#             continue\n",
    "            \n",
    "        feat = feat.drop(columns=[\"currently_scored\"])\n",
    "        \n",
    "        # feat = generate_all_features(cache)[-len(test):]\n",
    "        test_predss = np.zeros(feat.shape[0])\n",
    "        # prediction\n",
    "        for i in range(config[\"n_splits\"]):\n",
    "            model_pipeline.predict(idx=i, X_test=feat)\n",
    "            if config[\"stacking_mode\"] and len(config[\"model_name\"]) > 1:\n",
    "                model_pipeline.stacking(idx=i, infer=True)\n",
    "            test_predss += model_pipeline.inference_prediction / config[\"n_splits\"]\n",
    "        if config[\"stacking_mode\"] and len(config[\"model_name\"]) == 1:  # single model 에 대한 stacking\n",
    "            model_pipeline.stacking(idx=-1, infer=True)\n",
    "            test_predss = model_pipeline.inference_prediction\n",
    "        test_predss = zero_sum(test_predss, test['bid_size'] + test['ask_size'])\n",
    "        clipped_predictions = np.clip(test_predss, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T14:14:01.350089Z",
     "iopub.status.busy": "2023-12-13T14:14:01.349694Z",
     "iopub.status.idle": "2023-12-13T14:14:01.357496Z",
     "shell.execute_reply": "2023-12-13T14:14:01.356432Z",
     "shell.execute_reply.started": "2023-12-13T14:14:01.350059Z"
    }
   },
   "outputs": [],
   "source": [
    "# if config[\"infer_mode\"]:\n",
    "#     import optiver2023\n",
    "#     optiver2023.make_env.func_dict['__called__'] = False\n",
    "#     env = optiver2023.make_env()\n",
    "#     iter_test = env.iter_test()\n",
    "\n",
    "    # y_min, y_max = -64, 64\n",
    "    # qps = []\n",
    "    # counter = 0\n",
    "    # cache = pd.DataFrame()\n",
    "\n",
    "    # model_pipeline.load_models()\n",
    "    # model_pipeline.load_optuna_weights()\n",
    "\n",
    "    # # This is for the generate_global_features (only need to run once)\n",
    "    # df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "    # data_processor = DataPreprocessor(data=df)\n",
    "    # df = data_processor.transform()\n",
    "    # feature_engineer = FeatureEngineer(data=df)\n",
    "    # feature_engineer.generate_global_features(data=df)\n",
    "\n",
    "    # for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "    #     test['time_id'] = (test['seconds_in_bucket'] / 10) + (test['date_id'] * 55)\n",
    "    #     now_time = time.time()\n",
    "    #     cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "    #     if counter > 0:\n",
    "    #         cache = cache.groupby(['stock_id']).tail(21).sort_values(\n",
    "    #             by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "    #     # preprocessing\n",
    "    #     data_processor = DataPreprocessor(data=cache, infer=True)\n",
    "    #     cache_df = data_processor.transform()\n",
    "\n",
    "    #     # feature engineering\n",
    "    #     feature_engineer = FeatureEngineer(data=cache_df, infer=True,\n",
    "    #                                        feature_versions=['feature_version_chan_1'])\n",
    "    #     cache_df = feature_engineer.transform()\n",
    "\n",
    "    #     feat = cache_df[-len(test):]\n",
    "    #     if not test.currently_scored.iloc[0]:\n",
    "    #         sample_prediction['target'] = 0\n",
    "    #         env.predict(sample_prediction)\n",
    "    #         counter += 1\n",
    "    #         qps.append(time.time() - now_time)\n",
    "    #         if counter % 10 == 0:\n",
    "    #             print(counter, 'qps:', np.mean(qps))\n",
    "    #         continue\n",
    "    #     feat = feat.drop(columns=[\"currently_scored\"])\n",
    "\n",
    "    #     # feat = generate_all_features(cache)[-len(test):]\n",
    "    #     test_predss = np.zeros(feat.shape[0])\n",
    "    #     # prediction\n",
    "    #     for i in range(config[\"n_splits\"]):\n",
    "    #         model_pipeline.predict(idx=i, X_test=feat)\n",
    "    #         if config[\"stacking_mode\"] and len(config[\"model_name\"]) > 1:\n",
    "    #             model_pipeline.stacking(idx=i, infer=True)\n",
    "    #         test_predss += model_pipeline.inference_prediction / config[\"n_splits\"]\n",
    "    #     if config[\"stacking_mode\"] and len(config[\"model_name\"]) == 1:  # single model 에 대한 stacking\n",
    "    #         model_pipeline.stacking(idx=-1, infer=True)\n",
    "    #         test_predss = model_pipeline.inference_prediction\n",
    "    #     test_predss = zero_sum(test_predss, test['bid_size'] + test['ask_size'])\n",
    "    #     clipped_predictions = np.clip(test_predss, y_min, y_max)\n",
    "    #     sample_prediction['target'] = clipped_predictions\n",
    "    #     env.predict(sample_prediction)\n",
    "    #     counter += 1\n",
    "    #     qps.append(time.time() - now_time)\n",
    "    #     if counter % 10 == 0:\n",
    "    #         print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    # time_cost = 1.146 * np.mean(qps)\n",
    "    # print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T13:15:31.417313Z",
     "start_time": "2023-11-24T13:15:31.417017Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-13T10:13:47.604299Z",
     "iopub.status.busy": "2023-12-13T10:13:47.604008Z",
     "iopub.status.idle": "2023-12-13T10:13:47.622311Z",
     "shell.execute_reply": "2023-12-13T10:13:47.621559Z",
     "shell.execute_reply.started": "2023-12-13T10:13:47.604249Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.042958,
     "end_time": "2023-11-09T03:17:21.150593",
     "exception": false,
     "start_time": "2023-11-09T03:17:21.107635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single 1fold final / fianl no\n",
    "# single 1fold final / fianl\n",
    "# single 5fold final / fianl no\n",
    "# single 5fold final / fianl\n",
    "# stacking 1fold final / fianl no\n",
    "# stacking 1fold final / fianl\n",
    "# stacking 5fold final / fianl no\n",
    "# stacking 5fold final / fianl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4154807,
     "sourceId": 7186691,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4154762,
     "sourceId": 7186793,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4154897,
     "sourceId": 7186812,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "chan-venv",
   "language": "python",
   "name": "chan-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 99.440385,
   "end_time": "2023-11-09T03:17:22.555761",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-09T03:15:43.115376",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
