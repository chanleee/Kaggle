# Kaggle
I am revealing some of the code I used on Kaggle here. Since the data is provided by Kaggle, I am not uploading it as much as possible due to permission issues.
Our team did not participate continuously during the two weeks of this competition. Instead, we were more focused on another competition and only devoted the last six hours to this one. However, this doesn't mean we took this competition lightly. We consistently read through the discussions, keeping abreast of the competition's trends and the techniques being used. As time progressed, it became apparent that the top solutions in the competition were no longer improving single models but were instead leveraging various ensemble techniques to maximize their performance. Accordingly, our team also focused intensely on ensemble methods.
Ultimately, by choosing to increase the weight of deep learning in the weighted average ensemble of public solutions, we managed to secure second place out of 1910 teams. This experience taught us that deep learning models can capture insights and intelligence that tree models cannot, a lesson we would later apply in the 'Optiver-Trading at the Close' competition.
